<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.22.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <style> 
      ::-webkit-scrollbar{ 
        width: 10px;
        height: 10px;
      }

      ::-webkit-scrollbar-track {
        width: 0px;
        background-color: rgba(224, 224, 224, 0.2);
        /* border-radius: 5px; */
      }

      ::-webkit-scrollbar-thumb {
        width: 0px;
        background-color: rgba(220, 219, 218, 0.6);
        border-radius: 5px;
      }

      ::-webkit-scrollbar-thumb:hover {
        width: 10px;
        height: 20px;
        /* background-color: rgba(190, 190, 190, 0.2); */
        background-color: rgba(193, 192, 191, 0.7);
        border-radius: 5px;
      }

      ::-webkit-scrollbar-track:hover {
        width: 10px;
        /* background-color: rgba(150, 150, 150, 0.1); */
        background-color: rgba(224, 224, 224, 0.5);
        border-radius: 5px;
        /* background: transparent; */
        /* border-radius: 10px; */
      }

      ::-webkit-scrollbar-button:start:decrement,::-webkit-scrollbar-button:end:increment {
          width:0px;
          height: 0px;
          /* background-color: rgb(14, 221, 24); */
          /* border-radius: 50%; */
      }
    </style>
    
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>11장 케라스를 사용한 인공 신경망 소개(1) | Seojin</title>
<meta name="description" content="그레이디언트/전이학습/비지도학습">


  <meta name="author" content="Seojin">
  
  <meta property="article:author" content="Seojin">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Seojin Devlog">
<meta property="og:title" content="11장 케라스를 사용한 인공 신경망 소개(1)">
<meta property="og:url" content="http://localhost:4000/hands-on/DNN-1/">


  <meta property="og:description" content="그레이디언트/전이학습/비지도학습">







  <meta property="article:published_time" content="2025-03-07T00:00:00+09:00">



  <meta property="article:modified_time" content="2025-03-07T00:00:00+09:00">



  

  


<link rel="canonical" href="http://localhost:4000/hands-on/DNN-1/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Seojin",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Seojin Devlog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      processEscapes: true
    }
  });
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->


    <link rel="apple-touch-icon" sizes="180x180" href="https://Parkseojin2001.github.io/assets/images/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://Parkseojin2001.github.io/assets/images/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://Parkseojin2001.github.io/assets/images/favicon/favicon-16x16.png">
    <link rel="manifest" href="https://Parkseojin2001.github.io/assets/images/favicon/site.webmanifest">
    <link rel="mask-icon" href="https://Parkseojin2001.github.io/assets/images/favicon/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#ffc40d">
    <meta name="theme-color" content="#ffffff">
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Seojin Devlog
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="https://Parkseojin2001.github.io/">Home</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="https://github.com/Parkseojin2001">GitHub</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/meee.png" alt="Seojin" itemprop="image">
      
    </div>
  

  <!-- 2022.02.17 author content hidden -->
  <!-- <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Seojin</h3>
    
    
  </div> -->

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">Follow</button> -->
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle menu</label>
  <ul class="nav__items">
    <!-- <li>
      <span class="nav__total">🌴 Total Posts: 25</span>
    </li> -->
    
    
      <li>
        
          <!-- title -->
              <span class="nav__sub-title">COMPUTER SCIENCE</span>
              <hr>
        

        
        <ul>
          
            <!-- sub-title -->
            
              
            
              
                <li><a href="/categories/data-structure/">Data Structure (4)</a></li>
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
                <li><a href="/categories/algorithm/">Algorithm (1)</a></li>
              
            
              
            
              
            
              
            
              
            
          
            <!-- sub-title -->
            
              
                <li><a href="/categories/os/">Operating System (1)</a></li>
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
          
        </ul>
        
      </li>
    
      <li>
        
          <!-- title -->
              <span class="nav__sub-title">AI</span>
              <hr>
        

        
        <ul>
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
                <li><a href="/categories/pytorch/">Pytorch (3)</a></li>
              
            
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
          
            <!-- sub-title -->
            
              
            
              
            
              
                <li><a href="/categories/hands-on/">핸즈온 머신러닝 (7)</a></li>
              
            
              
            
              
            
              
            
              
            
              
            
              
            
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
                <li><a href="/categories/nlp/">NLP (5)</a></li>
              
            
              
            
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
          
        </ul>
        
      </li>
    
      <li>
        
          <!-- title -->
              <span class="nav__sub-title">PROGRAMMING</span>
              <hr>
        

        
        <ul>
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
                <li><a href="/categories/python/">Python (2)</a></li>
              
            
              
            
              
            
              
            
              
            
              
            
          
        </ul>
        
      </li>
    
      <li>
        
          <!-- title -->
              <span class="nav__sub-title">AWS</span>
              <hr>
        

        
        <ul>
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
          
        </ul>
        
      </li>
    
      <li>
        
          <!-- title -->
              <span class="nav__sub-title">PROJECT</span>
              <hr>
        

        
        <ul>
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
            
              
                <li><a href="/categories/capstone-design/">Capstone Design (1)</a></li>
              
            
              
            
              
            
          
        </ul>
        
      </li>
    
      <li>
        
          <!-- title -->
              <span class="nav__sub-title">GIT</span>
              <hr>
        

        
        <ul>
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
                <li><a href="/categories/git/">Git (1)</a></li>
              
            
              
            
              
            
              
            
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="11장 케라스를 사용한 인공 신경망 소개(1)">
    <meta itemprop="description" content="그레이디언트/전이학습/비지도학습">
    <meta itemprop="datePublished" content="2025-03-07T00:00:00+09:00">
    <meta itemprop="dateModified" content="2025-03-07T00:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">11장 케라스를 사용한 인공 신경망 소개(1)
</h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2025-03-07T00:00:00+09:00">March 7, 2025</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#111-그레이디언트-소실과-폭주-문제">11.1 그레이디언트 소실과 폭주 문제</a><ul><li><a href="#1111-글로럿과-he-초기화">11.1.1 글로럿과 He 초기화</a><ul><li><a href="#활성화-함수와-초기화-방식">활성화 함수와 초기화 방식</a></li></ul></li></ul></li><li><a href="#1112-수렴하지-않는-활성화-함수">11.1.2 수렴하지 않는 활성화 함수</a><ul><li><a href="#활성화-함수-쓰는-방법">활성화 함수 쓰는 방법</a></li></ul></li></ul></li><li><a href="#1113-배치-정규화">11.1.3 배치 정규화</a><ul><li><a href="#케라스로-배치-정규화-구현하기">케라스로 배치 정규화 구현하기</a></li></ul></li><li><a href="#1114-그레이디언트-클리핑">11.1.4 그레이디언트 클리핑</a></li></ul></li><li><a href="#112-사전훈련된-층-재사용하기">11.2 사전훈련된 층 재사용하기</a><ul><li><a href="#1121-케라스를-사용한-전이-학습">11.2.1 케라스를 사용한 전이 학습</a></li><li><a href="#1122-비지도-사전훈련">11.2.2 비지도 사전훈련</a></li><li><a href="#1123-보조-작업에서-사전훈련">11.2.3 보조 작업에서 사전훈련</a></li></ul></li></ul>

            </nav>
          </aside>
        
        <p>데이터에 따라 깊은 심층 신경망을 훈련해야 한다. 심층 신경망을 훈련하는 도중에 다음과 같은 문제를 마주할 수 있다.</p>

<ul>
  <li><strong>그레이디언트 소실</strong> 또는 <strong>그레이디언트 폭주</strong> 문제에 직면할 수 있다. 심층 신경망의 아래쪽으로 갈수록 그레이디언트가 점점 더 작아지거나 커지는 현상이다. 두 현상 모두 하위층을 훈련하기 매우 어렵게 만든다.</li>
  <li>대규모 신경망을 위한 훈련 데이터가 충분하지 않거나 레이블을 만드는 작업에 비용이 너무 많이 들 수 있다.</li>
  <li>훈련이 극단적으로 느려질 수 있다.</li>
  <li>수백만 개의 파라미터를 가진 모델은 훈련 세트에 과대적합될 위험이 매우 크며 특히 훈련 샘플이 충분하지 않거나 잡음이 많은 경우 발생한다.</li>
</ul>

<h1 id="111-그레이디언트-소실과-폭주-문제">11.1 그레이디언트 소실과 폭주 문제</h1>

<p><strong>그레이디언트 소실</strong><br />
알고리즘이 하위층으로 진행될수록 그레이디언트가 점점 작아지는 경우가 많으며 이는 하위층의 연결 가중치를 변경되지 않은 채로 두게되며 훈련이 좋은 솔루션으로 수렴되지 않는 현상</p>

<p><strong>그레이디언트 폭주</strong><br />
그레이디언트가 점점 커져서 여러 층이 비정상적으로 큰 가중치로 갱신되어 알고리즘이 발산할 수 있다. 주로 순환 신경망에서 나타난다.</p>

<p>원인은 로지스틱 시그모이드 활성화 함수와 가중치 초기화 방법의 조합이었다.</p>
<ul>
  <li>각 층에서 출력의 분산이 입력의 분산보다 더 크다.</li>
  <li>신경망의 위쪽으로 갈수록 층을 지날 때마다 분산이 계속 커져 가장 높은 층에서는 활성화 함수가 0이나 1로 수렴한다.</li>
  <li>로지스틱 함수의 평균이 0이 아니고 0.5라는 사실 때문에 더 나빠진다.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/78655692/148024675-70b17a10-cba9-480a-ab91-189d4722c19b.png" height="400px" width="500px" /></p>

<ul>
  <li>입력이 0이나 1로 수렴해서 기울기가 0에 매우 가까워진다.</li>
  <li>역전파가 될 때 전파할 그레이디언트가 거의 없고 조금 있는 그레이디언트는 최상위층에서부터 역전파가 진행되면서 점차 약해져서 실제로 아래쪽 층에는 아무것도 도달하지 않게 된다.</li>
</ul>

<h2 id="1111-글로럿과-he-초기화">11.1.1 글로럿과 He 초기화</h2>
<ul>
  <li>층에 사용되는 활성화 함수의 종류에 따라 세 가지 초기화 방식 중 하나를 선택
    <ul>
      <li>글로럿(Glorot) 초기화</li>
      <li>르쿤(LeCun) 초기화</li>
      <li>헤(He) 초기화</li>
    </ul>
  </li>
  <li>예측을 할 때는 정방향으로, 그레이디언트를 역전파할 때는 역방향으로 양방향 신호가 적절하게 흘러야 한다.</li>
  <li>글로럿과 벤지오는 적절한 신호가 흐르기 위해서는 각 충의 출력에 대한 분산이 입력에 대한 분산과 같아야 한다고 주장했다. 그리고 역방향에서 층을 통과하기 전과 후의 그레이디언트 분산이 동일해야 한다.</li>
</ul>

<p><strong>세이비어 초기화</strong> or <strong>글로럿 초기화</strong><br />
\(fan_{avg} = \frac{fan_{in} + fan_{out}}{2}\)</p>
<ul>
  <li>$fan_{in}$(팬-인) : 층에 들어오는 입력 수</li>
  <li>$fan_{out}$(팬-아웃) : 층에 들어오는 입력 수</li>
  <li>평균($\mu$)이 0이고 분산($\sigma^2$)이 $\sigma^2 = \frac{1}{fan_{avg}}$ 인 정규분포</li>
  <li>$r = \sqrt{\frac{3}{fan_{avg}}}$ 일 때 $-r$과 $+r$ 사이의 균등분포</li>
</ul>

<p><strong>르쿤 초기화</strong><br /></p>
<ul>
  <li>글로럿 초기화 정의에서 $fan_{avg}$를 $fan_{in}$으로 변경</li>
</ul>

<p><strong>He 초기화</strong><br /></p>
<ul>
  <li>ReLU와 ReLU의 변종 활성화 함수에 대한 초기화 전략</li>
</ul>

<h4 id="활성화-함수와-초기화-방식">활성화 함수와 초기화 방식</h4>

<table>
  <thead>
    <tr>
      <th>초기화 전략</th>
      <th>활성화 함수</th>
      <th>$\sigma^2$(정규분포)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>글로럿</td>
      <td>활성화 함수 없음, 하이퍼볼릭 탄젠트, 로지스틱, 소프트맥스</td>
      <td>$1/fan_{avg}$</td>
    </tr>
    <tr>
      <td>He</td>
      <td>ReLU 함수와 그 변종들</td>
      <td>$2/fan_{in}$</td>
    </tr>
    <tr>
      <td>르쿤</td>
      <td>SELU</td>
      <td>$1/fan_{in}$</td>
    </tr>
  </tbody>
</table>

<p>케라스는 기본적으로 균등분포의 글로럿 초기화를 사용한다. 다음과 같이 층을 만들 때 <code class="language-plaintext highlighter-rouge">kernel_initializer="he_uniform"</code> 이나 <code class="language-plaintext highlighter-rouge">kernel_initializer="he_normal"</code>로 바꾸어 He 초기화를 사용할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">)</span>
</code></pre></div></div>

<p>$fan_{in}$ 대신 $fan_{out}$ 기반의 균등분포 He 초기화를 사용하는 경우</p>
<ul>
  <li>Variance Scaling 사용</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.initializers</span> <span class="kn">import</span> <span class="n">VarianceScaling</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="n">he_avg_init</span> <span class="o">=</span> <span class="n">VarianceScaling</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'fan_avg'</span><span class="p">,</span>
                              <span class="n">distribution</span><span class="o">=</span><span class="s">'uniform'</span><span class="p">)</span>
<span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">he_avg_init</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="1112-수렴하지-않는-활성화-함수">11.1.2 수렴하지 않는 활성화 함수</h2>

<p>그전에는 대부분 시그모이드 활성화 함수가 최선의 선택일 것이라고 생각했다. 하지만 ReLU 함수는 특정 양수값에 수렴하지 않으며 계산이 빠르다는 큰 장점이 있다.<br />
하지만 dying ReLU로 알려진 문제점이 있다.</p>
<ul>
  <li>훈련하는 동안 일부 뉴런이 0 이외의 값을 출력하지 않는다라는 의미</li>
</ul>

<p>가중치 합이 음수이면 ReLU 함수의 그레이디언트가 0이 되므로 경사하강법이 더는 작동하지 않는다.</p>

<p>해결책: <code class="language-plaintext highlighter-rouge">LeakyReLU</code> 함수</p>
<ul>
  <li>$LeakyReLU_{\alpha}(z) = max(\alpha z, z)$</li>
  <li>하이퍼파라미터 $\alpha$가 이 함수가 ‘새는’ 정도를 결정한다.
    <ul>
      <li>새는 정도란 $z &lt; 0$ 일 때 이 함수의 기울기이며, 일반적으로 0.01로 설정한다. 이 작은 기울기가 LeakyReLU를 절대 죽지 않게 만들어준다.</li>
    </ul>
  </li>
  <li>ReLU보다 좋은 성능 발휘(default = $\alpha = 0.1$)
    <ul>
      <li>$\alpha = 0.2$로 할 때 좀 더 성능이 좋아짐</li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/78655692/148028052-8b6c9ff4-9e5b-44bc-9676-f317fe533aab.png" height="400px" width="500px" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">RReLU</code>(randomized leaky ReLU) : 훈련하는 동안 주어진 범위에서 $\alpha$를 무작위로 선택하고 테스트시에는 평균을 사용
    <ul>
      <li>과대적합을 줄이는 규제역할도 수행</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">PReLU</code>(parametric leaky ReLU) : $\alpha$가 훈련하는 동안 학습(하이퍼파라미터가 아니고 다른 모델 파라미터와 마찬가지로 역전파에 의해 변경)
    <ul>
      <li>대규모 이미지 데이터셋에서 ReLU보다 성능이 좋음</li>
      <li>소규모 데이터셋에서는 과대적합 위험성 존재</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">ELU</code>(exponential linear unit): 앞에서 언급된 ReLU 변종들보다 성능이 좋음
    <ul>
      <li>훈련 시간이 줄고 신경망의 테스트 세트 성능도 더 높음</li>
    </ul>
  </li>
</ul>

\[ELU_{\alpha}(z)=
\begin{cases}
\alpha(exp(z)-1) \; if\;z&lt;0\\
z \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; if\;z\geq0
\end{cases}\]

<p><img src="https://user-images.githubusercontent.com/78655692/148224276-da25c5c6-1bf5-469c-93f7-ae6a1e3226bc.png" height="400px" width="500px" /></p>

<ul>
  <li>$z &lt; 0$일 때 음수값이 들어오므로 활성화 함수의 평균 출력이 0에 더 가꿔워진다. 이는 그레이디언트 소실 문제를 완화해준다.</li>
  <li>하이퍼파라미터 $\alpha$는 z가 큰 음수값일 때 ELU가 수렴할 값을 정의한다.</li>
  <li>$z &lt; 0$이어도 그레이디언트가 0이 아니므로 죽은 뉴런을 만들지 않는다.</li>
  <li>$\alpha=1$이면 이 함수는 $z=0$에서 급격히 변동하지 않으므로 $z=0$을 포함해 모든 구간에서 매끄러워 경사 하강법의 속도를 높여준다.</li>
</ul>

<p><strong>장단점</strong><br /></p>
<ul>
  <li>수렴 속도가 빠르다.</li>
  <li>지수 함수를 사용하므로 ReLU나 그 변종들보다 계산이 느리다.</li>
  <li>
    <p>훈련 시에는 수렴 속도가 빨라서 느린 계산이 상쇄되지만 테스트 시에는 ELU를 사용한 네트워크기 ReLU를 사용한 네트워크보다 느릴 것이다.</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">SELU</code>(Scaled ELU) : ELU 활성화 함수의 변종
    <ul>
      <li>완전 연결 층만 쌓아서 신경망을 만들고 모든 은닉층이 SELU 활성화 함수를 사용한다면 네트워크가 자기 정규화(self-normalized)된다고 주장</li>
      <li>훈련하는 동안 각 층의 출력이 평균 0과 표준편차 1을 유지하는 경향이 있다.
        <ul>
          <li>그레이디언트 소실과 폭주 문제를 막아준다.</li>
        </ul>
      </li>
      <li>다른 활성화 함수보다 뛰어난 성능을 종종 보이지만 자기 정규화가 일어나기 위한 몇 가지 조건이 있다.
1) 입력 특성이 반드시 표준화(평균 0, 표준편차 1)되어야 한다.
2) 모든 은닉층의 가중치는 르쿤 정규분포 초기화로 초기화되어야 한다. 케라스에서는 <code class="language-plaintext highlighter-rouge">kernel_initializer="lecun_normal</code>로 설정
3) 네트워크는 일렬로 쌓은 층으로 구성되어야 한다. 순환 신경망이나 스킵 연결과 같이 순차적이지 않은 구조에서 사용하면 자기 정규화되는 것을 보장하지 않는다.
4) 모든 층이 완전연결층이어야 한다.</li>
    </ul>
  </li>
</ul>

<h4 id="활성화-함수-쓰는-방법">활성화 함수 쓰는 방법</h4>
<ul>
  <li>일반적으로 SELU &gt; ELU &gt; LeakyReLU(그리고 변종들) &gt; ReLU &gt; tanh &gt; sigmoid 순</li>
  <li>네트워크가 자기 정규화되지 못하는 구조라면 SELU 보단 <code class="language-plaintext highlighter-rouge">ELU</code></li>
  <li>실행 속도가 중요하다면 <code class="language-plaintext highlighter-rouge">LeakyReLU</code>(하이퍼파라미터를 더 추가하고 싶지 않다면 케라스에서 사용하는 기본값 
$\alpha$ 사용)</li>
  <li>시간과 컴퓨팅 파워가 충분하다면 교차 검증을 사용해 여러 활성화 함수를 평가</li>
  <li>신경망이 과대적합되었다면 <code class="language-plaintext highlighter-rouge">RReLU</code></li>
  <li>훈련세트가 아주 크다면 <code class="language-plaintext highlighter-rouge">PReLU</code></li>
  <li>ReLU가 가장 널리 사용되는 활성화 함수이므로 많은 라이브러리와 하드웨어 가속기들이 ReLU에 특화되어 최적화.
따라서 속도가 중요하다면 <code class="language-plaintext highlighter-rouge">ReLU</code>가 가장 좋은 선택</li>
</ul>

<p><strong><code class="language-plaintext highlighter-rouge">LeakyReLU</code> 활성화 함수 사용</strong><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">LeakyReLU</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
  <span class="p">[...]</span>
  <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
  <span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
  <span class="p">[...]</span>
<span class="p">])</span>
</code></pre></div></div>

<p><strong><code class="language-plaintext highlighter-rouge">PReLU</code> 활성화 함수 사용</strong><br /></p>
<ul>
  <li>PReLU층을 만들고 모델에서 적용하려는 층 뒤에 추가</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">PReLU</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
  <span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span>
  <span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_normal'</span><span class="p">),</span>
  <span class="n">PReLU</span><span class="p">(),</span>
  <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_normal'</span><span class="p">),</span>
  <span class="n">PReLU</span><span class="p">(),</span>
  <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p><strong><code class="language-plaintext highlighter-rouge">SELU</code> 활성화 함수 사용</strong><br /></p>
<ul>
  <li>층을 만들 때 <code class="language-plaintext highlighter-rouge">activation='selu'</code> 와 <code class="language-plaintext highlighter-rouge">kernel_initializer='lecun_normal</code> 지정</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'selu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'lecun_normal'</span><span class="p">))</span>

<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">99</span><span class="p">):</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'selu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'lecun_normal'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="1113-배치-정규화">11.1.3 배치 정규화</h2>

<p><code class="language-plaintext highlighter-rouge">ELU(or ReLU 변종) + He 초기화</code>를 사용하면 훈련 초 단계서 그이언트 소실이나 폭주 문제를 크게 감소 시킬 수 있지만, 훈련하는 동안 다시 발생할 수 있다.</p>

<p><strong>배치정규화</strong>(BN; batch normalization)는 그레이디언트 소실과 폭주 문제를 해결하기 위해 등장했다.</p>
<ul>
  <li>활성화 함수를 통과하기 전이나 후에 모델에 연산을 하나 추가한다.</li>
  <li>단순하게 입력을 원점에 맞추고 정규화한 다음, 각 층에서 두 개의 새로운 파라미터로 결과값의 스케일을 조정하고 이동시킨다.
    <ul>
      <li>평균: 0으로 조정</li>
      <li>분산: 스케일 조정</li>
    </ul>
  </li>
  <li>하나는 스케일 조정에, 다른 하나는 이동에 사용한다.</li>
  <li>대부분 신경망의 첫 번째 층으로 배치 정규화를 추가하면 훈련 세트를 표준화할 필요가 없다.</li>
  <li>입력 데이터를 원점에 맞추고 정규화하려는 알고리즘은 평균과 표준편차를 추정해야하므로 현재 미니배치에서 입력의 평균과 표준편차를 평가한다.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/78655692/163700923-e5387adf-2460-43f1-bb11-58aba432abbc.png" height="350px" width="500px" /></p>

<p><strong>배치 정규화 알고리즘</strong><br /></p>

<ol>
  <li>
    <p>$\quad \mu_{\beta} = \frac{1}{m_{\beta}} \sum_{i=1}^{m_{\beta}} x^i$</p>
  </li>
  <li>
    <p>$\quad \sigma_B^2 = \frac{1}{m_{\beta}} \sum_{i=1}^{m_{\beta}} (x^{(i)} - \mu_B)^2$</p>
  </li>
  <li>
    <p>$\quad \hat{x}^{(i)} = \frac{x^{(i)} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$</p>
  </li>
  <li>
    <p>$\quad z^{(i)} = \gamma \otimes \hat{x}^{(i)} + \beta$</p>
  </li>
</ol>

<ul>
  <li>훈련하는 동안 배치 정규화는 입력을 정규화한 다음 스케일을 조정하고 이동시킨다.</li>
  <li>훈련이 끝난 후 전체 훈련 세트를 신경망에 통과시켜 배치 정규화 층의 각 입력에 대한 평균과 표준편차를 계산한다.
    <ul>
      <li>예측할 때 배치 입력 평균과 표준 편차로 이 ‘최종’ 입력 평균과 표준편차를 대신 사용할 수 있다.</li>
    </ul>
  </li>
  <li>대부분 배치 정규화 구현은 층의 입력 평균과 표준편차의 이동 평균을 사용해 훈련하는 동안 최종 통계를 추정한다.</li>
  <li>케라스의 <code class="language-plaintext highlighter-rouge">BatchNormalization</code> 층은 이를 자동으로 수행한다.</li>
  <li>배치 정규화 층마다 네 개의 파라미터 벡터가 학습된다.
    <ul>
      <li>$\gamma$(출력 스케일 벡터)와 $\beta$(출력 이동 벡터)는 일반적인 역전파를 통해 학습된다.</li>
      <li>$\mu$(최종 입력 평균 벡터)와 $\sigma$(최종 입력 표준편차 벡터)는 지수 이동 평균을 사용하여 추정된다.
        <ul>
          <li>$\mu$와 $\sigma$는 훈련하는 동안 추정되지만 훈련이 끝난 후에 사용된다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>결과적으로 심층 신경망에서 배치 정규화가 성능을 크게 향상시켰다.</p>
<ul>
  <li>그레이디언트 소실/폭주 문제가 크게 감소하여 하이퍼볼릭 탄젠트 또는 로지스틱 활성화 함수를 사용할 수 있다.</li>
  <li>가중치 초기화에 네트워크가 덜 민감</li>
  <li>훨씬 큰 학습률을 사용하여 학습 과정의 속도를 크게 높을 수 있다.</li>
  <li>규제와 같은 역할을 하여 다른 규제 기법의 필요성을 줄여준다.</li>
</ul>

<p>하지만, 배치 정규화를 사용할 때 에포크마다 더 많은 시간이 걸리므로 훈련이 느려지지만 더 적은 에포크로 동일한 성능에 도달할 수 있어 실제 걸리는 시간은 보통 더 짧다.</p>

<h3 id="케라스로-배치-정규화-구현하기">케라스로 배치 정규화 구현하기</h3>

<p>은닉층의 활성화 함수 전이나 후에 <code class="language-plaintext highlighter-rouge">BatchNormalization</code> 층을 추가하면 된다.</p>
<ul>
  <li>모델의 첫 번째 층으로 배치 정규화를 추가할 수 있다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">BatchNormalization</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Seqnetial</span><span class="p">([</span>
  <span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span>
  <span class="n">BatchNormalization</span><span class="p">(),</span>
  <span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
  <span class="n">BatchNormalization</span><span class="p">(),</span>
  <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"elu"</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">"he_normal"</span><span class="p">),</span>
  <span class="n">BatchNormalization</span><span class="p">(),</span>
  <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"softmax"</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>

<span class="c1"># Model: "sequential_1"
# _________________________________________________________________
# Layer (type)                 Output Shape              Param #   
# =================================================================
# flatten_1 (Flatten)          (None, 784)               0         
# _________________________________________________________________
# batch_normalization_1 (Batch (None, 784)               3136      
# _________________________________________________________________
# dense_1 (Dense)              (None, 300)               235500    
# _________________________________________________________________
# batch_normalization_2 (Batch (None, 300)               1200      
# _________________________________________________________________
# dense_2 (Dense)              (None, 100)               30100     
# _________________________________________________________________
# batch_normalization_3 (Batch (None, 100)               400       
# _________________________________________________________________
# dense_3 (Dense)              (None, 10)                1010      
# =================================================================
# Total params: 271,346
# Trainable params: 268,978
# Non-trainable params: 2,368
# _________________________________________________________________
</span></code></pre></div></div>

<ul>
  <li>배치 정규화 층은 입력마다 네 개의 파라미터 $\gamma$, $\beta$, $\mu$, $\sigma$를 추가한다. (784 x 4 = 3,316개의 파라미터)</li>
  <li>마지막 2개의 파라미터 $\mu$와 $\sigma$는 이동 평균이다. 이 파라미터는 역전파로 학습되지 않기 때문에 케라스는 ‘Non-trainable’ 파라미터로 분류한다.</li>
</ul>

<p>활성화 함수 전에 배치 정규화 층을 추가하려면 은닉층에서 활성화 함수를 지정하지 말고 배치 정규화 층 뒤에 별도의 층으로 추가해야한다. 또한 배치 정규화 층은 입력마다 이동 파라미터를 포함하기 때문에 이전 층에서 편향을 뺄 수 있다.(<code class="language-plaintext highlighter-rouge">use_bias=False</code>)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">BatchNormalization</span><span class="p">,</span> <span class="n">Activation</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
  <span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span>
  <span class="n">BatchNormalization</span><span class="p">(),</span>
  <span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_normal'</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
  <span class="n">BatchNormalization</span><span class="p">(),</span>
  <span class="n">Activation</span><span class="p">(</span><span class="s">'elu'</span><span class="p">),</span>
  <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'he_normal'</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
  <span class="n">BatchNormalization</span><span class="p">(),</span>
  <span class="n">Activation</span><span class="p">(</span><span class="s">'elu'</span><span class="p">),</span>
  <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">BatchNormalization</code> 클래스는 조정할 하이퍼파라미터가 적다.
    <ul>
      <li>가끔 <code class="language-plaintext highlighter-rouge">momentum</code> 매개변수를 변경이 필요(지수 이동 평균을 업데이트할 때 사용)
        <ul>
          <li>새로운 값 <code class="language-plaintext highlighter-rouge">v</code>(입력 평균 벡터 or 표준편차 벡터)가 주어지면 다음 식을 사용해 이동 평균 $\hat v$를 업데이트한다.</li>
        </ul>

\[\hat{v} \leftarrow \hat{v} \times momentum + v \times (1 - momentum)\]

        <ul>
          <li>적절한 모멘텀 값은 일반적으로 1에 가깝다.(데이터셋이 크고 미니배치가 작으면 1에 더 가깝게 조정)</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">axis</code> 하이퍼파라미터는 정규화할 축을 결정하며 기본값은 -1(마지막 축)이다.
        <ul>
          <li>입력 배치가 2D이면 각 입력 특성이 배치에 있는 모든 샘플에 대해 계산한 평균과 표준편차를 기반으로 정규화된다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1114-그레이디언트-클리핑">11.1.4 그레이디언트 클리핑</h2>

<p>그레이디언트 폭주 문제를 완화하는 방법으로 역전파될 때 일정 임계값을 넘어서지 못하게 그레이디언트를 잘래내는 <strong>그레이디언트 클리핑</strong>이 있다.</p>
<ul>
  <li>순환신경망에서 배치정규화를 사용하지 못할 때 유용</li>
  <li>케라스에서 그레이디언트 클리핑을 구현하려면 옵티마이저를 만들 때 <code class="language-plaintext highlighter-rouge">clipvalue</code>와 <code class="language-plaintext highlighter-rouge">clipnorm</code>매개변수를 지정하면 된다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">clipvalue</code> : 그레이디언트 벡터 방향을 바꿀 수 있다.</li>
      <li><code class="language-plaintext highlighter-rouge">clipnorm</code> : 그레이디언트 벡터 방향을 바꾸지 못한다.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tensorflow</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">clipvalue</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="c1"># 손실의 모든 편미분 값을 -1.0에서 1.0으로 잘라낸다.
</span><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="112-사전훈련된-층-재사용하기">11.2 사전훈련된 층 재사용하기</h1>

<p><strong>전이 학습(Transfer Learning)</strong>은 해결하려는 것과 비슷한 유형의 문제를 처리한 신경망이 이미 있는지 찾아본 다음, 그 신경망의 하위층을 재사용하는 방법을 말한다.</p>
<ul>
  <li>훈련 속도를 크게 높이고 필요한 훈련 데이터도 크게 줄여준다.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/78655692/148234268-c7aa8781-0f7e-401f-9baf-e188b9c3a182.png" height="500px" width="500px" /></p>

<blockquote>
  <p>만약 원래 문제에서 사용한 것과 크기가 다른 이미지를 입력으로 사용한다면 원본 모델에 맞는 크기로 변경하는 전처리 단계를 추가해야 한다. 일반적으로 전이 학습은 저수준 특성이 비슷한 입력에서 잘 작동한다.</p>
</blockquote>

<p>전이 학습 시 보통 원본 모델의 출력층을 바꿔야 한다.</p>
<ul>
  <li>이 층이 새로운 작업에 가장 유용하지 않는 층이고 새로운 작업에 필요한 출력 개수와 맞지 않을 수도 있다.</li>
  <li>원본 모델의 하위 은닉층이 훨씬 유용함</li>
</ul>

<p><strong>전이학습 방법</strong><br /></p>
<ul>
  <li>재사용하는 층을 모두 동결(경사 하강법으로 가중치가 바뀌지 않도록 훈련되지 않는 가중치로 만듦)</li>
  <li>모델을 훈련하고 성능을 평가</li>
  <li>맨 위에 있는 한두개의 은닉층의 동결을 해제하고 역전파를 통해 가중치를 조정하여 성능이 향상되는지 확인
    <ul>
      <li>훈련 데이터가 많을수록 많은 층의 동결 해제 가능</li>
      <li>재사용 층의 동결을 해제할 때는 학습률을 줄이는 것이 효율적이며 가중치를 세밀하게 튜닝하는 데 도움을 줌</li>
    </ul>
  </li>
  <li>성능이 좋아지지 않거나 훈련 데이터가 적을 경우 상위 은닉층 제거 후 남은 은닉층을 다시 동결하고 훈련
    <ul>
      <li>훈련 데이터가 아주 많은 경우 은닉층을 제거하기보다는 다른 것으로 바꾸거나 더 많은 은닉층을 추가</li>
    </ul>
  </li>
</ul>

<h2 id="1121-케라스를-사용한-전이-학습">11.2.1 케라스를 사용한 전이 학습</h2>

<ul>
  <li>먼저 모델 A를 로드하고 이 모델의 층을 기반으로 새로운 모델을 만든다.</li>
  <li>가정: model_A가 존재
    <ul>
      <li>샌들과 셔츠를 제외한 8개의 클래스만 담겨 있는 패션 MNIST 활용하여 학습한 모델</li>
    </ul>
  </li>
  <li>문제점: 셔츠와 샌들을 분류하는 이진 분류기 model_B 훈련을 하려고 하지만 데이터가 매우 적음</li>
  <li>목표: 전이학습을 이용한 model_B_on_A 훈련
    <ul>
      <li>출력층만 제외하고 모든 층을 재사용</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span><span class="p">,</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="n">model_A</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s">"my_model_A.h5"</span><span class="p">)</span>
<span class="n">model_B_on_A</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">model_A</span><span class="p">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">model_B_on_A</span> <span class="o">=</span> <span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"sigmoid"</span><span class="p">))</span>
</code></pre></div></div>

<p>위의 코드는 model_B_on_A를 훈련할 때 model_A도 영향을 받는다. 이를 원하지 않는 경우 model_A를 클론한다.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">clone_model()</code> 메서드로 모델 A의 구조를 복제한 후 가중치를 복사(가중치는 별도로 복사가 필요)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">clone_model</span>

<span class="n">model_A_clone</span> <span class="o">=</span> <span class="n">clone_model</span><span class="p">(</span><span class="n">model_A</span><span class="p">)</span>
<span class="n">model_A_clone</span><span class="p">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">model_A</span><span class="p">.</span><span class="n">get_weights</span><span class="p">())</span>
</code></pre></div></div>

<ul>
  <li>새로운 출력층이 랜덤하게 초기화되어 있으므로 큰 오차를 만들 것이다. 이를 피하기 위해서 처음 몇 번의 에포크 동안 재사용된 층을 동결하고 새로운 층에게 적절한 가중치를 학습할 시간을 준다.
    <ul>
      <li>모든 층의 trainable 속성을 False로 지정하고 모델을 컴파일한다.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>층을 동결하거나 동결을 해제한 후 반드시 모델을 컴파일해야 한다.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="c1"># 재사용 층 동결
</span><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model_B_on_A</span><span class="p">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
  <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">model_B_on_A</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"binary_crossentropy"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">"sgd"</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">])</span>

<span class="c1"># 큰 오차를 피하기 위한 몇 번의 에포크 동안 모델 훈련
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model_B_on_A</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_B</span><span class="p">,</span> <span class="n">y_train_B</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid_B</span><span class="p">,</span> <span class="n">y_valid_B</span><span class="p">))</span>

<span class="c1"># 재사용 층 동결해제
</span><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model_B_on_A</span><span class="p">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
  <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>  <span class="c1"># 기본 학습률은 1e-2
</span>
<span class="n">model_B_on_A</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"binary_crossentropy"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">])</span>
  
<span class="c1"># 모델 학습
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model_B_on_A</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_B</span><span class="p">,</span> <span class="n">y_train_B</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid_B</span><span class="p">,</span> <span class="n">y_valid_B</span><span class="p">))</span>

<span class="c1"># 모델 평가
</span><span class="n">model_B_on_A</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test_B</span><span class="p">,</span> <span class="n">y_test_B</span><span class="p">)</span>
<span class="c1"># [0.06887910133600235, 0.9925]
</span></code></pre></div></div>

<ul>
  <li>얕은 신경망 모델에서는 전이학습 성능이 좋지 않다.</li>
  <li>전이 학습은 조금 더 일반적인 특성을 감지하는 경향이 있는 심층 합성곱 신경망에서 잘 동작한다.</li>
</ul>

<h2 id="1122-비지도-사전훈련">11.2.2 비지도 사전훈련</h2>

<p>레이블된 훈련 데이터가 많지 않은 복잡한 문제가 있는데, 이 작업에 대해 훈련된 모델을 핮을 수 없을 때 <strong>비지도 사전훈련</strong>을 수행하여 문제를 해결할 수 있다.</p>
<ul>
  <li>레이블되지 않은 훈련 데이터를 많이 모을 수 있는 경우에 오토인코더(autoencoder)나 생성적 적대 신경망과 같은 비지도 학습 모델을 훈련
    <ul>
      <li>오토인코더나 GAN 판별자의 하위층을 재사용하고 그 위에 새로운 작업에 맞는 출력증을 추가한 후 지도 학습으로 최종 네트워크를 세밀하게 튜닝</li>
    </ul>
  </li>
</ul>

<h2 id="1123-보조-작업에서-사전훈련">11.2.3 보조 작업에서 사전훈련</h2>

<p>레이블된 훈련 데이터가 많지 않는 경우에 또 다른 방법으로는 레이블된 훈련 데이터를 쉽게 얻거나 생성할 수 있는 보조 작업에서 첫 번째 신경망을 훈련하는 것이다.</p>
<ul>
  <li>이 신경망의 하위층을 실제 작업을 위해 재사용한다.</li>
  <li>첫 번째 신경망의 하위층은 두 번째 신경망에 재사용될 수 있는 특성 추출기를 학습하게 된다.</li>
</ul>

<blockquote>
  <p><strong>자기 지도 학습</strong>은 데이터에서 스스로 레이블을 생성하고 지도 학습 기법으로 레이블된 데이터셋에서 모델을 훈련하는 방법으로 사람이 레이블을 부여할 필요가 없어 비지도 학습의 형태로 분류된다.</p>
</blockquote>

        
      </section>

      <footer class="page__meta">
        
        


  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#%ED%95%B8%EC%A6%88%EC%98%A8-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D" class="page__taxonomy-item" rel="tag">핸즈온 머신러닝</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2025-03-07">March 7, 2025</time></p>


      </footer>

      

      
  <nav class="pagination">
    
      <a href="/hands-on/ANN-3/" class="pagination--pager" title="10장 케라스를 사용한 인공 신경망 소개(3)
">Prev</a>
    
    
      <a href="/hands-on/DNN-2/" class="pagination--pager" title="11장 케라스를 사용한 인공 신경망 소개(2)
">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/basic-2/" rel="permalink">파이토치 기초(2)
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2025-04-30T00:00:00+09:00">April 30, 2025</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">데이터 세트와 데이터 로더 / 모델 &amp; 데이터세트 분리 / 모델 저장 및 불러오기
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/rnn/" rel="permalink">순환 신경망
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2025-04-17T00:00:00+09:00">April 17, 2025</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">RNN / LSTM
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/cnn/" rel="permalink">합성곱 신경망
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2025-04-16T00:00:00+09:00">April 16, 2025</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">CNN / 완전 연결 계층 / 1차원 합성곱 &amp; 자연어 처리
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/embedding-2/" rel="permalink">임베딩(2)
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2025-04-05T00:00:00+09:00">April 5, 2025</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Word2Vec / fastText
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 Seojin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
