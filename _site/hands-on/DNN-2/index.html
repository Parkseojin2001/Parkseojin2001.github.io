<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.22.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <style> 
      ::-webkit-scrollbar{ 
        width: 10px;
        height: 10px;
      }

      ::-webkit-scrollbar-track {
        width: 0px;
        background-color: rgba(224, 224, 224, 0.2);
        /* border-radius: 5px; */
      }

      ::-webkit-scrollbar-thumb {
        width: 0px;
        background-color: rgba(220, 219, 218, 0.6);
        border-radius: 5px;
      }

      ::-webkit-scrollbar-thumb:hover {
        width: 10px;
        height: 20px;
        /* background-color: rgba(190, 190, 190, 0.2); */
        background-color: rgba(193, 192, 191, 0.7);
        border-radius: 5px;
      }

      ::-webkit-scrollbar-track:hover {
        width: 10px;
        /* background-color: rgba(150, 150, 150, 0.1); */
        background-color: rgba(224, 224, 224, 0.5);
        border-radius: 5px;
        /* background: transparent; */
        /* border-radius: 10px; */
      }

      ::-webkit-scrollbar-button:start:decrement,::-webkit-scrollbar-button:end:increment {
          width:0px;
          height: 0px;
          /* background-color: rgb(14, 221, 24); */
          /* border-radius: 50%; */
      }
    </style>
    
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>11장 케라스를 사용한 인공 신경망 소개(2) | Seojin</title>
<meta name="description" content="옵티마이저/스케줄러/과대적합">


  <meta name="author" content="Seojin">
  
  <meta property="article:author" content="Seojin">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Seojin Devlog">
<meta property="og:title" content="11장 케라스를 사용한 인공 신경망 소개(2)">
<meta property="og:url" content="http://localhost:4000/hands-on/DNN-2/">


  <meta property="og:description" content="옵티마이저/스케줄러/과대적합">







  <meta property="article:published_time" content="2025-03-07T00:00:00+09:00">



  <meta property="article:modified_time" content="2025-03-07T00:00:00+09:00">



  

  


<link rel="canonical" href="http://localhost:4000/hands-on/DNN-2/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Seojin",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Seojin Devlog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      processEscapes: true
    }
  });
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->


    <link rel="apple-touch-icon" sizes="180x180" href="https://Parkseojin2001.github.io/assets/images/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://Parkseojin2001.github.io/assets/images/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://Parkseojin2001.github.io/assets/images/favicon/favicon-16x16.png">
    <link rel="manifest" href="https://Parkseojin2001.github.io/assets/images/favicon/site.webmanifest">
    <link rel="mask-icon" href="https://Parkseojin2001.github.io/assets/images/favicon/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#ffc40d">
    <meta name="theme-color" content="#ffffff">
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Seojin Devlog
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="https://Parkseojin2001.github.io/">Home</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="https://github.com/Parkseojin2001">GitHub</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/meee.png" alt="Seojin" itemprop="image">
      
    </div>
  

  <!-- 2022.02.17 author content hidden -->
  <!-- <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Seojin</h3>
    
    
  </div> -->

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">Follow</button> -->
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle menu</label>
  <ul class="nav__items">
    <!-- <li>
      <span class="nav__total">🌴 Total Posts: 15</span>
    </li> -->
    
    
      <li>
        
          <!-- title -->
              <span class="nav__sub-title">COMPUTER SCIENCE</span>
              <hr>
        

        
        <ul>
          
            <!-- sub-title -->
            
              
            
              
                <li><a href="/categories/data-structure/">Data Structure (4)</a></li>
              
            
              
            
              
            
              
            
              
            
              
            
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
                <li><a href="/categories/algorithm/">Algorithm (1)</a></li>
              
            
              
            
              
            
          
            <!-- sub-title -->
            
              
                <li><a href="/categories/os/">Operating System (1)</a></li>
              
            
              
            
              
            
              
            
              
            
              
            
              
            
          
        </ul>
        
      </li>
    
      <li>
        
          <!-- title -->
              <span class="nav__sub-title">AI</span>
              <hr>
        

        
        <ul>
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
          
            <!-- sub-title -->
            
              
            
              
            
              
                <li><a href="/categories/hands-on/">핸즈온 머신러닝 (5)</a></li>
              
            
              
            
              
            
              
            
              
            
          
        </ul>
        
      </li>
    
      <li>
        
          <!-- title -->
              <span class="nav__sub-title">PROGRAMMING</span>
              <hr>
        

        
        <ul>
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
                <li><a href="/categories/python/">Python (2)</a></li>
              
            
              
            
              
            
              
            
          
        </ul>
        
      </li>
    
      <li>
        
          <!-- title -->
              <span class="nav__sub-title">PROJECT</span>
              <hr>
        

        
        <ul>
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
            
              
                <li><a href="/categories/capstone-design/">Capstone Design (1)</a></li>
              
            
          
        </ul>
        
      </li>
    
      <li>
        
          <!-- title -->
              <span class="nav__sub-title">GIT</span>
              <hr>
        

        
        <ul>
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
                <li><a href="/categories/git/">Git (1)</a></li>
              
            
              
            
          
            <!-- sub-title -->
            
              
            
              
            
              
            
              
            
              
            
              
            
              
            
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="11장 케라스를 사용한 인공 신경망 소개(2)">
    <meta itemprop="description" content="옵티마이저/스케줄러/과대적합">
    <meta itemprop="datePublished" content="2025-03-07T00:00:00+09:00">
    <meta itemprop="dateModified" content="2025-03-07T00:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">11장 케라스를 사용한 인공 신경망 소개(2)
</h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2025-03-07T00:00:00+09:00">March 7, 2025</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#113-고속-옵티마이저">11.3 고속 옵티마이저</a><ul><li><a href="#1131-모멘텀-최적화">11.3.1 모멘텀 최적화</a></li><li><a href="#1132-네스테로프-가속-경사">11.3.2 네스테로프 가속 경사</a></li><li><a href="#1133-adagrad">11.3.3 AdaGrad</a></li><li><a href="#1134-rmsprop">11.3.4 RMSProp</a></li><li><a href="#1135-adam과-nadam-최적화">11.3.5 Adam과 Nadam 최적화</a></li><li><a href="#1136-학습률-스케줄링">11.3.6 학습률 스케줄링</a></li></ul></li><li><a href="#114-규제를-사용해-과대적합-피하기">11.4 규제를 사용해 과대적합 피하기</a><ul><li><a href="#1141--l_1-과--l_2-규제">11.4.1 $\ l_1$ 과 $\ l_2$ 규제</a></li><li><a href="#1142-드롭아웃">11.4.2 드롭아웃</a></li><li><a href="#1143-몬테-카를로-드롭아웃">11.4.3 몬테 카를로 드롭아웃</a></li><li><a href="#1144-맥스-노름-규제">11.4.4 맥스-노름 규제</a></li></ul></li><li><a href="#115-요약-및-실용적인-가이드라인">11.5 요약 및 실용적인 가이드라인</a></li></ul>

            </nav>
          </aside>
        
        <h1 id="113-고속-옵티마이저">11.3 고속 옵티마이저</h1>

<p><strong>훈련 속도를 높이는 방법</strong></p>
<ul>
  <li>연결 가중치에 좋은 초기화 전략 사용하기</li>
  <li>좋은 활성화 함수 사용하기</li>
  <li>배치 정규화 사용하기</li>
  <li>사전훈련 네트워크 일부 재사용</li>
</ul>

<p>훈련 속도를 크게 높일 수 있는 또 다른 방법으로 표준적인 경사 하강법 옵티마이저 대신 더 빠른 옵티마이저를 사용할 수 있다.</p>
<ul>
  <li>모멘텀 최적화</li>
  <li>네스테로프(Nesterov) 가속 경사(NAG)</li>
  <li>AdaGrad</li>
  <li>RMSProp</li>
  <li>Adam 최적화</li>
  <li>Nadam 최적화</li>
</ul>

<p><img src="https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fd72c4064-00fd-4e81-984e-cf5f4a98340e%2FUntitled.png&amp;blockId=1248bc15-77fa-4ba8-954c-a57962abe99f" /></p>

<h2 id="1131-모멘텀-최적화">11.3.1 모멘텀 최적화</h2>
<p>표준적인 경사 하강법은 경사면을 따라 일정한 크기의 스텝으로 조금씩 내려간다.</p>
<ul>
  <li>경사 하강법 공식 : $\theta \leftarrow \theta - \eta \nabla_\theta J(\theta)$
    <ul>
      <li>$\theta$ : 가중치</li>
      <li>$\eta$ : 학습률</li>
      <li>$J(\theta)$ : 가중치에 대한 비용 함수</li>
    </ul>
  </li>
  <li>이 식은 이전 그레이디언트가 얼마였는지 고려하지 않는다.</li>
</ul>

<p>모멘텀 최적화는 이전 그레이디언트가 얼마였는지를 상당히 중요하게 생각한다.</p>
<ul>
  <li>매 반복에서 현재 그레이디언트를 (학습률 $\eta$를 곱한 후) <strong>모멘텀 벡터 m</strong>에 더하고 이 값을 빼는 방식으로 가중치를 갱신한다.
    <ul>
      <li>그레이디언트를 가속도롤 사용한다.</li>
      <li>일종의 마찰저항을 표현하고 모멘텀이 너무 커지는 것을 막기 위해 <strong>모멘텀</strong>이라는 새로운 하이퍼파라미터 $\beta$를 사용
        <ul>
          <li>0(높은 마찰 저항)과 1(마찰 저항 없음) 사이로 설정되어야 한다. (default = 0.9)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>모멘텀 알고리즘</strong><br /></p>

<ol>
  <li>$m \leftarrow \beta m - eta \nabla_\theta J(\theta)$</li>
  <li>$\theta \leftarrow \theta + m$</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/78655692/147542303-8f8b631e-b95f-4e35-9deb-b524c3f03948.png" /></p>

<ul>
  <li>모멘텀 최적화는 골짜기를 따라 바닥(최적점)에 도달할 때까지 점점 더 빠르게 내려간다.</li>
  <li>모멘텀 최적화를 사용하면 지역 최적점(local optimia)을 건너 뛰는데 도움을 준다.</li>
</ul>

<blockquote>
  <p>모멘텀떄문에 옵티마이저가 최적값에 안정되기 전까지 건너뛰었다가 다시 돌아오고, 다시 또 건너뛰는 식으로 여러 번 왔다 갔다 할 수 있다.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="1132-네스테로프-가속-경사">11.3.2 네스테로프 가속 경사</h2>

<p>네스테로프 가속 경사(NAG: Nesterov accelerated gradient)는 현재 위치가 $\theta$가 아니라 모멘텀의 방향으로 조금 앞선 $\theta + \beta m$에서 비용 함수의 그레이디언트를 계산한다.</p>
<ul>
  <li>모멘텀 최적화의 변종으로 거의 항상 빠르다.</li>
</ul>

<p><strong>네스테로프 가속 경사 알고리즘</strong><br /></p>

<ol>
  <li>$m \leftarrow \beta m - \eta \nabla_\theta J(\theta + \beta m)$</li>
  <li>$\theta \leftarrow \theta + m$</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/78655692/147552717-982cc6ea-1e0f-41fc-82a7-a94e1d5a8ea1.png" height="600px" width="500px" /></p>

<ul>
  <li>기본 모멘텀 최적화는 시작점에서 그레이디언트를 사용하고 네스테로프 가속 경사는 그 방향으로 조금 더 나아가서($\bata m$) 그레이디언트를 측정하고 이것을 사용한다.
    <ul>
      <li>모멘텀은 골짜기를 가로지르는($\eta \nabla_1$) 반면에 NAG는 계곡의 아래쪽($\eta \nabla_2$)으로 잡아당기게 되며 이는 진동을 감소시키고 수렴을 빠르게 만들어준다.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="1133-adagrad">11.3.3 AdaGrad</h2>

<p>경사 하강법은 전역 최적점 방향으로 곧장 향하지 않고 가장 가파른 경사를 따라 빠르게 내려가기 시작해서 골짜기 아래로 느리게 이동한다. 이 문제를 보완하기 위해서 <code class="language-plaintext highlighter-rouge">AdaGrad</code> 알고리즘이 고안되었다.</p>
<ul>
  <li>가장 가파른 차원을 따라 그레이디언트 벡터의 스케일이 감소된다.</li>
</ul>

<p><strong>AdaGrad 알고리즘</strong><br /></p>
<ol>
  <li>$s \leftarrow s + \nabla_\theta J(\theta) \otimes \nabla_\theta J(\theta)$</li>
  <li>$\theta \leftarrow \theta - \eta \nabla_\theta J(\theta) \oslash \sqrt{s + \epsilon}$</li>
</ol>

<p>이 알고리즘은 학습률을 감소시키지만 경사가 완만한 차원보다 가파른 차원에 대해 더 빠르게 감소되며 이를 <strong>적응적 학습률</strong>이라고 부른다.</p>

<p><img src="https://user-images.githubusercontent.com/78655692/147553505-2a077b07-0fa7-4769-ba10-24b1a359d933.png" height="350px" width="500px" /></p>

<p>장점<br /></p>
<ul>
  <li>전역 최적점 방향으로 더 곧장 가도록 갱신되는 데 도움이 된다.</li>
  <li>학습률 하이퍼파라미터 $\eta$를 덜 튜닝해도 된다.</li>
  <li>2차방정식 문제에 잘 작동한다.</li>
</ul>

<p>단점<br /></p>
<ul>
  <li>학습률이 너무 감소되어 전역 최적점에 도착하기 전에 알고리즘이 완전히 멈춘다.</li>
  <li>심층 신경망에는 사용하지 않아야 하며 선형 회귀 같은 간단한 작업에는 효과적이다.</li>
</ul>

<h2 id="1134-rmsprop">11.3.4 RMSProp</h2>

<p>AdaGrad는 너무 빨라 느려져서 전역 최적점에 수렴하지 못하는 위험을 <code class="language-plaintext highlighter-rouge">RMSProp</code> 알고리즘으로 가장 최근 반복에서 비롯된 그레이디언트만 누적함으로써 이 문제를 해결했다.</p>
<ul>
  <li>알고리즘의 첫 번째 단계에서 지수 감소를 사용한다.</li>
</ul>

<p><strong>RMSProp 알고리즘</strong></p>
<ol>
  <li>$s \leftarrow \beta s + (1 - \beta)\nabla_\theta J(\theta) \otimes \nabla_\theta J(\theta)$</li>
  <li>$\theta \leftarrow \theta - \eta \nabla_\theta J(\theta) \oslash \sqrt{s + \epsilon}$</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">RMSProp</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">RMSProp</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span> <span class="c1"># rho는 베터에 해당(default=0.9)
</span></code></pre></div></div>
<ul>
  <li>간단한 문제를 제외하고는 RMSProp 옵티마이저가 훨씬 성능이 좋았으며 Adam 최적화가 나오기 전까지 가장 선호하는 최적화 알고리즘이었다.</li>
</ul>

<h2 id="1135-adam과-nadam-최적화">11.3.5 Adam과 Nadam 최적화</h2>

<p><code class="language-plaintext highlighter-rouge">Adam</code>은 <strong>적응적 모멘트 추정</strong>을 의미한다.</p>
<ul>
  <li>모멘트 최적화(지난 그레이디언트의 지수 감소 평균을 따름)와 RMSProp(지난 그레이디언트 제곱의 지수 감소된 평균을 따름)의 아이디어를 합침</li>
</ul>

<p><strong>Adam 알고리즘</strong><br /></p>
<ol>
  <li>$m \leftarrow \beta_1 m - (1 - \beta_1)\nabla_\theta J(\theta)$</li>
  <li>$s \leftarrow \beta_2 s + (1 - \beta_2)\nabla_\theta J(\theta) \otimes \nabla_\theta J(\theta)$</li>
  <li>$\hat m \leftarrow \frac{m}{1 - \beta_1^t}$</li>
  <li>$\hat s \leftarrow \frac{s}{1 - \beta_2^t}$</li>
  <li>$\theta \leftarrow \theta + \eta \hat m \oslash \sqrt{\hat s + \epsilon}$
    <ul>
      <li>$\beta_1$ : 모멘텀 감쇠 하이퍼파라미터(default=0.9)</li>
      <li>$\beta_2$ : 스케일 감쇠 하이퍼파라미터(default=0.999)</li>
    </ul>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>Adam이 적응적 학습률 알고리즘이기 때문에 학습률 파라미터 $\eta$(default=0.001)를 튜닝할 필요가 적다.</li>
</ul>

<p><strong>Adam 변종</strong><br /></p>
<ul>
  <li>AdamMax
    <ul>
      <li>실전에서 AdaMax가 Adam보다 안정적이지만 데이터셋에 따라 다르고 일반적으로 Adam 성능이 낫다.</li>
      <li>어떤 작업에서 Adam이 잘 동작하지 않는다면 시도할 수 있는 옵티마이저 중 하나</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adamax</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adamax</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Nadam
    <ul>
      <li>Adam 옵티마이저 + 네스테로프 기법으로 종종 Adam보다 조금 더 빠르게 수렴</li>
      <li>일반적으로 Adam보다 성능이 좋지만 경우에 따라 RMSProp이 더 좋기도 함</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adamax</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adamax</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>적응적 최적화 방법(RMSProp, Adam, Nadam)이 좋은 솔루션으로 빠르게 수렴하지만 일부 데이터셋에서 나뿐 결과를 낸다. 이런 경우에는 기본 네스테로프 가속 경사를 사용할 수 있다.</p>
</blockquote>

<p>위의 모든 최적화 기법은 <strong>1차 편미분(야코비안)</strong>에만 의존한다. <strong>2차 편미분(헤시안)</strong>을 기반으로 한 알고리즘이 있지만 메모리 용량을 넘어서는 문제와 계산 속도가 느리기때문에 심층 신경망에 적용하기 어렵다.</p>

<p><strong>옵티마이저 비교</strong>(<code class="language-plaintext highlighter-rouge">*</code> = 나쁨, <code class="language-plaintext highlighter-rouge">**</code> = 보통, <code class="language-plaintext highlighter-rouge">***</code> = 좋음)<br /></p>
<ul>
  <li>선택한 옵티마이저의 성능이 만족스럽지 않을 경우 기본 <code class="language-plaintext highlighter-rouge">Nesterov</code> 가속 경사 사용 추천</li>
</ul>

<table>
  <thead>
    <tr>
      <th>클래스</th>
      <th>수렴 속도</th>
      <th>수렴 품질</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SGD</td>
      <td><code class="language-plaintext highlighter-rouge">*</code></td>
      <td><code class="language-plaintext highlighter-rouge">***</code></td>
    </tr>
    <tr>
      <td>SGD(momentum=…)</td>
      <td><code class="language-plaintext highlighter-rouge">**</code></td>
      <td><code class="language-plaintext highlighter-rouge">***</code></td>
    </tr>
    <tr>
      <td>SGD(momentum=…, nesterov=True)</td>
      <td><code class="language-plaintext highlighter-rouge">**</code></td>
      <td><code class="language-plaintext highlighter-rouge">***</code></td>
    </tr>
    <tr>
      <td>Adagrad</td>
      <td><code class="language-plaintext highlighter-rouge">***</code></td>
      <td><code class="language-plaintext highlighter-rouge">*</code>(너무 일찍 멈춤)</td>
    </tr>
    <tr>
      <td>RMSprop</td>
      <td><code class="language-plaintext highlighter-rouge">***</code></td>
      <td><code class="language-plaintext highlighter-rouge">**</code> 또는 <code class="language-plaintext highlighter-rouge">***</code></td>
    </tr>
    <tr>
      <td>Adam</td>
      <td><code class="language-plaintext highlighter-rouge">***</code></td>
      <td><code class="language-plaintext highlighter-rouge">**</code> 또는 <code class="language-plaintext highlighter-rouge">***</code></td>
    </tr>
    <tr>
      <td>Nadam</td>
      <td><code class="language-plaintext highlighter-rouge">***</code></td>
      <td><code class="language-plaintext highlighter-rouge">**</code> 또는 <code class="language-plaintext highlighter-rouge">***</code></td>
    </tr>
    <tr>
      <td>AdaMax</td>
      <td><code class="language-plaintext highlighter-rouge">***</code></td>
      <td><code class="language-plaintext highlighter-rouge">**</code> 또는 <code class="language-plaintext highlighter-rouge">***</code></td>
    </tr>
  </tbody>
</table>

<h2 id="1136-학습률-스케줄링">11.3.6 학습률 스케줄링</h2>

<p>학습률 선택이 훈련의 성패를 가른다.</p>

<p><img src="https://user-images.githubusercontent.com/78655692/148238088-308988e0-92f9-495c-90db-8e3da6d2ea6f.png" /></p>

<p>큰 학습률로 시작하고 학습 속도가 느려질 때 학습률을 낮추면 최적의 고정 학습률보다 좋은 솔루션을 더 빨리 발견할 수 있다. 훈련하는 동안 학습률을 감소시키는 전략 중 <strong>학습 스케줄</strong>이 있다.</p>

<ul>
  <li>거듭제곱 기반 스케줄링
    <ul>
      <li>학습률을 반복 횟수에 t에 대한 함수 $\eta(t) = \eta_0 / {(1 + t/s)^c}$로 지정</li>
      <li>$t = k \cdot s$로 커지면 학습률이 $\frac{\eta_0}{k + 1}$로 줄어듦.</li>
      <li>하이퍼파라미터
        <ul>
          <li>$\eta_0$ : 초기 학습률</li>
          <li>$c$ : 거듭제곱수, 일반적으로 1로 지정</li>
          <li>$s$ : 스텝 횟수</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>지수 기반 스케줄링
    <ul>
      <li>$\eta(t) = \eta_0 \ 0.1^{t/s}$</li>
      <li>학습률이 $s$ 스텝마다 10배씩 점차 줄어든다.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">exponential_decay_fn</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span> <span class="c1"># 현재 에포크의 학습률을 받아 반환하는 함수 필요
</span>    <span class="k">return</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">**</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">/</span> <span class="mi">20</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">exponential_decay_fn</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>  <span class="c1"># 현재 학습률을 매개변수로 받음
</span>    <span class="k">return</span> <span class="n">lr</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">20</span><span class="p">)</span>

<span class="c1"># 변수를 설정한 클로저를 반환하는 방식
</span><span class="k">def</span> <span class="nf">exponential_decay</span><span class="p">(</span><span class="n">lr0</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">exponential_decay_fn</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">lr0</span> <span class="o">*</span> <span class="mf">0.1</span><span class="o">**</span><span class="p">(</span><span class="n">epoch</span> <span class="o">/</span> <span class="n">s</span><span class="p">)</span>

<span class="n">exponential_decay_fn</span> <span class="o">=</span> <span class="n">exponential_decay</span><span class="p">(</span><span class="n">lr0</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div></div>

<p>스케줄링 함수를 전달하여 <code class="language-plaintext highlighter-rouge">LearningRateScheduler</code> 콜백을 만들고 이 콜백을 <code class="language-plaintext highlighter-rouge">fit()</code> 메서드에 전달한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">LearningRateScheduler</span>

<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">LearningRateScheduler</span><span class="p">(</span><span class="n">exponential_decay_fn</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="p">[...],</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lr_scheduler</span><span class="p">])</span>
</code></pre></div></div>

<ul>
  <li>구간별 고정 스케줄링
    <ul>
      <li>일정 횟수의 에포크 동안 일정한 학습률을 사용하고 그 다음 또 다른 횟수의 에포크 동안 작은 학습률을 사용한다.</li>
      <li>학습률</li>
    </ul>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">piecewise_constant_fn</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>
      <span class="k">return</span> <span class="mf">0.01</span>
  <span class="k">elif</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">15</span><span class="p">:</span>
      <span class="k">return</span> <span class="mf">0.005</span>
  <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="mf">0.001</span>
</code></pre></div>    </div>

    <ul>
      <li>콜백함수 지정</li>
    </ul>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">LearningRateScheduler</span>

<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">LearningRateScheduler</span><span class="p">(</span><span class="n">piecewise_contant_fn</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid_scaled</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lr_scheduler</span><span class="p">])</span>
</code></pre></div>    </div>
  </li>
  <li>성능 기반 스케줄링
    <ul>
      <li>매 $N$ 스텝마다 검증 오차를 측정하고 오차가 줄어들지 않으면 $\lambda$배만큼 학습률을 감소시킨다.</li>
      <li><code class="language-plaintext highlighter-rouge">ReduceLROnPlateau</code> 콜백 클래스 활용</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">ReduceLROnPlateau</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.schedules</span> <span class="kn">import</span> <span class="n">ExponentialDecay</span>
<span class="kn">from</span> <span class="nn">tensorflow.kear.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">s</span> <span class="o">=</span> <span class="mi">20</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">//</span> <span class="mi">32</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="n">ExponentialDecay</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>1 사이클 스케줄링
    <ul>
      <li>학습률을 훈련 과정 중에 올리거나 내리도록 조정
        <ul>
          <li>훈련 전반부: 낮은 학습률 $\eta_0$에서 높은 학습률 $\eta_1$까지 선형적으로 높힘</li>
          <li>훈련 후반부: 다시 선형적으로 $\eta_0$까지 낮춤</li>
          <li>훈련 마지막 몇 번의 에포크: 학습률을 소수점 몇 자리까지 선형적으로 낮춤</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>결론</strong><br /></p>
<ul>
  <li>지수 기반 스케줄링, 성능 기반 스케줄링, 1 사이클 스케줄링이 수렴 속도를 크게 높일 수 있다.</li>
  <li>성능 기반과 지수 기반 모두 좋지만 <strong>지수 기반 스케줄링</strong>을 선호
    <ul>
      <li>높은 성능, 튜닝과 구현이 쉬음</li>
    </ul>
  </li>
</ul>

<h1 id="114-규제를-사용해-과대적합-피하기">11.4 규제를 사용해 과대적합 피하기</h1>

<h2 id="1141--l_1-과--l_2-규제">11.4.1 $\ l_1$ 과 $\ l_2$ 규제</h2>

<h2 id="1142-드롭아웃">11.4.2 드롭아웃</h2>

<h2 id="1143-몬테-카를로-드롭아웃">11.4.3 몬테 카를로 드롭아웃</h2>

<h2 id="1144-맥스-노름-규제">11.4.4 맥스-노름 규제</h2>

<h1 id="115-요약-및-실용적인-가이드라인">11.5 요약 및 실용적인 가이드라인</h1>

        
      </section>

      <footer class="page__meta">
        
        


  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#%ED%95%B8%EC%A6%88%EC%98%A8-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D" class="page__taxonomy-item" rel="tag">핸즈온 머신러닝</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2025-03-07">March 7, 2025</time></p>


      </footer>

      

      
  <nav class="pagination">
    
      <a href="/hands-on/DNN-1/" class="pagination--pager" title="11장 케라스를 사용한 인공 신경망 소개(1)
">Prev</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/hands-on/DNN-1/" rel="permalink">11장 케라스를 사용한 인공 신경망 소개(1)
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2025-03-07T00:00:00+09:00">March 7, 2025</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">그레이디언트/전이학습/비지도학습
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/hands-on/ANN-3/" rel="permalink">10장 케라스를 사용한 인공 신경망 소개(3)
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2025-03-06T00:00:00+09:00">March 6, 2025</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">하이퍼파라미터 튜닝
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/capstone-design/firebase/" rel="permalink">Firebase
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2025-03-04T00:00:00+09:00">March 4, 2025</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Firebase를 이용한 모델 배포
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python/grammar/" rel="permalink">Python 문법
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2025-02-26T00:00:00+09:00">February 26, 2025</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">zip( ) / 아스테리스크(*) / itertools
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 Seojin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
