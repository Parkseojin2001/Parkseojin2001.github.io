<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" >
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="임베딩(2)" />
<meta property="og:locale" content="en" />
<meta name="description" content="Word2Vec / fastText" />
<meta property="og:description" content="Word2Vec / fastText" />
<link rel="canonical" href="http://localhost:4000/pytorch-book/nlp/embedding-2/" />
<meta property="og:url" content="http://localhost:4000/pytorch-book/nlp/embedding-2/" />
<meta property="og:site_name" content="Seojin Devlog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-05T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="임베딩(2)" />
<meta name="twitter:site" content="@twitter_username" />
<meta name="google-site-verification" content="WfkV1-iar8_v-Mbi-ytl5RBJhiNOfpUEWrLuOCyThLQ" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-04-10T00:00:00+09:00","datePublished":"2025-04-05T00:00:00+09:00","description":"Word2Vec / fastText","headline":"임베딩(2)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/pytorch-book/nlp/embedding-2/"},"url":"http://localhost:4000/pytorch-book/nlp/embedding-2/"}</script>
<!-- End Jekyll SEO tag -->


  <title>임베딩(2) | Seojin Devlog
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">

  <link rel="manifest" href="/assets/img/favicons/site.webmanifest">

<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Seojin Devlog">
<meta name="application-name" content="Seojin Devlog">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Resource Hints -->
  
    
      
        <link rel="preconnect" href="https://fonts.googleapis.com" >
      
        <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
      
    
      
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      
        <link rel="dns-prefetch" href="https://fonts.gstatic.com" >
      
    
      
        <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      
        <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
      
    
  

  <!-- Bootstrap -->
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css">
  

  <!-- Theme style -->
  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  <!-- Web Font -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css">

  <!-- 3rd-party Dependencies -->

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Image Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css">
  

  <!-- Scripts -->

  <script src="/assets/js/dist/theme.min.js"></script>

  <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js,npm/mermaid@11.4.0/dist/mermaid.min.js"></script>







<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script src="/assets/js/data/mathjax.js"></script>
  <script async src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>


<!-- Pageviews -->

  

  



  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <a class="site-title d-block" href="/">Seojin Devlog</a>
    <p class="site-subtitle fst-italic mb-0">AI 기술을 배우고 코드로 기록하는 학생 개발자입니다</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/Parkseojin2001"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['seojin0510612','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
      

      
        <a
          href="https://www.linkedin.com/in/seojin-park-793a15287/"
          aria-label="linkedin"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-linkedin"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">Home</a>
            </span>

          
        
          
        
          
        
          
            
              <span>임베딩(2)</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link" aria-label="Search">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search id="search" class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  

  
  

  




<!-- return -->










<article class="px-1" data-toc="true">
  <header>
    <h1 data-toc-skip>임베딩(2)</h1>
    
      <p class="post-desc fw-light mb-4">Word2Vec / fastText</p>
    

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1743778800"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Apr  5, 2025
</time>

      </span>

      <!-- lastmod date -->
      
        <span>
          Updated
          <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1744243200"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Apr 10, 2025
</time>

        </span>
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              <a href="https://twitter.com/username">Seojin Park</a>
            
          </em>
        </span>

        <div>
          <!-- pageviews -->
          

          <!-- read time -->
          <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="7479 words"
>
  <em>41 min</em> read</span>

        </div>
      </div>
    </div>
  </header>

  
    <div id="toc-bar" class="d-flex align-items-center justify-content-between invisible">
      <span class="label text-truncate">임베딩(2)</span>
      <button type="button" class="toc-trigger btn me-1">
        <i class="fa-solid fa-list-ul fa-fw"></i>
      </button>
    </div>

    <button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm">
      <span class="label ps-2 pe-1">Contents</span>
      <i class="fa-solid fa-angle-right fa-fw"></i>
    </button>

    <dialog id="toc-popup" class="p-0">
      <div class="header d-flex flex-row align-items-center justify-content-between">
        <div class="label text-truncate py-2 ms-4">임베딩(2)</div>
        <button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75">
          <i class="fas fa-close"></i>
        </button>
      </div>
      <div id="toc-popup-content" class="px-4 py-3 pb-4"></div>
    </dialog>
  

  <div class="content">
    <h2 id="word2vec"><span class="me-2">Word2Vec</span><a href="#word2vec" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<hr />

<p><strong>Word2Vec</strong>은 단어 간의 유사성을 측정하기 위해 분포 가설(distributional hypothesis)을 기반으로 개발된 임베딩 모델이다.</p>

<ul>
  <li>분포 가설: 같은 문맥에서 함께 자주 나타나는 단어들은 서로 유사한 의미를 가질 가능성이 높다는 가정이며 단어 간의 <strong>동시 발생(co-occurrence)</strong> 확률 분포를 이용해 단어 간의 유사성을 측정</li>
</ul>

<p>ex. ‘내일 자동차를 타고 부산에 간다’ 와 ‘내일 비행기를 타고 부산에 간다’ 라는 두 문장에서 ‘자동차’와 ‘비행기’는 주변에 분포한 단어들이 동일하거나 유사하므로 두 단어는 비슷한 의미를 가질 것이라고 예상</p>

<p>가정을 통해 단어의 <strong>분산 표현(Distributed Representation)</strong>을 학습할 수 있다.</p>
<ul>
  <li>분산 표현: 단어를 고차원 벡터 공간에 매핑하여 단어의 의미를 담은 것을 의미</li>
</ul>

<p>분포 가설에 따라 유사한 문맥에서 등장하는 단어는 비슷한 벡터 공간상 위치를 갖게 된다. 위의 예시에서 ‘비행기’와 ‘자동차’는 벡터 공간에서 서로 가까운 위치에 표현된다.</p>

<p>이는 빈도 기반 벡터화 기법에서 발생했던 단어의 의미 정보를 저장하지 못하는 한계를 극복했으며, 대량의 텍스트 데이터에서 단어 간의 관계를 파악하고 벡터 공간상에서 유사한 단어를 군집화해 단어의 의미 정보를 효과적으로 표현한다.</p>

<h3 id="단어-벡터화"><span class="me-2">단어 벡터화</span><a href="#단어-벡터화" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>단어를 벡터화하는 방법은 크게 <strong>희소 표현(sparse representation)</strong>과 <strong>밀집 표현(dense representation)</strong>으로 나눌 수 있다.</p>
<ul>
  <li>희소 표현: 빈도 기반 방법으로 대표적으로는 원-핫 인코딩, TF-IDF가 존재
    <ul>
      <li>대부분의 벡터 요소가 0으로 표현</li>
      <li>단어 사전의 크기가 커지면 벡터의 크기도 커지므로 공간적 낭비가 발생</li>
      <li>단어 간의 유사성을 반영하지 못함</li>
      <li>벡터 간의 유사성을 계산하는 데도 많은 비용이 발생</li>
    </ul>

    <div class="table-wrapper"><table>
      <thead>
        <tr>
          <th>단어</th>
          <th> </th>
          <th> </th>
          <th> </th>
          <th> </th>
          <th> </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>소</td>
          <td>0</td>
          <td>1</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr>
          <td>잃고</td>
          <td>1</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr>
          <td>외양간</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>1</td>
          <td>0</td>
        </tr>
        <tr>
          <td>고친다</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>1</td>
        </tr>
      </tbody>
    </table></div>
  </li>
  <li>밀집 표현: Word2Vec
    <ul>
      <li>단어를 고정된 크기의 실수 벡터로 표현하기 때문에 단어 사전의 크기가 커지더라도 벡터의 크기가 커지지 않음</li>
      <li>벡터 공간상에서 단어 간의 거리를 효과적으로 계산할 수 있으며, 벡터의 대부분이 0이 아닌 실수로 이루어져 있어 효율적으로 공간을 활용</li>
    </ul>

    <div class="table-wrapper"><table>
      <thead>
        <tr>
          <th>단어</th>
          <th> </th>
          <th> </th>
          <th> </th>
          <th> </th>
          <th> </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>소</td>
          <td>0.3914</td>
          <td>-0.1749</td>
          <td>…</td>
          <td>0.5912</td>
          <td>0.1321</td>
        </tr>
        <tr>
          <td>잃고</td>
          <td>-0.2893</td>
          <td>0.3814</td>
          <td>…</td>
          <td>-0.1492</td>
          <td>-0.2814</td>
        </tr>
        <tr>
          <td>외양간</td>
          <td>0.4812</td>
          <td>0.1214</td>
          <td>…</td>
          <td>-0.2745</td>
          <td>0.0132</td>
        </tr>
        <tr>
          <td>고친다</td>
          <td>-0.1314</td>
          <td>-0.2809</td>
          <td>…</td>
          <td>0.2014</td>
          <td>0.3016</td>
        </tr>
      </tbody>
    </table></div>
  </li>
</ul>

<p>밀집 표현 벡터화는 학습을 통해 단어를 벡터화하기 때문에 단어의 의미를 비교할 수 있다. 밀집 표현된 벡터를 <strong>단어 임베딩 벡터(Word Embedding Vector)</strong>라고 하며, Word2Vec은 대표적인 단어 임베딩 기법 중 하나다.</p>

<p>Word2Vec은 밀집 표현을 위해 CBoW와 Skip-gram이라는 두 가지 방법을 사용한다.</p>

<h3 id="cbow"><span class="me-2">CBoW</span><a href="#cbow" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p><strong>CBoW(Continuous Bag of Words)</strong>란 주변에 있는 단어를 가지고 중간에 있는 단어를 예측하는 방법이다.</p>
<ul>
  <li>중심 단어(Center Word): 예측해야 할 단어를 의미</li>
  <li>주변 단어(Context Word): 예측에 사용되는 단어들</li>
</ul>

<p>중심 단어를 맞추기 위해 몇 개의 주변 단어를 고려할지를 정해야 하는데, 이 범위를 <strong>윈도(Window)</strong>라고 한다. 이 윈도를 활용해 주어진 하나의 문장에서 첫 번째 단어부터 중심 단어로 하여 마지막 단어까지 학습한다.</p>
<ul>
  <li>윈도가 N일 때, 범위는 중심 단어의 앞에 위치한 N개의 주변 단어부터 뒤에 위치한 N개의 주변 단어이다.</li>
</ul>

<p>학습을 위해 윈도를 이동해 가며 학습하는데, 이러한 방법을 <strong>슬라이딩 윈도(Sliding Window)</strong>라 한다. CBoW는 슬라이딩 윈도를 사용해 한 번의 학습으로 여러 갱의 중심 단어와 그에 대한 주변 단어를 학습할 수 있다.</p>

<p><a href="https://github.com/user-attachments/assets/2e100fec-e458-42e3-b90d-684f4d3dc1a5" class="popup img-link shimmer"><img src="https://github.com/user-attachments/assets/2e100fec-e458-42e3-b90d-684f4d3dc1a5" loading="lazy"></a></p>

<p>위의 그림은 하나의 입력 문장에서 윈도 크기가 2일 때 학습 데이터가 어떻게 구성되는지를 보여준다.</p>

<p>학습 데이터는 (주변 단어 \ 중심 단어)로 구성된다. 이를 통해 대량의 말뭉치에서 효율적으로 단어의 분산 표현을 학습할 수 있다. 얻어진 학습 데이터는 인공 신경망을 학습하는데 사용된다.</p>

<p><a href="https://github.com/user-attachments/assets/7694ed72-b24f-43d6-a8e7-045c3692ff7c" class="popup img-link shimmer"><img src="https://github.com/user-attachments/assets/7694ed72-b24f-43d6-a8e7-045c3692ff7c" loading="lazy"></a></p>

<p>CBoW 모델은 각 입력 단어의 원-핫 벡터를 입력값으로 받는다. 입력 문장 내 모든 단어의 임베딩 벡터를 평균 내어 중심 단어의 임베딩 벡터를 예측한다.</p>

<ol>
  <li>입력 단어는 원-핫 벡터로 표현돼 <strong>투사층(Projection Layer)</strong>에 입력된다.
    <ul>
      <li>투사층: 원-핫 벡터의 인덱스에 해당하는 임베딩 벡터를 반환하는 <strong>순람표(Lookup table, LUT)</strong> 구조</li>
    </ul>
  </li>
  <li>투사층을 통과하면 각 단어는 E 크기의 임베딩 벡터로 변환한다.
    <ul>
      <li>입력된 임베딩 벡터 $V_1, V_2, … , V_n$들의 평균값을 계산</li>
    </ul>
  </li>
  <li>계산된 평균 벡터를 가중치 행렬 $W’_{E \times V}$와 곱하면 $V$ 크기의 벡터를 얻는다.</li>
  <li>소프트맥스 함수를 이용해 중심 단어를 예측한다.</li>
</ol>

<h3 id="skip-gram"><span class="me-2">Skip-gram</span><a href="#skip-gram" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p><strong>Skip-gram</strong>은 CBoW와 반대로 중심 단어를 입력으로 받아서 주변 단어를 예측하는 모델이다.</p>
<ul>
  <li>중심 단어를 기준으로 양쪽으로 윈도 크기만큼의 단어들을 주변 단어로 삼아 훈련 데이터세트를 만든다.</li>
  <li>중심 단어와 각 주변 단어를 하나의 쌍으로 하여 모델을 학습시킨다.</li>
</ul>

<p><a href="https://github.com/user-attachments/assets/9166523b-8ce5-4d64-8380-b7d34c18fc17" class="popup img-link shimmer"><img src="https://github.com/user-attachments/assets/9166523b-8ce5-4d64-8380-b7d34c18fc17" loading="lazy"></a></p>

<p>Skip-gram과 CBoW는 학습 데이터의 구성 방식에 차이가 있다.</p>
<ul>
  <li>CBoW: 하나의 윈도에서 하나의 학습 데이터가 만들어짐</li>
  <li>Skip-gram: 중심 단어와 주변 단어를 하나의 쌍으로 하여 여러 학습 데이터가 만들어짐</li>
</ul>

<p>데이터 구성 방식 차이 때문에 Skip-gram은 하나의 중심 단어를 통해 여러 개의 주변 단어를 예측하므로 <strong>더 많은 학습 데이터세트</strong>를 추출할 수 있으며, 일반적으로 CBoW보다 더 뛰어난 성능을 보인다.</p>

<p>Skip-gram은 비교적 드물게 등장하느 단어를 더 잘 학습할 수 있게 되고 단어 벡터 공간에서 더 유의미한 거리 관계를 형성할 수 있다.</p>

<p><a href="https://github.com/user-attachments/assets/b4e2d048-ebb3-46aa-a444-942abef884b0" class="popup img-link shimmer"><img src="https://github.com/user-attachments/assets/b4e2d048-ebb3-46aa-a444-942abef884b0" loading="lazy"></a></p>

<ol>
  <li>입력 단어의 원-핫 벡터를 투사층에 입력하여 해당 단어의 임베딩 벡터를 가져온다.</li>
  <li>입력 단어의 임베딩과 $W’_{E \times V}$ 가중치와의 곱셈을 통해 $V$ 크기의 벡터를 얻는다.</li>
  <li>$V$ 벡터에 소프트맥스 연산을 취하여 주변 단어를 예측한다.</li>
</ol>

<p>소프트맥스 연산은 모든 단어를 대상으로 내적 연산을 수행한다. 말뭉치의 크기가 커지면 필연적으로 단어 사전의 크기도 커지므로 대량의 말뭉치를 통해 Word2Vec 모델을 학습할 때 학습 속도가 느려지는 단점이 있다.</p>

<p>단점을 보완하는 방법은 계층적 소프트맥스와 네거티브 샘플링 기법을 적용해 학습 속도가 느려지는 문제를 완화할 수 있다.</p>

<h3 id="계층적-소프트맥스"><span class="me-2">계층적 소프트맥스</span><a href="#계층적-소프트맥스" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p><strong>계층적 소프트맥스(Hierachical Softmax)</strong>는 출력층을 이진 트리(Binary tree) 구조로 표현해 연산을 수행한다.</p>
<ul>
  <li>자주 등장하는 단어일수록 트리의 상위 노드에 위치</li>
  <li>드물게 등장하는 단어일수록 하위 노드에 배치</li>
</ul>

<p><a href="https://github.com/user-attachments/assets/17fd491f-1d8c-4c9c-b4d8-3c3d5df6d987" class="popup img-link shimmer"><img src="https://github.com/user-attachments/assets/17fd491f-1d8c-4c9c-b4d8-3c3d5df6d987" loading="lazy"></a></p>

<p>각 노드는 학습이 가능한 벡터를 가지며, 입력값은 해당 노드의 벡터와 내적값을 계산한 후 시그모이드 함수를 통해 확률을 계산한다.</p>

<p><strong>잎 노드(Leaf Node)</strong>는 가장 깊은 노드로, 각 단어를 의미하며, 모델은 각 노드의 벡터를 촤적화하여 단어를 잘 예측할 수 있게 한다. 각 단어의 확률은 경로 노드의 확률을 곱해서 구할 수 있다.</p>

<p>ex. ‘추천해요’ → $0.43 \times 0.74 \times 0.27 = 0.085914$ 의 확률을 갖게 된다. 이 경우 학습 시 1, 2번 노드의 벡터만 최적화하면 된다.</p>

<p>단어 사전 크기를 $V$라고 했을 때 일반적은 소프트맥스 연산은 $O(V)$의 시간 복잡도를 갖지만, 계층적 소프트맥스의 시간 복잡도는 $O(log_2 \ V)$의 시간 복잡도를 갖는다.</p>

<h3 id="네거티브-샘플링"><span class="me-2">네거티브 샘플링</span><a href="#네거티브-샘플링" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p><strong>네거티브 샘플링(Negative Sampling)</strong>은 Word2Vec 모델에서 사용되는 확률적인 샘플링 기법으로 전체 단어 집합에서 일부 단어 샘플링하여 오답 단어로 사용한다.</p>

<p>학습 윈도 내에 등장하지 않는 단어를 n개 추출하여 정답 단어와 함께 소프트맥스 연산을 수행한다. 이를 통해 전체 단어의 확률을 계산할 필요 없이 모델을 효율적으로 학습할 수 있다.</p>
<ul>
  <li>n은 일반적으로 5 ~ 20개를 사용</li>
</ul>

<p>네거티브 샘플링의 추출 확률은 아래 수식을 통해 구할 수 있다.</p>

\[P(w_i) = \frac {f(w_i)^{0.75}}{\sum_{j = 0}^{V}f(w_j)^{0.75}}\]

<ul>
  <li>$f(w_i)$: 각 단어 $w_i$의 출형 빈도수
    <ul>
      <li>‘추천해요’ 100번 등장하고 전체 단어의 빈도가 2000이라면 $f(추천해요) = \frac{100}{2000} = 0.05$</li>
    </ul>
  </li>
  <li>$P(w_i)$: 단어 $w_i$가 네거티브 샘플로 추출될 확률
    <ul>
      <li>출현 빈도수에 0.75제곱한 값을 정규화 상수로 사용하는데, 이 값은 실험을 통해 얻어진 최적의 값이다.</li>
    </ul>
  </li>
</ul>

<p>네거티브 샘플링에서는 입력 단어 쌍이 데이터로부터 추출된 단어 쌍인지, 아니면 네거티브 샘플링으로 생성된 단어 쌍인지 이진 분류를 한다. 이를 위해 로지스틱 회귀 모델을 사용하며, 이 모델의 학습 과정에서는 추출할 단어의 확률 분포를 구하기 위해 먼저 각 단어에 대한 가중치를 학습한다.</p>

<p><a href="https://github.com/user-attachments/assets/a64e8d78-dedd-4628-9c8f-c100686557c7" class="popup img-link shimmer"><img src="https://github.com/user-attachments/assets/a64e8d78-dedd-4628-9c8f-c100686557c7" loading="lazy"></a></p>

<p>네거티브 샘플링 Word2Vec 모델은 실제 데이터에서 추출된 단어 쌍은 1로, 네거티브 샘플링을 통해 추출된 가짜 단어쌍은 0으로 레이블링한다. 즉, 다중 분류에서 이진 분류로 학습 목적이 바뀌게 된다.</p>

<p><a href="https://github.com/user-attachments/assets/9fbc8029-3068-47f5-9bfd-5399272bf03e" class="popup img-link shimmer"><img src="https://github.com/user-attachments/assets/9fbc8029-3068-47f5-9bfd-5399272bf03e" loading="lazy"></a></p>

<p>네거티브 샘플링 모델에서는 입력 단어의 임베딩과 해당 단어가 맞는지 여부를 나타내는 레이블(1 또는 0)을 가져와 내적 연산을 수행한다. 내적 연산을 통해 얻은 값은 시그모이드 함수를 통해 확률값으로 변환된다.</p>
<ul>
  <li>레이블이 1인 경우: 해당 확률값이 높아지도록 가중치를 최적화</li>
  <li>레이블이 0인 경우: 해당 확률값이 낮아지도록 가중치를 최적화</li>
</ul>

<h3 id="모델-실습-skip-gram"><span class="me-2">모델 실습: Skip-gram</span><a href="#모델-실습-skip-gram" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Word2Vec 모델은 학습할 단어의 수를 $V$로, 임베딩 차원을 $E$로 설정해 $W_{V \times E}$ 행렬과 $W’_{E \times V}$ 행렬을 최적화하며 학습한다.</p>
<ul>
  <li>$W_{V \times E}$ 행렬은 <strong>룩업(Lookup)</strong> 연산을 수행하며 이는 <strong>임베딩(<code class="language-plaintext highlighter-rouge">Embedding</code>)</strong> 클래스를 사용하여 구현이 가능</li>
</ul>

<p>임베딩 클래스는 단어나 범주형 변수와 같은 이산 변수를 연속적인 벡터 형태로 변환해 사용할 수 있다. 연속적인 벡터 표현은 모델이 학습하는 동안 단어의 의미와 관련된 정보를 포착하고, 이를 기반으로 단어 간의 유사도를 계산한다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="c1"># 임베딩 클래스
</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span>
  <span class="n">num_embeddings</span><span class="p">,</span>
  <span class="n">embedding_dim</span><span class="p">,</span>
  <span class="n">padding_idx</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">max_norm</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">norm_type</span><span class="o">=</span><span class="mf">2.0</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">num_embeddings</code>: 이산 변수의 개수로 단어 사전의 크기를 의미</li>
  <li><code class="language-plaintext highlighter-rouge">embedding_dim</code>: 임베딩 벡터의 차원 수로 임베딩 벡터의 크기를 의미</li>
  <li><code class="language-plaintext highlighter-rouge">padding_idx</code>: 패딩 토큰의 인덱스를 지정해 해당 인덱스의 임베딩 벡터를 0으로 설정
    <ul>
      <li>병렬 처리는 입력 배치의 문장 길이가 동일해야 하므로 입력 분장들을 일정한 길이로 설정</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">norm_type</code>: 임베딩 벡터의 크기를 제한하는 방법을 선택
    <ul>
      <li>기본값은 2로 L2 정규화 방식을 사용하며 1로 설정하면 L1 정규화 방식을 사용한다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">max_norm</code>: 임베딩 벡터의 최대 크기를 지정
    <ul>
      <li>각 임베딩 벡터의 크기가 최대 노름 값 이상이면 임베딩 벡터를 최대 노름 크기로 잘라내고 크기를 감소시킨다.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="c1"># 기본 Skip-gram 클래스
</span><span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">VanillaSkipgram</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span>
      <span class="n">num_embeddings</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
      <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span>
    <span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
      <span class="n">in_features</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
      <span class="n">out_features</span><span class="o">=</span><span class="n">vocab_size</span>
    <span class="p">)</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></td></tr></tbody></table></code></div></div>

<p>기본 형식의 Skip-gram 모델은 입력 단어와 주변 단어를 룩업 테이블에서 가져와서 내적을 계산한 다음, 손실 함수를 통해 예측 오차를 최소화하는 방식으로 학습된다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="c1"># 영화 리뷰 데이터세트 전처리
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">Korpora</span> <span class="kn">import</span> <span class="n">Korpora</span>
<span class="kn">from</span> <span class="n">konlpy.tag</span> <span class="kn">import</span> <span class="n">Okt</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="n">Korpora</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">nsmc</span><span class="sh">"</span><span class="p">)</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">corpus</span><span class="p">.</span><span class="n">test</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Okt</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">morphs</span><span class="p">(</span><span class="n">review</span><span class="p">)</span> <span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">.</span><span class="n">text</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></div></div>

<p>데이터세트를 <code class="language-plaintext highlighter-rouge">Okt</code> 토크나이저를 사용해 형태소를 추출하고 이를 통해 단어 사전을 구축한다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="c1"># 단어 사전 구축
</span><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="k">def</span> <span class="nf">build_vocab</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">n_vocab</span><span class="p">,</span> <span class="n">special_tokens</span><span class="p">):</span>
  <span class="n">counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">tokens</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
    <span class="n">counter</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
  <span class="n">vocab</span> <span class="o">=</span> <span class="n">special_tokens</span>
  <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">counter</span><span class="p">.</span><span class="nf">most_common</span><span class="p">(</span><span class="n">n_vocab</span><span class="p">):</span>
    <span class="n">vocab</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">vocab</span>


<span class="n">vocab</span> <span class="o">=</span> <span class="nf">build_vocab</span><span class="p">(</span><span class="n">corpus</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span> <span class="n">n_vocab</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">&lt;unk</span><span class="sh">"</span><span class="p">])</span>
<span class="n">token_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
<span class="n">id_to_token</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
</pre></td></tr></tbody></table></code></div></div>

<p><code class="language-plaintext highlighter-rouge">Okt</code> 토크나이저를 통해 토큰화된 데이터를 활용해 <code class="language-plaintext highlighter-rouge">build_vocab</code> 함수로 단어 사전을 구축한다.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">n_vocab</code>: 구축할 단어 사전의 크기
    <ul>
      <li>문서 내에 <code class="language-plaintext highlighter-rouge">n_vocab</code>보다 많은 종류의 토큰이 있다면, 가장 많이 등장한 토큰 순서로 사전을 구축</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">special_tokens</code>: 특별한 의미를 갖는 토큰들을 의미
    <ul>
      <li><code class="language-plaintext highlighter-rouge">&lt;unk&gt;</code> 토큰은 OOV에 대응하기 위한 토큰으로 단어 사전 내에 없는 모든 단어는 <code class="language-plaintext highlighter-rouge">&lt;unk&gt;</code> 토큰으로 대체</li>
    </ul>
  </li>
</ul>

<p>단어 사전의 크기는 구축할 단어 사전의 크기와 <strong>특수 토큰(Special Token)</strong>의 크기 합과 동일하다.</p>

<p>그 다음은 윈도 크기를 정의하고 학습에 사용될 단어 쌍을 추출한다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="c1"># Skip-gram의 단어 쌍 추출
</span><span class="k">def</span> <span class="nf">get_word_pairs</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">window_size</span><span class="p">):</span>
  <span class="n">pairs</span><span class="o">=</span><span class="p">[]</span>
  <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
    <span class="n">sentence_length</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">setence</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">center_word</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
      <span class="n">window_start</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span> <span class="o">-</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">window_end</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">sentence_length</span><span class="p">,</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">center_word</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
      <span class="n">context_words</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="n">window_start</span><span class="p">:</span><span class="n">idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentence</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">window_end</span><span class="p">]</span>
      <span class="k">for</span> <span class="n">context_word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">:</span>
        <span class="n">pairs</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">center_word</span><span class="p">,</span> <span class="n">context_word</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">pairs</span>

<span class="n">word_pairs</span> <span class="o">=</span> <span class="nf">get_word_pairs</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><code class="language-plaintext highlighter-rouge">get_word_pairs</code> 함수는 토큰을 입력받아 Skip-gram 모델의 입력 데이터로 사용할 수 있게 전처리한다.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">window_size</code>: 주변 단어를 몇 개까지 고려할 것인지를 설정한다.
    <ul>
      <li>각 문장에서는 중심 단어와 주변 단어를 고려하여 쌍을 생성</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">idx</code>: 현재 단어의 인덱스를 나타냄</li>
  <li><code class="language-plaintext highlighter-rouge">center_word</code>: 중심 단어</li>
  <li><code class="language-plaintext highlighter-rouge">window_start</code> &amp; <code class="language-plaintext highlighter-rouge">window_end</code>: 현재 단어에서 얼마나 멀리 떨어진 주변 단어를 고려할 것인지를 결정
    <ul>
      <li>문장의 경계를 넘어가는 경우가 없게 조정</li>
    </ul>
  </li>
</ul>

<p>출력 결과는 각 단어 쌍이 [중심 단어, 주변 단어]로 구성되어 있다. 임베딩 층은 단어의 인덱스를 입력으로 받기 때문에 단어 쌍을 인덱스 쌍으로 변환해야 한다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">get_index_pairs</span><span class="p">(</span><span class="n">word_pairs</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">):</span>
  <span class="n">pair</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">unk_idx</span> <span class="o">=</span> <span class="n">token_to_id</span><span class="p">[</span><span class="sh">"</span><span class="s">&lt;unk&gt;</span><span class="sh">"</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">word_pair</span> <span class="ow">in</span> <span class="n">word_pairs</span><span class="p">:</span>
    <span class="n">center_word</span><span class="p">,</span> <span class="n">context_word</span> <span class="o">=</span> <span class="n">word_pair</span>
    <span class="n">center_index</span> <span class="o">=</span> <span class="n">token_to_id</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">center_word</span><span class="p">,</span> <span class="n">unk_index</span><span class="p">)</span>
    <span class="n">context_index</span> <span class="o">=</span> <span class="n">token_to_id</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">context_word</span><span class="p">,</span> <span class="n">unk_index</span><span class="p">)</span>
    <span class="n">pairs</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">center_index</span><span class="p">,</span> <span class="n">context_index</span><span class="p">])</span>
<span class="k">return</span> <span class="n">pairs</span>

<span class="n">index_pairs</span> <span class="o">=</span> <span class="nf">get_index_pairs</span><span class="p">(</span><span class="n">word_pairs</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><code class="language-plaintext highlighter-rouge">get_index_pairs</code> 함수는 <code class="language-plaintext highlighter-rouge">get_word_pairs</code> 함수에서 생성된 단어 쌍을 토큰 인덱스 쌍으로 변환한다.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">word_pairs</code> 단어와 해당 단어의 ID를 매핑한 딕셔너리인 <code class="language-plaintext highlighter-rouge">token_to_id</code>로 인덱스 쌍을 생성</li>
  <li><code class="language-plaintext highlighter-rouge">get</code> 메서드로 토큰이 단어 사전 내에 있으면 해당 토큰의 인덱스를 반환하고, 단어 사전 내에 없다면 <code class="language-plaintext highlighter-rouge">&lt;unk&gt;</code> 토큰의 인덱스를 반환</li>
</ul>

<p>생성된 인덱스 쌍은 Skip-gram 모델의 입력 데이터로 사용되며 이를 학습에 사용하기 위해서는 텐서 형식으로 변환해야한다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="c1"># 데이터로더 적용
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>

<span class="n">index_pairs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">index_pairs</span><span class="p">)</span>
<span class="n">center_indexs</span> <span class="o">=</span> <span class="n">index_pairs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">contenxt_indexs</span> <span class="o">=</span> <span class="n">index_pairs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nc">TensorDataset</span><span class="p">(</span><span class="n">center_indexs</span><span class="p">,</span> <span class="n">context_indexs</span><span class="p">)</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><code class="language-plaintext highlighter-rouge">index_pairs</code>는 <code class="language-plaintext highlighter-rouge">get_index_pairs</code> 함수에서 생성된 중심 단어와 주변 단어 토큰의 인덱스 쌍으로 이루어진 리스트다. 이 리스트를 텐서 형식으로 변환한다. 이 텐서는 [N, 2]의 구조를 가지므로 중심 단어와 주변 단어로 나눠 데이터세트로 변환한다.</p>

<p>인덱스 싸을 텐서 데이터세트로 변환하고 데이터로더에 적용했다면 모델을 학습하기 위한 준비 작업을 진행한다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="c1"># Skip-gram 모델 준비 작업
</span><span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="n">word2Vec</span> <span class="o">=</span> <span class="nc">VaillaSkipgram</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">),</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">word2vec</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>
<p><code class="language-plaintext highlighter-rouge">VanillaSkipgram</code> 클래스의 단어 사전 크기(<code class="language-plaintext highlighter-rouge">vocab_size</code>)에 전체 단어 집합의 크기를 전달하고 임베딩 크기(embedding_dim)는 128로 할당한다.</p>

<p>손실함수는 단어 사전 크기만큼 클래스가 있는 분류 문제이므로 교차 엔트로피를 사용하고 교차 엔트로피는 내부적으로 소프트맥스 연산을 수행하므로 신경망의 출력값을 후처리 없이 활용할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="rouge-code"><pre><span class="c1"># 모델 학습
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">cost</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="k">for</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">target_ids</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_ids</span> <span class="o">=</span> <span class="n">target_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="nf">word2Vec</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">target_ids</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="n">cost</span> <span class="o">+=</span> <span class="n">loss</span>

  <span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">, Cost : </span><span class="si">{</span><span class="n">cost</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
  <span class="c1"># Epoch:   1, Cost : 6.177
</span>  <span class="c1"># Epoch:   2, Cost : 5.992
</span>  <span class="c1"># ...
</span>  <span class="c1"># Epoch:   9, Cost : 5.805
</span>  <span class="c1"># Epoch:  10, Cost : 5.791 
</span></pre></td></tr></tbody></table></code></div></div>

<p>모델 학습이 완료되면 $W_{V \times E}$ 행렬과 $W_{E \times V}’$ 행렬 중 하나의 행렬을 선택해 임베딩 값을 추출한다. 임베딩 층으로 구현된 $W_{V \times E}$ 행렬에서 임베딩 값을 추출하는 코드는 아래와 같다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre><span class="c1"># 임베딩 값 추출
</span><span class="n">token_to_embedding</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>
<span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">word2Vec</span><span class="p">.</span><span class="n">embedding</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">):</span>
  <span class="n">token_to_embedding</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">token</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="mi">30</span><span class="p">]</span>
<span class="n">token_embedding</span> <span class="o">=</span> <span class="n">token_to_embedding</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">token_embedding</span><span class="p">)</span>

<span class="c1"># 연기
# [-0.3942838  -0.09151211  0.53217596  -1.1725438  0.48068285  -0.65455276
# ...
# -0.8654873  -0.22460045]
</span></pre></td></tr></tbody></table></code></div></div>

<p>임베딩 값으로 단어 간의 유사도를 확인할 수 있다. 임베딩 유사도를 측정할 때는 <strong>코사인 유사도(Cosine Similarity)</strong>가 가장 일반적으로 사용되는 방법이다.</p>
<ul>
  <li>코사인 유사도는 두 벡터 간의 각도를 이용하여 유사도를 계산</li>
  <li>두 벡터가 유사할수록 값이 1에 가까워지고, 다를수록 0에 가까워진다.</li>
</ul>

<p>두 벡터 간의 코사인 유사도는 두 벡터의 내적을 벡터의 크기(유클리드 노름)의 곱으로 나누어 계산할 수 있다.</p>

\[cosine\ similarity(a, b) = \frac{a \cdot b}{||a||||b||}\]

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td><td class="rouge-code"><pre><span class="c1"># 단어 임베딩 유사도 계산
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="n">cosine</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nf">norm</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="nf">norm</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">cosine</span>

<span class="k">def</span> <span class="nf">top_n_index</span><span class="p">(</span><span class="n">cosine_matrix</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
  <span class="n">closest_indexes</span> <span class="o">=</span> <span class="n">cosine_matrix</span><span class="p">.</span><span class="nf">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">top_n</span> <span class="o">=</span> <span class="n">closest_indexes</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">top_n</span>

<span class="n">cosine_matrix</span> <span class="o">=</span> <span class="nf">consine_similarity</span><span class="p">(</span><span class="n">token_embedding</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">)</span>
<span class="n">top_n</span> <span class="o">=</span> <span class="nf">top_n_index</span><span class="p">(</span><span class="n">cosine_matrix</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s">와 가장 유사한 5개 단어</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">top_n</span><span class="p">:</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">id_to_token</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="si">}</span><span class="s"> - 유사도 : </span><span class="si">{</span><span class="n">cosine_matrix</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 연기와 유사한 5개 단어
# 연기력 - 유사도: 0.3976
# 배우 - 유사도: 0.3167
# 시나리오 - 유사도: 0.3130
# 악마 - 유사도: 0.2977
# 까지도 - 유사도: 0.2892
</span></pre></td></tr></tbody></table></code></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">cosine_similarity</code> 함수는 입력 단어와 단어 사전 내의 모든 단어와의 코사인 유사도를 계산한다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">a</code> 매개변수는 임베딩 토큰을 의미</li>
      <li><code class="language-plaintext highlighter-rouge">b</code> 매개변수는 임베딩 행렬을 의미하며 [5001, 128]의 구조를 가지므로 노름을 계산할 때 <code class="language-plaintext highlighter-rouge">axis=1</code> 방향으로 계산한다.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">top_n_index</code> 함수는 유사도 행렬을 내림차순으로 정렬해 어떤 단어가 가장 가까운 단어인지 반환한다.
    <ul>
      <li>입력 단어도 단어 사전에 포함되므로 입력 단어 자신이 가장 가까운 단어가 된다.</li>
    </ul>
  </li>
</ul>

<h3 id="모델-실습-gensim"><span class="me-2">모델 실습: Gensim</span><a href="#모델-실습-gensim" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Word2Vec 모델을 학습할 때 데이터 수가 적음에도 불구하고 오랜 시간이 걸린다. 이러한 경우, 계층적 소프트맥스나 네거티브 샘플링 같은 기법을 사용하면 더 효율적으로 학습할 수 있다.</p>

<p><strong>젠심(Gensim)</strong> 라이브러리를 활용하면 자연어 처리 모델을 쉽게 구성할 수 있다.</p>
<ul>
  <li>대용량 텍스트 데이터의 처리를 위한 메모리 효율적인 방법을 제공해 대규모 데이터 세트에서도 효과적으로 모델을 학습</li>
  <li>학습된 모델을 저장하여 관리할 수 있고, 비슷한 단어 찾기 등 유사도와 관련된 기능도 제공</li>
</ul>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="c1"># Word2Vec 클래스
</span><span class="n">word2Vec</span> <span class="o">=</span> <span class="n">gensim</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Word2Vec</span><span class="p">(</span>
  <span class="n">sentences</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">corpus_file</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span>
  <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">workers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
  <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
  <span class="n">hs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
  <span class="n">cbow_mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">negative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">ns_exponent</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span>
  <span class="n">max_final_vocab</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">batch_words</span><span class="o">=</span><span class="mi">10000</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">sentences</code>: 모델의 학습 데이터를 나타내며 토큰 리스트로 표현된다.</li>
  <li><code class="language-plaintext highlighter-rouge">corpus_file</code>: 학습 데이터를 파일로 입력할 때 파일 경로를 의미(입력 문장 대신 사용 가능)</li>
  <li><code class="language-plaintext highlighter-rouge">vector_size</code>: 학습할 임베딩 벡터의 크기를 의미하며, 임베딩 차원 수를 설정</li>
  <li><code class="language-plaintext highlighter-rouge">alpha</code>: Word2Vec 모델의 학습률을 의미</li>
  <li><code class="language-plaintext highlighter-rouge">window</code>: 학습 데이터를 생성할 윈도의 크기</li>
  <li><code class="language-plaintext highlighter-rouge">min_count</code>: 학습에 사용할 단어의 최소 빈도
    <ul>
      <li>최소 빈도만큼 등장하지 않으면 학습에 사용되지 않음</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">max_final_vocab</code>: 단어 사전의 최대 크기
    <ul>
      <li>최소 빈도를 충족하는 단어가 최대 최종 단어 사전보다 많으면 자주 등장하는 순으로 단어 사전 구축</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">workers</code>: 빠른 학습을 위해 병렬 학습할 스레드의 수</li>
  <li><code class="language-plaintext highlighter-rouge">sg</code>: Skip-gram 모델의 사용 여부
    <ul>
      <li>1이면 Skip-gram 모델 사용</li>
      <li>0이면 CBoW 모델 사용</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">hs</code>: 계층적 사용 여부를 설정(1일 떄 사용)</li>
  <li><code class="language-plaintext highlighter-rouge">cbow_mean</code>: CBoW 모델로 구성할 때 합한 벡터의 평균화 여부를 설정(1일 떄 평균화)</li>
  <li><code class="language-plaintext highlighter-rouge">negative</code>: 네거티브 샘플링의 단어 수(0이면 사용하지 않음)</li>
  <li><code class="language-plaintext highlighter-rouge">ns_exponent</code>: 네거티브 샘플링 확률의 지수</li>
  <li><code class="language-plaintext highlighter-rouge">epochs</code>: 학습 에폭 수</li>
  <li><code class="language-plaintext highlighter-rouge">batch_words</code>: 몇 개의 단어로 학습 배치를 구성할지 결정</li>
</ul>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="c1"># Word2Vec 모델 학습
</span><span class="kn">from</span> <span class="n">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="n">word2Vec</span> <span class="o">=</span> <span class="nc">Word2Vec</span><span class="p">(</span>
  <span class="n">sentences</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span>
  <span class="n">vector_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
  <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
  <span class="n">max_final_vocab</span><span class="o">=</span><span class="mi">10000</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p>모델 학습 속도는 <code class="language-plaintext highlighter-rouge">VanillaSkipgram</code> 클래스보다 훨씬 빠르게 학습된다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="n">word</span> <span class="o">=</span> <span class="sh">"</span><span class="s">연기</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">word2vec</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">word2vec</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">word2vec</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">similar</span><span class="p">(</span><span class="n">w1</span><span class="o">=</span><span class="n">word</span><span class="p">,</span> <span class="n">w2</span><span class="o">=</span><span class="sh">"</span><span class="s">연기력</span><span class="sh">"</span><span class="p">))</span>

<span class="c1"># [-0.4074033 -0.19263862, ...,
#  ...
# -0.36874628 -0.41801444]
# [("연기력", 0.7762452363967896), ('캐스팅', 0.7704317569732666), ('연기자', 0.7353872060775757), ('여배우', 0.7160670161247253), ('조연', 0.7131801247596741)]
</span></pre></td></tr></tbody></table></code></div></div>
<p><code class="language-plaintext highlighter-rouge">word2vec</code> 인스턴스의 <code class="language-plaintext highlighter-rouge">wv</code> 속성은 학습된 단어 벡터 모델을 포함한 <code class="language-plaintext highlighter-rouge">Word2VecKeyedVectors</code> 객체를 반환한다. 이 객체는 단어 벡터 검색과 유사도 계산 등의 작업을 수행한다.</p>

<p>이 객체는 유사한 단어를 찾아주는 <code class="language-plaintext highlighter-rouge">most_similar</code> 메서드와 두 단어 간의 유사도를 계산하는 <code class="language-plaintext highlighter-rouge">similarity</code> 메서드를 제공한다.</p>

<p>하지만 Word2Vec은 분포 가설을 통해 쉽고 빠르게 단어의 임베딩을 학습하지만, 단어의 형태학적 특징을 반영하지 못한다. 특히 한국어는 어근과 접사, 조사 등으로 이루어지는 규칙이 있기 때문에 Word2Vec 모델이 구조적 특징을 제대로 학습하기 어렵다. 이는 OOV를 발생시키는 원인이 된다.</p>

<h2 id="fasttext"><span class="me-2">fastText</span><a href="#fasttext" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<hr />

<p><strong>fastText</strong>는 임베딩 모델로, 텍스트 분류 및 텍스트 마이닝을 위한 알고리즘이다.</p>
<ul>
  <li>단어와 문장을 벡터로 변환하는 기술을 기반으로 함</li>
  <li>머신러닝 알고리즘이 텍스트 데이터를 분석하고 이해하는 데 사용</li>
</ul>

<p>fastText에서는 단어의 벡터화를 위해 &lt;, &gt;와 같은 특수 기호를 사용하여 단어의 시작과 끝을 나타낸다. 이 기호가 단어의 하위 문자열을 고려하는 데 중요한 역할을 한다.</p>

<p>기호가 추가된 단어는 N-gram을 사용하여 <strong>하위 단어 집합(Subword set)</strong>으로 분해된다.</p>

<p>ex. ‘서울특별시’ → ‘서울’, ‘울특’, ‘특별’, ‘별시’</p>

<p>분해된 하위 단어 집합에는 나누지 않은 단어 자신도 포함되며, 단어 집합이 만들어지면 각 하위 단어는 고유한 벡터값을 갖게 되며 이는 단어의 벡터 표현을 구성하며, 이를 사용해 자연어 처리 작업을 수행한다.</p>

<p><a href="https://github.com/user-attachments/assets/83095867-331a-4738-b019-2bf95131fca4" class="popup img-link shimmer"><img src="https://github.com/user-attachments/assets/83095867-331a-4738-b019-2bf95131fca4" loading="lazy"></a></p>

<ol>
  <li>토큰의 양 끝에 ‘&lt;’와 ‘&gt;’를 붙여 토큰의 시작과 끝을 인식할 수 있게 한다.</li>
  <li>분해된 토큰은 N-gram을 사용하여 하위 단어 집합으로 분해한다.</li>
  <li>분해된 하위 단어 집합에는 나눠지지 않은 토큰 자체도 포함한다. 이렇게 하위 단어 집합이 만들어지면, 각 하위 단어는 고유한 벡터값을 갖는다.</li>
</ol>

<p>일반적으로 fastText는 다양한 N-gram을 적용해 입력 토큰을 분해하고 하위 단어 벡터를 구성함으로써 단어의 부분 문자열을 고려하는 유연하고 정확한 하위 단어 집합을 생성한다.</p>

<p>즉, 같은 하위 단어를 공유하는 단어끼리는 정보를 공유해 학습할 수 있으므로 비슷한 단어끼리는 비슷한 임베딩 벡터를 갖게 되어, 단어 간 유사도를 높일 수 있다.</p>

<p>또한 OOV 단어도 하위 단어로 나누어 임베딩을 계산할 수 있게 된다.</p>

<p>이렇게 하위 단어 기반의 임베딩 방법을 사용하면, 말뭉치에 등장하지 않은 단어라도 유사한 하위 단어를 가지고 있으면 유사한 임베딩 벡터를 갖게 된다.</p>

<h3 id="모델-실습"><span class="me-2">모델 실습</span><a href="#모델-실습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>fastText 모델은 CBoW와 Skip-gram으로 구성되며 네거티브 샘플링 기법을 사용해 학습한다. 단 fastText는 하위 단어로 학습한다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="rouge-code"><pre><span class="c1"># Fast 클래스
</span><span class="n">fasttext</span> <span class="o">=</span> <span class="n">gensim</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">FastText</span><span class="p">(</span>
  <span class="n">sentences</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">corpus_file</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span>
  <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">workers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
  <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
  <span class="n">hs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
  <span class="n">cbow_mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">negative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">ns_exponent</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span>
  <span class="n">max_final_vocab</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
  <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">batch_words</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
  <span class="n">min_n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
  <span class="n">max_n</span><span class="o">=</span><span class="mi">6</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<ul>
  <li>대부분은 <code class="language-plaintext highlighter-rouge">Word2Vec</code> 클래스의 하이퍼파라미터와 동일</li>
  <li>N-gram 범위를 결정하는 하이퍼파라미터 추가
    <ul>
      <li>최소 N(<code class="language-plaintext highlighter-rouge">min_n</code>): N-gram의 최솟값</li>
      <li>최대 N(<code class="language-plaintext highlighter-rouge">max_n</code>): N-gram의 최댓값</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="c1"># KorNLI 데이터세트 전처리
</span><span class="kn">from</span> <span class="n">Korpora</span> <span class="kn">import</span> <span class="n">Korpora</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="n">Korpora</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">kornli</span><span class="sh">"</span><span class="p">)</span>
<span class="n">corous_texts</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">.</span><span class="nf">get_all_text</span><span class="p">()</span> <span class="o">+</span> <span class="n">corpus</span><span class="p">.</span><span class="nf">get_all_pairs</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">corpus_texts</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></div></div>

<p>fastText 모델은 입력 단어의 구조적 특징을 학습할 수 있어 형태소 분석기를 통해 토큰화하지 않고 띄어쓰기를 기준으로 단어를 토큰화해 학습을 진행한다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="c1"># fastText 모델
</span><span class="kn">from</span> <span class="n">gensim.models</span> <span class="kn">import</span> <span class="n">FastText</span>

<span class="n">fastText</span> <span class="o">=</span> <span class="nc">FastText</span><span class="p">(</span>
  <span class="n">sentences</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span>
  <span class="n">vector_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
  <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
  <span class="n">min_n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
  <span class="n">max_n</span><span class="o">=</span><span class="mi">6</span>
<span class="p">)</span>

<span class="c1"># 모델 저장
</span><span class="n">fastText</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sh">"</span><span class="s">../models/fastText.model</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 모델 불러오기
</span><span class="n">fastText</span> <span class="o">=</span> <span class="n">FastText</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">../models/fastText.model</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p>Word2Vec 모델과는 다르게 OOV 단어를 대상으로도 의미 있는 임베딩을 추출할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="c1"># fastText OOV 처리
</span><span class="n">oov_token</span> <span class="o">=</span> <span class="sh">"</span><span class="s">사랑해요</span><span class="sh">"</span>
<span class="n">oov_vector</span> <span class="o">=</span> <span class="n">fastText</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="n">oov_token</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="n">oov_token</span> <span class="ow">in</span> <span class="n">fastText</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="n">index_to_key</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">fastText</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="n">oov_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="c1"># False
# [('사랑', 0.8812), ('사랑해', 0.8438), ('사랑의', 0.7931), ('사랑을', 0.75945), ('사랑하는', 0.750636)]
</span></pre></td></tr></tbody></table></code></div></div>

<ul>
  <li>Word2Vec 모델은 단어 사전에 존재하지 않는 단어의 임베딩 계산이 불가능했지만 fastText는 하위 단어로 나뉘어 있기 때문에 단어를 처리할 수 있다.</li>
</ul>

<p>즉, fastText는 OOV 문제를 효과적으로 해결할 수 있으며 한국어와 같은 많은 언어는 형태적 구조를 갖고 있기 때문에 효과적 처리가 가능하다.</p>


  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/deep-learning/">Deep Learning</a>,
          <a href="/categories/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC%EC%99%80-%EC%BB%B4%ED%93%A8%ED%84%B0-%EB%B9%84%EC%A0%84-%EC%8B%AC%EC%B8%B5%ED%95%99%EC%8A%B5/">파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터 비전 심층학습</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/nlp/"
            class="post-tag no-text-decoration"
          >NLP</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=%EC%9E%84%EB%B2%A0%EB%94%A9(2)%20-%20Seojin%20Devlog&url=http%3A%2F%2Flocalhost%3A4000%2Fpytorch-book%2Fnlp%2Fembedding-2%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=%EC%9E%84%EB%B2%A0%EB%94%A9(2)%20-%20Seojin%20Devlog&u=http%3A%2F%2Flocalhost%3A4000%2Fpytorch-book%2Fnlp%2Fembedding-2%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fpytorch-book%2Fnlp%2Fembedding-2%2F&text=%EC%9E%84%EB%B2%A0%EB%94%A9(2)%20-%20Seojin%20Devlog" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/naver-boostcamp/PyTorch/pytorch-01/">PyTorch 기초</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/algorithm/dynamic-programming/">다이나믹 프로그래밍</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/data-structure/nonlinear-1/">비선형 자료구조(1)</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/boostcamp/pre-course/pandas/">Pandas 라이브러리 사용법</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/boostcamp/pre-course/numpy/">Numpy 라이브러리 사용법</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/naver-boostcamp/">Naver-Boostcamp</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pre-course/">Pre-Course</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pytorch/">pytorch</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/ml/">ml</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/scikit-learn/">scikit-learn</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/nlp/">NLP</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/algorithm/">algorithm</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/supervised-learning/">supervised-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/cnn/">CNN</a>
      
    </div>
  </section>


            </div>

            
              
              






  <div class="toc-border-cover z-3"></div>
  <section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4">
    <h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->















  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  











  <aside id="related-posts" aria-labelledby="related-label">
    <h3 class="mb-4" id="related-label">Further Reading</h3>
    <nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        <article class="col">
          <a href="/pytorch-book/nlp/cnn/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1744729200"
  data-df="ll"
  
>
  Apr 16, 2025
</time>

              <h4 class="pt-0 my-2">합성곱 신경망</h4>
              <div class="text-muted">
                <p>CNN / 완전 연결 계층 / 1차원 합성곱 &amp; 자연어 처리</p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/pytorch-book/nlp/embedding-1/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1743433200"
  data-df="ll"
  
>
  Apr  1, 2025
</time>

              <h4 class="pt-0 my-2">임베딩(1)</h4>
              <div class="text-muted">
                <p>언어 모델 / N-gram / TF-IDF</p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/pytorch-book/nlp/tokenization/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1743174000"
  data-df="ll"
  
>
  Mar 29, 2025
</time>

              <h4 class="pt-0 my-2">토큰화</h4>
              <div class="text-muted">
                <p>단어 및 글자 토큰화 / 형태소 토큰화 / 하위 단어 토큰화</p>
              </div>
            </div>
          </a>
        </article>
      
    </nav>
  </aside>
  <!-- #related-posts -->


            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/pytorch-book/nlp/embedding-1/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>임베딩(1)</p>
    </a>
  

  
    <a
      href="/pytorch-book/nlp/cnn/"
      class="btn btn-outline-primary"
      aria-label="Newer"
    >
      <p>합성곱 신경망</p>
    </a>
  
</nav>

            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>©
    <time>2025</time>

    
      <a href="https://twitter.com/username">Seojin Park</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Using the <a
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="v7.3.0"
        href="https://github.com/cotes2020/jekyll-theme-chirpy"
        target="_blank"
        rel="noopener"
      >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center d-none">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/naver-boostcamp/">Naver-Boostcamp</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pre-course/">Pre-Course</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/python/">python</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pytorch/">pytorch</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/ml/">ml</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/scikit-learn/">scikit-learn</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/nlp/">NLP</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/algorithm/">algorithm</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/supervised-learning/">supervised-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/cnn/">CNN</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- Embedded scripts -->

    
      
      <!-- The comments switcher -->

  
  <!-- https://utteranc.es/ -->
<script>
  (function () {
    const origin = 'https://utteranc.es';
    const themeMapper = Theme.getThemeMapper('github-light', 'github-dark');
    const initTheme = themeMapper[Theme.visualState];

    let script = document.createElement('script');
    script.src = 'https://utteranc.es/client.js';
    script.setAttribute('repo', 'Parkseojin2001/Parkseojin2001.github.io');
    script.setAttribute('issue-term', 'pathname');
    script.setAttribute('theme', initTheme);
    script.crossOrigin = 'anonymous';
    script.async = true;

    const $footer = document.querySelector('footer');
    $footer.insertAdjacentElement('beforebegin', script);

    addEventListener('message', (event) => {
      let newTheme;if (event.source === window && event.data && event.data.id === Theme.ID) {
        newTheme = themeMapper[Theme.visualState];

        const message = {
          type: 'set-theme',
          theme: newTheme
        };

        const utterances = document.querySelector('.utterances-frame').contentWindow;
        utterances.postMessage(message, origin);
      }
    });
  })();
</script>



    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  
  document.addEventListener('DOMContentLoaded', () => {
    SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('search-results'),
      json: '/assets/js/data/search.json',
      searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{content}</p>  </article>',
      noResultsText: '<p class="mt-5">Oops! No results found.</p>',
      templateMiddleware: function(prop, value, template) {
        if (prop === 'categories') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
          }
        }

        if (prop === 'tags') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
          }
        }
      }
    });
  });
</script>

  </body>
</html>

