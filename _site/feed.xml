

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://localhost:4000/</id>
  <title>Seojin Devlog</title>
  <subtitle>"A blog about AI, ML, coding, and personal projects."</subtitle>
  <updated>2025-09-30T17:22:17+09:00</updated>
  <author>
    <name>Seojin Park</name>
    <uri>http://localhost:4000/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://localhost:4000/"/>
  <generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator>
  <rights> © 2025 Seojin Park </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>RNN과 Language Modeling</title>
    <link href="http://localhost:4000/naver-boostcamp/nlp/03" rel="alternate" type="text/html" title="RNN과 Language Modeling" />
    <published>2025-09-30T00:00:00+09:00</published>
  
    <updated>2025-09-30T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/nlp/03</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/nlp/03" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Deep Learning" />
    
    <category term="NLP" />
    
  

  <summary>자연어 처리 분야에서 Recurrent Neural Network(RNN)를 활용하는 방법과 Language Modeling에 대한 설명을 정리한 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>Word Embedding</title>
    <link href="http://localhost:4000/naver-boostcamp/nlp/02" rel="alternate" type="text/html" title="Word Embedding" />
    <published>2025-09-29T00:00:00+09:00</published>
  
    <updated>2025-09-29T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/nlp/02</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/nlp/02" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Deep Learning" />
    
    <category term="NLP" />
    
  

  <summary>단어 또는 토큰을 컴퓨터가 이해할 수 있는 벡터로 표현하는 방법인 Word2Vec에 대한 내용을 정리한 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>Tokenization</title>
    <link href="http://localhost:4000/naver-boostcamp/nlp/01" rel="alternate" type="text/html" title="Tokenization" />
    <published>2025-09-29T00:00:00+09:00</published>
  
    <updated>2025-09-29T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/nlp/01</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/nlp/01" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Deep Learning" />
    
    <category term="NLP" />
    
  

  <summary>토큰화가 무엇인지 그리고 토큰화의 종류에 대해 정리한 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>Transformer3: Transformer</title>
    <link href="http://localhost:4000/naver-boostcamp/ml-life-cycle/10" rel="alternate" type="text/html" title="Transformer3: Transformer" />
    <published>2025-09-26T00:00:00+09:00</published>
  
    <updated>2025-09-26T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/ml-life-cycle/10</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/ml-life-cycle/10" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Deep Learning" />
    
    <category term="NLP" />
    
  

  <summary>Transformer와 BERT의 개념 및 활용 과제, 그리고 Vision Transformer(ViT)에 대한 학습 내용을 정리한 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>Segmentation &amp; Detection</title>
    <link href="http://localhost:4000/naver-boostcamp/computer-vision/03" rel="alternate" type="text/html" title="Segmentation &amp;amp; Detection" />
    <published>2025-09-24T00:00:00+09:00</published>
  
    <updated>2025-09-24T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/computer-vision/03</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/computer-vision/03" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Deep Learning" />
    
    <category term="CV" />
    
  

  <summary>CNN 모델의 내부 동작을 가시화하는 방법과 데이터 증강 기법에 대해 정리한 포스트입니다.</summary>

  </entry>

</feed>


