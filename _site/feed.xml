

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://localhost:4000/</id>
  <title>Seojin Devlog</title>
  <subtitle>"A blog about AI, ML, coding, and personal projects."</subtitle>
  <updated>2026-01-04T00:18:19+09:00</updated>
  <author>
    <name>Seojin Park</name>
    <uri>http://localhost:4000/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://localhost:4000/"/>
  <generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator>
  <rights> © 2026 Seojin Park </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>Self-supervised Pre-training Model: BERT</title>
    <link href="http://localhost:4000/naver-boostcamp/nlp/09" rel="alternate" type="text/html" title="Self-supervised Pre-training Model: BERT" />
    <published>2026-01-03T00:00:00+09:00</published>
  
    <updated>2026-01-03T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/nlp/09</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/nlp/09" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Naver-Boostcamp" />
    
    <category term="NLP 이론" />
    
  

  <summary>Transformer 기반의 자연어 처리 성능을 혁신한 모델 BERT에 대한 내용을 정리한 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>Transformer 2</title>
    <link href="http://localhost:4000/naver-boostcamp/nlp/08" rel="alternate" type="text/html" title="Transformer 2" />
    <published>2025-11-27T00:00:00+09:00</published>
  
    <updated>2025-12-25T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/nlp/08</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/nlp/08" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Naver-Boostcamp" />
    
    <category term="NLP 이론" />
    
  

  <summary>Transformer의 주요 구성요소와 Masked Self-Attention에 관한 내용을 정리한 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>Transformer 1</title>
    <link href="http://localhost:4000/naver-boostcamp/nlp/07" rel="alternate" type="text/html" title="Transformer 1" />
    <published>2025-11-26T00:00:00+09:00</published>
  
    <updated>2025-11-26T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/nlp/07</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/nlp/07" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Naver-Boostcamp" />
    
    <category term="NLP 이론" />
    
  

  <summary>Transformer의 기본 원리인 Self-Attention와 Transformer의 장점에 대한 내용을 정리한 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>RAG</title>
    <link href="http://localhost:4000/ai-agent/inflearn/langchain-02" rel="alternate" type="text/html" title="RAG" />
    <published>2025-11-25T00:00:00+09:00</published>
  
    <updated>2025-11-25T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/ai-agent/inflearn/langchain-02</id>
    <content type="text/html" src="http://localhost:4000/ai-agent/inflearn/langchain-02" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="AI Agent" />
    
    <category term="LangChain" />
    
  

  <summary>RAG에 대한 기본 개념에 대한 내용을 정리한 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>Seq2Seq with Attention</title>
    <link href="http://localhost:4000/naver-boostcamp/nlp/06" rel="alternate" type="text/html" title="Seq2Seq with Attention" />
    <published>2025-11-24T00:00:00+09:00</published>
  
    <updated>2025-11-24T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/nlp/06</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/nlp/06" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Naver-Boostcamp" />
    
    <category term="NLP 이론" />
    
  

  <summary>Seq2Seq 모델과 이 모델의 문제점 그리고 Attention 기법에 대한 내용을 정리한 포스트입니다.</summary>

  </entry>

</feed>


