

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://localhost:4000/</id>
  <title>Seojin Devlog</title>
  <subtitle>"A blog about AI, ML, coding, and personal projects."</subtitle>
  <updated>2025-09-09T20:02:12+09:00</updated>
  <author>
    <name>Seojin Park</name>
    <uri>http://localhost:4000/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://localhost:4000/"/>
  <generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator>
  <rights> © 2025 Seojin Park </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>확률적 경사하강법</title>
    <link href="http://localhost:4000/naver-boostcamp/gradient-descent/03" rel="alternate" type="text/html" title="확률적 경사하강법" />
    <published>2025-09-09T00:00:00+09:00</published>
  
    <updated>2025-09-09T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/gradient-descent/03</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/gradient-descent/03" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Math for AI" />
    
    <category term="Gradient Descent" />
    
  

  <summary>확률적 경사하강법의 원리와 경사하강법과의 차이 그리고 확률적 경사하강법 학습시 주의사항에 대한 정리 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>그레디언트 벡터</title>
    <link href="http://localhost:4000/naver-boostcamp/gradient-descent/02" rel="alternate" type="text/html" title="그레디언트 벡터" />
    <published>2025-09-09T00:00:00+09:00</published>
  
    <updated>2025-09-09T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/gradient-descent/02</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/gradient-descent/02" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Math for AI" />
    
    <category term="Gradient Descent" />
    
  

  <summary>그레디언트 벡터과 경사하강법 기반의 선형회귀 알고리즘 그리고 경사하강법의 문제점에 대한 정리 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>경사하강법</title>
    <link href="http://localhost:4000/naver-boostcamp/gradient-descent/01" rel="alternate" type="text/html" title="경사하강법" />
    <published>2025-09-09T00:00:00+09:00</published>
  
    <updated>2025-09-09T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/gradient-descent/01</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/gradient-descent/01" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Math for AI" />
    
    <category term="Gradient Descent" />
    
  

  <summary>미분의 개념과 미분을 이용한 경사하강법에 대한 정리 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>텐서(Tensor)</title>
    <link href="http://localhost:4000/naver-boostcamp/linear-algebra/05" rel="alternate" type="text/html" title="텐서(Tensor)" />
    <published>2025-09-09T00:00:00+09:00</published>
  
    <updated>2025-09-09T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/linear-algebra/05</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/linear-algebra/05" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Math for AI" />
    
    <category term="Linear Algebra" />
    
  

  <summary>텐서 구조와 텐서 연산을 위한 도구 einsum과 einops의 사용법에 대한 정리 포스트입니다.</summary>

  </entry>

  
  <entry>
    <title>행렬 분해</title>
    <link href="http://localhost:4000/naver-boostcamp/linear-algebra/04" rel="alternate" type="text/html" title="행렬 분해" />
    <published>2025-09-08T00:00:00+09:00</published>
  
    <updated>2025-09-08T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/naver-boostcamp/linear-algebra/04</id>
    <content type="text/html" src="http://localhost:4000/naver-boostcamp/linear-algebra/04" />
    <author>
      <name>Seojin Park</name>
    </author>

  
    
    <category term="Math for AI" />
    
    <category term="Linear Algebra" />
    
  

  <summary>행렬 분해 중 고유값 분해와 특이값 분해 대해 그리고 고유값 분해를 활용한 주성분 분석(PCA)에 대한 내용 정리 포스트입니다.</summary>

  </entry>

</feed>


