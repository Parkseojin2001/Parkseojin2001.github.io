---
title: "임베딩(2)"
excerpt: "Word2Vec / fastText"

categories:
  - NLP
tags:
  - [NLP]

permalink: /nlp/embedding-2/

toc: true
toc_sticky: true

date: 2025-04-05
last_modified_at: 2025-04-05
---


## 🦥 Word2Vec

**Word2Vec**은 단어 간의 유사성을 측정하기 위해 분포 가설(distributional hypothesis)을 기반으로 개발된 임베딩 모델이다.

- 분포 가설: 같은 문맥에서 함께 자주 나타나는 단어들은 서로 유사한 의미를 가질 가능성이 높다는 가정이며 단어 간의 **동시 발생(co-occurrence)** 확률 분포를 이용해 단어 간의 유사성을 측정

ex. '내일 자동차를 타고 부산에 간다' 와 '내일 비행기를 타고 부산에 간다' 라는 두 문장에서 '자동차'와 '비행기'는 주변에 분포한 단어들이 동일하거나 유사하므로 두 단어는 비슷한 의미를 가질 것이라고 예상

가정을 통해 단어의 **분산 표현(Distributed Representation)**을 학습할 수 있다.
- 분산 표현: 단어를 고차원 벡터 공간에 매핑하여 단어의 의미를 담은 것을 의미

분포 가설에 따라 유사한 문맥에서 등장하는 단어는 비슷한 벡터 공간상 위치를 갖게 된다. 위의 예시에서 '비행기'와 '자동차'는 벡터 공간에서 서로 가까운 위치에 표현된다.

이는 빈도 기반 벡터화 기법에서 발생했던 단어의 의미 정보를 저장하지 못하는 한계를 극복했으며, 대량의 텍스트 데이터에서 단어 간의 관계를 파악하고 벡터 공간상에서 유사한 단어를 군집화해 단어의 의미 정보를 효과적으로 표현한다.

### 단어 벡터화

단어를 벡터화하는 방법은 크게 **희소 표현(sparse representation)**과 **밀집 표현(dense representation)**으로 나눌 수 있다. 
- 희소 표현: 빈도 기반 방법으로 대표적으로는 원-핫 인코딩, TF-IDF가 존재
    - 대부분의 벡터 요소가 0으로 표현
    - 단어 사전의 크기가 커지면 벡터의 크기도 커지므로 공간적 낭비가 발생
    - 단어 간의 유사성을 반영하지 못함
    - 벡터 간의 유사성을 계산하는 데도 많은 비용이 발생

    | 단어|   |   |   |    |   |
    |----|---|---|---|----|---|
    |소   |0  |1  |0  | 0  | 0 |
    |잃고 |1  |0  | 0 |  0 | 0 |
    |외양간| 0 | 0 | 0 | 1 | 0 |
    |고친다| 0 | 0 | 0 | 0 | 1 |

- 밀집 표현: Word2Vec
    - 단어를 고정된 크기의 실수 벡터로 표현하기 때문에 단어 사전의 크기가 커지더라도 벡터의 크기가 커지지 않음
    - 벡터 공간상에서 단어 간의 거리를 효과적으로 계산할 수 있으며, 벡터의 대부분이 0이 아닌 실수로 이루어져 있어 효율적으로 공간을 활용

    | 단어|   |   |   |    |   |
    |----|---|---|---|----|---|
    |소   |0.3914|-0.1749| ... |0.5912|0.1321|
    |잃고 |-0.2893|0.3814| ... |-0.1492|-0.2814|
    |외양간|0.4812|0.1214| ... |-0.2745|0.0132|
    |고친다|-0.1314|-0.2809| ... |0.2014|0.3016|

밀집 표현 벡터화는 학습을 통해 단어를 벡터화하기 때문에 단어의 의미를 비교할 수 있다. 밀집 표현된 벡터를 **단어 임베딩 벡터(Word Embedding Vector)**라고 하며, Word2Vec은 대표적인 단어 임베딩 기법 중 하나다.

Word2Vec은 밀집 표현을 위해 CBoW와 Skip-gram이라는 두 가지 방법을 사용한다.

### CBoW

**CBoW(Continuous Bag of Words)**란 주변에 있는 단어를 가지고 중간에 있는 단어를 예측하는 방법이다. 
- 중심 단어(Center Word): 측해야 할 단어를 의미
- 주변 단어(Context Word): 예측에 사용되는 단어들

중심 단어를 맞추기 위해 몇 개의 주변 단어를 고려할지를 정해야 하는데, 이 범위를 **윈도(Window)**라고 한다. 이 윈도를 활용해 주어진 하나의 문장에서 첫 번째 단어부터 중심 단어로 하여 마지막 단어까지 학습한다.
- 윈도가 N일 때, 범위는 중심 단어의 앞에 위치한 N개의 주변 단어부터 뒤에 위치한 N개의 주변 단어이다.

학습을 위해 윈도를 이동해 가며 학습하는데, 이러한 방법을 **슬라이딩 윈도(Sliding Window)**라 한다. CBoW는 슬라이딩 윈도를 사용해 한 번의 학습으로 여러 갱의 중심 단어와 그에 대한 주변 단어를 학습할 수 있다.

<img src="https://private-user-images.githubusercontent.com/85439023/430617831-2e100fec-e458-42e3-b90d-684f4d3dc1a5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4NTg5NzgsIm5iZiI6MTc0Mzg1ODY3OCwicGF0aCI6Ii84NTQzOTAyMy80MzA2MTc4MzEtMmUxMDBmZWMtZTQ1OC00MmUzLWI5MGQtNjg0ZjRkM2RjMWE1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDEzMTExOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY3OGU2OWZjMWY0MjZjOGQ4ZTRlMzc0NzRlMzY0Y2Y2YzEyODM2M2Q1OTg5ZmFiMWE1NmYzMDNiMjI0NmIyNWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.ZZibNpBN2W4hQX7b-c1Ta_RJIhRRdaENyKod4HGJ9wk">

위의 그림은 하나의 입력 문장에서 윈도 크기가 2일 때 학습 데이터가 어떻게 구성되는지를 보여준다.

학습 데이터는 (주변 단어 \ 중심 단어)로 구성된다. 이를 통해 대량의 말뭉치에서 효율적으로 단어의 분산 표현을 학습할 수 있다. 얻어진 학습 데이터는 인공 신경망을 학습하는데 사용된다.

<img src="https://private-user-images.githubusercontent.com/85439023/430618041-7694ed72-b24f-43d6-a8e7-045c3692ff7c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4NTg5NzgsIm5iZiI6MTc0Mzg1ODY3OCwicGF0aCI6Ii84NTQzOTAyMy80MzA2MTgwNDEtNzY5NGVkNzItYjI0Zi00M2Q2LWE4ZTctMDQ1YzM2OTJmZjdjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDEzMTExOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTliZTlmZjNkOWNhMjNiOTA3NzQxN2U3ZGVkYzgxMmUxZmJhZDMzNzVhNTJlM2QyNzYwNjk1YTFiMWYxNWVmYTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.elZpGMRZev0DYjfzUA9U5xbXoh1M3_9Lu5wTaDVs6n8">

CBoW 모델은 각 입력 단어의 원-핫 벡터를 입력값으로 받는다. 입력 문장 내 모든 단어의 임베딩 벡터를 평균 내어 중심 단어의 임베딩 벡터를 예측한다.

1. 입력 단어는 원-핫 벡터로 표현돼 **투사층(Projection Layer)**에 입력된다.
- 투사층: 원-핫 벡터의 인덱스에 해당하는 임베딩 벡터를 반환하는 **순람표(Lookup table, LUT)** 구조
2. 투사층을 통과하면 각 단어는 E 크기의 임베딩 벡터로 변환한다.
- 입력된 임베딩 벡터 $V_1, V_2, ... , V_n$들의 평균값을 계산
3. 계산된 평균 벡터를 가중치 행렬 $W'_{E \times V}$와 곱하면 $V$ 크기의 벡터를 얻는다.
4. 소프트맥스 함수를 이용해 중심 단어를 예측한다.

### Skip-gram

**Skip-gram**은 CBoW와 반대로 중심 단어를 입력으로 받아서 주변 단어를 예측하는 모델이다.
- 중심 단어를 기준으로 양쪽으로 윈도 크기만큼의 단어들을 주변 단어로 삼아 훈련 데이터세트를 만든다.
- 중심 단어와 각 주변 단어를 하나의 쌍으로 하여 모델을 학습시킨다.

<img src="https://private-user-images.githubusercontent.com/85439023/430619463-9166523b-8ce5-4d64-8380-b7d34c18fc17.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4NTkxNzYsIm5iZiI6MTc0Mzg1ODg3NiwicGF0aCI6Ii84NTQzOTAyMy80MzA2MTk0NjMtOTE2NjUyM2ItOGNlNS00ZDY0LTgzODAtYjdkMzRjMThmYzE3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDEzMTQzNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZhZTdkOGNkYmNkYmIzZDFjNDc2OTdmNWQ2YzIzMzMyMGQ4NTVhNjExMzkxNTc1NDI5ZGQ5N2ZhZjYzZjhkYjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.2qk0AeAli1haFx6-PUx3KVBqjXoxg2hYZdF7aU9GBUI">

Skip-gram과 CBoW는 학습 데이터의 구성 방식에 차이가 있다. 
- CBoW: 하나의 윈도에서 하나의 학습 데이터가 만들어짐
- Skip-gram: 중심 단어와 주변 단어를 하나의 쌍으로 하여 여러 학습 데이터가 만들어짐

데이터 구성 방식 차이 때문에 Skip-gram은 하나의 중심 단어를 통해 여러 개의 주변 단어를 예측하므로 **더 많은 학습 데이터세트**를 추출할 수 있으며, 일반적으로 CBoW보다 더 뛰어난 성능을 보인다.

Skip-gram은 비교적 드물게 등장하느 단어를 더 잘 학습할 수 있게 되고 단어 벡터 공간에서 더 유의미한 거리 관계를 형성할 수 있다.

<img src="https://private-user-images.githubusercontent.com/85439023/430619471-b4e2d048-ebb3-46aa-a444-942abef884b0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4NTkxNzYsIm5iZiI6MTc0Mzg1ODg3NiwicGF0aCI6Ii84NTQzOTAyMy80MzA2MTk0NzEtYjRlMmQwNDgtZWJiMy00NmFhLWE0NDQtOTQyYWJlZjg4NGIwLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDEzMTQzNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQ5NWZiN2Q0Yzg4OTkwYTI0MTlkNzE3YzcwNGNkZDYzY2YwMzUzZjFlNTJhZWU4ZGI1MGI0ZWIyMzMxYmYxODcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.FF7La5_yMJarn7rqakWT95yMU2nCCGp-nJNpdsH2wW8">


1. 입력 단어의 원-핫 벡터를 투사층에 입력하여 해당 단어의 임베딩 벡터를 가져온다.
2. 입력 단어의 임베딩과 $W'_{E \times V}$ 가중치와의 곱셈을 통해 $V$ 크기의 벡터를 얻는다.
3. $V$ 벡터에 소프트맥스 연산을 취하여 주변 단어를 예측한다.

소프트맥스 연산은 모든 단어를 대상으로 내적 연산을 수행한다. 말뭉치의 크기가 커지면 필연적으로 단어 사전의 크기도 커지므로 대량의 말뭉치를 통해 Word2Vec 모델을 학습할 때 학습 속도가 느려지는 단점이 있다.

단점을 보완하는 방법은 계층적 소프트맥스와 네거티브 샘플링 기법을 적용해 학습 속도가 느려지는 문제를 완화할 수 있다.

### 계층적 소프트맥스

**계층적 소프트맥스(Hierachical Softmax)**는 출력층을 이진 트리(Binary tree) 구조로 표현해 연산을 수행한다.
- 자주 등장하는 단어일수록 트리의 상위 노드에 위치
- 드물게 등장하는 단어일수록 하위 노드에 배치

<img src="https://private-user-images.githubusercontent.com/85439023/430620839-17fd491f-1d8c-4c9c-b4d8-3c3d5df6d987.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4NTg2MzgsIm5iZiI6MTc0Mzg1ODMzOCwicGF0aCI6Ii84NTQzOTAyMy80MzA2MjA4MzktMTdmZDQ5MWYtMWQ4Yy00YzljLWI0ZDgtM2MzZDVkZjZkOTg3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDEzMDUzOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc2MGUwNDZmYzhkZmVmNjg1MjFmNDFmNWU3NDMwOTY4YmM4YTEyYzgyNmY2Y2IxZDEwNDQyOWQzZDY2YWIwZWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.INMIjhp8zk_GzkQ4_NqR5p9OhYvII3hKouofD4ZidAQ">

각 노드는 학습이 가능한 벡터를 가지며, 입력값은 해당 노드의 벡터와 내적값을 계산한 후 시그모이드 함수를 통해 확률을 계산한다.

**잎 노드(Leaf Node)**는 가장 깊은 노드로, 각 단어를 의미하며, 모델은 각 노드의 벡터를 촤적화하여 단어를 잘 예측할 수 있게 한다. 각 단어의 확률은 경로 노드의 확률을 곱해서 구할 수 있다.

ex. '추천해요' &rarr; $0.43 \times 0.74 \times 0.27 = 0.085914$ 의 확률을 갖게 된다. 이 경우 학습 시 1, 2번 노드의 벡터만 최적화하면 된다.

단어 사전 크기를 $V$라고 했을 때 일반적은 소프트맥스 연산은 $O(V)$의 시간 복잡도를 갖지만, 계층적 소프트맥스의 시간 복잡도는 $O(log_2 \ V)$의 시간 복잡도를 갖는다.


### 네거티브 샘플링

### 모델 실습: Skip-gram

### 모델 실습: Gensim

## 🦥 fastText