---
title: "임베딩(2)"
excerpt: "Word2Vec / fastText"

categories:
  - NLP
tags:
  - [NLP]

permalink: /nlp/embedding-2/

toc: true
toc_sticky: true

date: 2025-04-05
last_modified_at: 2025-04-06
---


## 🦥 Word2Vec

**Word2Vec**은 단어 간의 유사성을 측정하기 위해 분포 가설(distributional hypothesis)을 기반으로 개발된 임베딩 모델이다.

- 분포 가설: 같은 문맥에서 함께 자주 나타나는 단어들은 서로 유사한 의미를 가질 가능성이 높다는 가정이며 단어 간의 **동시 발생(co-occurrence)** 확률 분포를 이용해 단어 간의 유사성을 측정

ex. '내일 자동차를 타고 부산에 간다' 와 '내일 비행기를 타고 부산에 간다' 라는 두 문장에서 '자동차'와 '비행기'는 주변에 분포한 단어들이 동일하거나 유사하므로 두 단어는 비슷한 의미를 가질 것이라고 예상

가정을 통해 단어의 **분산 표현(Distributed Representation)**을 학습할 수 있다.
- 분산 표현: 단어를 고차원 벡터 공간에 매핑하여 단어의 의미를 담은 것을 의미

분포 가설에 따라 유사한 문맥에서 등장하는 단어는 비슷한 벡터 공간상 위치를 갖게 된다. 위의 예시에서 '비행기'와 '자동차'는 벡터 공간에서 서로 가까운 위치에 표현된다.

이는 빈도 기반 벡터화 기법에서 발생했던 단어의 의미 정보를 저장하지 못하는 한계를 극복했으며, 대량의 텍스트 데이터에서 단어 간의 관계를 파악하고 벡터 공간상에서 유사한 단어를 군집화해 단어의 의미 정보를 효과적으로 표현한다.

### 단어 벡터화

단어를 벡터화하는 방법은 크게 **희소 표현(sparse representation)**과 **밀집 표현(dense representation)**으로 나눌 수 있다. 
- 희소 표현: 빈도 기반 방법으로 대표적으로는 원-핫 인코딩, TF-IDF가 존재
    - 대부분의 벡터 요소가 0으로 표현
    - 단어 사전의 크기가 커지면 벡터의 크기도 커지므로 공간적 낭비가 발생
    - 단어 간의 유사성을 반영하지 못함
    - 벡터 간의 유사성을 계산하는 데도 많은 비용이 발생

    | 단어|   |   |   |    |   |
    |----|---|---|---|----|---|
    |소   |0  |1  |0  | 0  | 0 |
    |잃고 |1  |0  | 0 |  0 | 0 |
    |외양간| 0 | 0 | 0 | 1 | 0 |
    |고친다| 0 | 0 | 0 | 0 | 1 |

- 밀집 표현: Word2Vec
    - 단어를 고정된 크기의 실수 벡터로 표현하기 때문에 단어 사전의 크기가 커지더라도 벡터의 크기가 커지지 않음
    - 벡터 공간상에서 단어 간의 거리를 효과적으로 계산할 수 있으며, 벡터의 대부분이 0이 아닌 실수로 이루어져 있어 효율적으로 공간을 활용

    | 단어|   |   |   |    |   |
    |----|---|---|---|----|---|
    |소   |0.3914|-0.1749| ... |0.5912|0.1321|
    |잃고 |-0.2893|0.3814| ... |-0.1492|-0.2814|
    |외양간|0.4812|0.1214| ... |-0.2745|0.0132|
    |고친다|-0.1314|-0.2809| ... |0.2014|0.3016|

밀집 표현 벡터화는 학습을 통해 단어를 벡터화하기 때문에 단어의 의미를 비교할 수 있다. 밀집 표현된 벡터를 **단어 임베딩 벡터(Word Embedding Vector)**라고 하며, Word2Vec은 대표적인 단어 임베딩 기법 중 하나다.

Word2Vec은 밀집 표현을 위해 CBoW와 Skip-gram이라는 두 가지 방법을 사용한다.

### CBoW

**CBoW(Continuous Bag of Words)**란 주변에 있는 단어를 가지고 중간에 있는 단어를 예측하는 방법이다. 
- 중심 단어(Center Word): 측해야 할 단어를 의미
- 주변 단어(Context Word): 예측에 사용되는 단어들

중심 단어를 맞추기 위해 몇 개의 주변 단어를 고려할지를 정해야 하는데, 이 범위를 **윈도(Window)**라고 한다. 이 윈도를 활용해 주어진 하나의 문장에서 첫 번째 단어부터 중심 단어로 하여 마지막 단어까지 학습한다.
- 윈도가 N일 때, 범위는 중심 단어의 앞에 위치한 N개의 주변 단어부터 뒤에 위치한 N개의 주변 단어이다.

학습을 위해 윈도를 이동해 가며 학습하는데, 이러한 방법을 **슬라이딩 윈도(Sliding Window)**라 한다. CBoW는 슬라이딩 윈도를 사용해 한 번의 학습으로 여러 갱의 중심 단어와 그에 대한 주변 단어를 학습할 수 있다.

<img src="https://private-user-images.githubusercontent.com/85439023/430617831-2e100fec-e458-42e3-b90d-684f4d3dc1a5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4NTg5NzgsIm5iZiI6MTc0Mzg1ODY3OCwicGF0aCI6Ii84NTQzOTAyMy80MzA2MTc4MzEtMmUxMDBmZWMtZTQ1OC00MmUzLWI5MGQtNjg0ZjRkM2RjMWE1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDEzMTExOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY3OGU2OWZjMWY0MjZjOGQ4ZTRlMzc0NzRlMzY0Y2Y2YzEyODM2M2Q1OTg5ZmFiMWE1NmYzMDNiMjI0NmIyNWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.ZZibNpBN2W4hQX7b-c1Ta_RJIhRRdaENyKod4HGJ9wk">

위의 그림은 하나의 입력 문장에서 윈도 크기가 2일 때 학습 데이터가 어떻게 구성되는지를 보여준다.

학습 데이터는 (주변 단어 \ 중심 단어)로 구성된다. 이를 통해 대량의 말뭉치에서 효율적으로 단어의 분산 표현을 학습할 수 있다. 얻어진 학습 데이터는 인공 신경망을 학습하는데 사용된다.

<img src="https://private-user-images.githubusercontent.com/85439023/430618041-7694ed72-b24f-43d6-a8e7-045c3692ff7c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4NTg5NzgsIm5iZiI6MTc0Mzg1ODY3OCwicGF0aCI6Ii84NTQzOTAyMy80MzA2MTgwNDEtNzY5NGVkNzItYjI0Zi00M2Q2LWE4ZTctMDQ1YzM2OTJmZjdjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDEzMTExOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTliZTlmZjNkOWNhMjNiOTA3NzQxN2U3ZGVkYzgxMmUxZmJhZDMzNzVhNTJlM2QyNzYwNjk1YTFiMWYxNWVmYTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.elZpGMRZev0DYjfzUA9U5xbXoh1M3_9Lu5wTaDVs6n8">

CBoW 모델은 각 입력 단어의 원-핫 벡터를 입력값으로 받는다. 입력 문장 내 모든 단어의 임베딩 벡터를 평균 내어 중심 단어의 임베딩 벡터를 예측한다.

1. 입력 단어는 원-핫 벡터로 표현돼 **투사층(Projection Layer)**에 입력된다.
- 투사층: 원-핫 벡터의 인덱스에 해당하는 임베딩 벡터를 반환하는 **순람표(Lookup table, LUT)** 구조
2. 투사층을 통과하면 각 단어는 E 크기의 임베딩 벡터로 변환한다.
- 입력된 임베딩 벡터 $V_1, V_2, ... , V_n$들의 평균값을 계산
3. 계산된 평균 벡터를 가중치 행렬 $W'_{E \times V}$와 곱하면 $V$ 크기의 벡터를 얻는다.
4. 소프트맥스 함수를 이용해 중심 단어를 예측한다.

### Skip-gram

**Skip-gram**은 CBoW와 반대로 중심 단어를 입력으로 받아서 주변 단어를 예측하는 모델이다.
- 중심 단어를 기준으로 양쪽으로 윈도 크기만큼의 단어들을 주변 단어로 삼아 훈련 데이터세트를 만든다.
- 중심 단어와 각 주변 단어를 하나의 쌍으로 하여 모델을 학습시킨다.

<img src="https://private-user-images.githubusercontent.com/85439023/430619463-9166523b-8ce5-4d64-8380-b7d34c18fc17.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4NTkxNzYsIm5iZiI6MTc0Mzg1ODg3NiwicGF0aCI6Ii84NTQzOTAyMy80MzA2MTk0NjMtOTE2NjUyM2ItOGNlNS00ZDY0LTgzODAtYjdkMzRjMThmYzE3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDEzMTQzNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZhZTdkOGNkYmNkYmIzZDFjNDc2OTdmNWQ2YzIzMzMyMGQ4NTVhNjExMzkxNTc1NDI5ZGQ5N2ZhZjYzZjhkYjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.2qk0AeAli1haFx6-PUx3KVBqjXoxg2hYZdF7aU9GBUI">

Skip-gram과 CBoW는 학습 데이터의 구성 방식에 차이가 있다. 
- CBoW: 하나의 윈도에서 하나의 학습 데이터가 만들어짐
- Skip-gram: 중심 단어와 주변 단어를 하나의 쌍으로 하여 여러 학습 데이터가 만들어짐

데이터 구성 방식 차이 때문에 Skip-gram은 하나의 중심 단어를 통해 여러 개의 주변 단어를 예측하므로 **더 많은 학습 데이터세트**를 추출할 수 있으며, 일반적으로 CBoW보다 더 뛰어난 성능을 보인다.

Skip-gram은 비교적 드물게 등장하느 단어를 더 잘 학습할 수 있게 되고 단어 벡터 공간에서 더 유의미한 거리 관계를 형성할 수 있다.

<img src="https://private-user-images.githubusercontent.com/85439023/430619471-b4e2d048-ebb3-46aa-a444-942abef884b0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4NTkxNzYsIm5iZiI6MTc0Mzg1ODg3NiwicGF0aCI6Ii84NTQzOTAyMy80MzA2MTk0NzEtYjRlMmQwNDgtZWJiMy00NmFhLWE0NDQtOTQyYWJlZjg4NGIwLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDEzMTQzNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQ5NWZiN2Q0Yzg4OTkwYTI0MTlkNzE3YzcwNGNkZDYzY2YwMzUzZjFlNTJhZWU4ZGI1MGI0ZWIyMzMxYmYxODcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.FF7La5_yMJarn7rqakWT95yMU2nCCGp-nJNpdsH2wW8">


1. 입력 단어의 원-핫 벡터를 투사층에 입력하여 해당 단어의 임베딩 벡터를 가져온다.
2. 입력 단어의 임베딩과 $W'_{E \times V}$ 가중치와의 곱셈을 통해 $V$ 크기의 벡터를 얻는다.
3. $V$ 벡터에 소프트맥스 연산을 취하여 주변 단어를 예측한다.

소프트맥스 연산은 모든 단어를 대상으로 내적 연산을 수행한다. 말뭉치의 크기가 커지면 필연적으로 단어 사전의 크기도 커지므로 대량의 말뭉치를 통해 Word2Vec 모델을 학습할 때 학습 속도가 느려지는 단점이 있다.

단점을 보완하는 방법은 계층적 소프트맥스와 네거티브 샘플링 기법을 적용해 학습 속도가 느려지는 문제를 완화할 수 있다.

### 계층적 소프트맥스

**계층적 소프트맥스(Hierachical Softmax)**는 출력층을 이진 트리(Binary tree) 구조로 표현해 연산을 수행한다.
- 자주 등장하는 단어일수록 트리의 상위 노드에 위치
- 드물게 등장하는 단어일수록 하위 노드에 배치

<img src="https://private-user-images.githubusercontent.com/85439023/430620839-17fd491f-1d8c-4c9c-b4d8-3c3d5df6d987.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4NTg2MzgsIm5iZiI6MTc0Mzg1ODMzOCwicGF0aCI6Ii84NTQzOTAyMy80MzA2MjA4MzktMTdmZDQ5MWYtMWQ4Yy00YzljLWI0ZDgtM2MzZDVkZjZkOTg3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDEzMDUzOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc2MGUwNDZmYzhkZmVmNjg1MjFmNDFmNWU3NDMwOTY4YmM4YTEyYzgyNmY2Y2IxZDEwNDQyOWQzZDY2YWIwZWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.INMIjhp8zk_GzkQ4_NqR5p9OhYvII3hKouofD4ZidAQ">

각 노드는 학습이 가능한 벡터를 가지며, 입력값은 해당 노드의 벡터와 내적값을 계산한 후 시그모이드 함수를 통해 확률을 계산한다.

**잎 노드(Leaf Node)**는 가장 깊은 노드로, 각 단어를 의미하며, 모델은 각 노드의 벡터를 촤적화하여 단어를 잘 예측할 수 있게 한다. 각 단어의 확률은 경로 노드의 확률을 곱해서 구할 수 있다.

ex. '추천해요' &rarr; $0.43 \times 0.74 \times 0.27 = 0.085914$ 의 확률을 갖게 된다. 이 경우 학습 시 1, 2번 노드의 벡터만 최적화하면 된다.

단어 사전 크기를 $V$라고 했을 때 일반적은 소프트맥스 연산은 $O(V)$의 시간 복잡도를 갖지만, 계층적 소프트맥스의 시간 복잡도는 $O(log_2 \ V)$의 시간 복잡도를 갖는다.


### 네거티브 샘플링

**네거티브 샘플링(Negative Sampling)**은 Word2Vec 모델에서 사용되는 확률적인 샘플링 기법으로 전체 단어 집합에서 일부 단어 샘플링하여 오답 단어로 사용한다.

학습 윈도 내에 등장하지 않는 단어를 n개 추출하여 정답 단어와 함께 소프트맥스 연산을 수행한다. 이를 통해 전체 단어의 확률을 계산할 필요 없이 모델을 효율적으로 학습할 수 있다.
- n은 일반적으로 5 ~ 20개를 사용

네거티브 샘플링의 추출 확률은 아래 수식을 통해 구할 수 있다.

$$P(w_i) = \frac {f(w_i)^{0.75}}{\sum_{j = 0}^{V}f(w_j)^{0.75}}$$

- $f(w_i)$: 각 단어 $w_i$의 출형 빈도수
  - '추천해요' 100번 등장하고 전체 단어의 빈도가 2000이라면 $f(추천해요) = \frac{100}{2000} = 0.05$
- $P(w_i)$: 단어 $w_i$가 네거티브 샘플로 추출될 확률
  - 출현 빈도수에 0.75제곱한 값을 정규화 상수로 사용하는데, 이 값은 실험을 통해 얻어진 최적의 값이다.

네거티브 샘플링에서는 입력 단어 쌍이 데이터로부터 추출된 단어 쌍인지, 아니면 네거티브 샘플링으로 생성된 단어 쌍인지 이진 분류를 한다. 이를 위해 로지스틱 회귀 모델을 사용하며, 이 모델의 학습 과정에서는 추출할 단어의 확률 분포를 구하기 위해 먼저 각 단어에 대한 가중치를 학습한다.

<img src="https://private-user-images.githubusercontent.com/85439023/430693482-a64e8d78-dedd-4628-9c8f-c100686557c7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM5MzM0MTAsIm5iZiI6MTc0MzkzMzExMCwicGF0aCI6Ii84NTQzOTAyMy80MzA2OTM0ODItYTY0ZThkNzgtZGVkZC00NjI4LTljOGYtYzEwMDY4NjU1N2M3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA2VDA5NTE1MFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY2YWIxODJjMTI1YmIxOGQ1YTIwZTVjZTM4YjZkMmFhZTBmYWZjM2I1N2RjNGFlOTIxZmQwYTUwMWU2YmQyYWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.X1cNQ1uvv6TXCIaY3JDJ48PVGGLSnrYLp5GenRdcpc0">

네거티브 샘플링 Word2Vec 모델은 실제 데이터에서 추출된 단어 쌍은 1로, 네거티브 샘플링을 통해 추출된 가짜 단어쌍은 0으로 레이블링한다. 즉, 다중 분류에서 이진 분류로 학습 목적이 바뀌게 된다.

<img src="https://private-user-images.githubusercontent.com/85439023/430693483-9fbc8029-3068-47f5-9bfd-5399272bf03e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM5MzM0MTAsIm5iZiI6MTc0MzkzMzExMCwicGF0aCI6Ii84NTQzOTAyMy80MzA2OTM0ODMtOWZiYzgwMjktMzA2OC00N2Y1LTliZmQtNTM5OTI3MmJmMDNlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA2VDA5NTE1MFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc0NjViNjMyYzg0MjBjMmJhNWQzZmJmZjU5NzRhOTE0ZWI4YzliOGE1ZWUwNWUwZmNlYmM3OWIwMjcxNGJmNjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.gNdAWUj6EU83PQYEi-g3dXbgPqIOuVH4tVWAk66jm78">

네거티브 샘플링 모델에서는 입력 단어의 임베딩과 해당 단어가 맞는지 여부를 나타내는 레이블(1 또는 0)을 가져와 내적 연산을 수행한다. 내적 연산을 통해 얻은 값은 시그모이드 함수를 통해 확률값으로 변환된다.
- 레이블이 1인 경우: 해당 확률값이 높아지도록 가중치를 최적화
- 레이블이 0인 경우: 해당 확률값이 낮아지도록 가중치를 최적화

### 모델 실습: Skip-gram

Word2Vec 모델은 학습할 단어의 수를 $V$로, 임베딩 차원을 $E$로 설정해 $W_{V \times E}$ 행렬과 $W'_{E \times V}$ 행렬을 최적화하며 학습한다.
- $W_{V\timesE}$ 행렬은 **룩업(Lookup)** 연산을 수행하며 이는 **임베딩(`Embedding`)** 클래스를 사용하여 구현이 가능

임베딩 클래스는 단어나 범주형 변수와 같은 이산 변수를 연속적인 벡터 형태로 변환해 사용할 수 있다. 연속적인 벡터 표현은 모델이 학습하는 동안 단어의 의미와 관련된 정보를 포착하고, 이를 기반으로 단어 간의 유사도를 계산한다.

```python
# 임베딩 클래스
embedding = torch.nn.Embedding(
  num_embeddings,
  embedding_dim,
  padding_idx=None,
  max_norm=None,
  norm_type=2.0
)
```

- `num_embeddings`: 이산 변수의 개수로 단어 사전의 크기를 의미
- `embedding_dim`: 임베딩 벡터의 차원 수로 임베딩 벡터의 크기를 의미
- `padding_idx`: 패딩 토큰의 인덱스를 지정해 해당 인덱스의 임베딩 벡터를 0으로 설정
  - 병렬 처리는 입력 배치의 문장 길이가 동일해야 하므로 입력 분장들을 일정한 길이로 설정
- `norm_type`: 임베딩 벡터의 크기를 제한하는 방법을 선택
  - 기본값은 2로 L2 정규화 방식을 사용하며 1로 설정하면 L1 정규화 방식을 사용한다.
- `max_norm`: 임베딩 벡터의 최대 크기를 지정
  - 각 임베딩 벡터의 크기가 최대 노름 값 이상이면 임베딩 벡터를 최대 노름 크기로 잘라내고 크기를 감소시킨다.

```python
# 기본 Skip-gram 클래스
from torch import nn

class VanillaSkipgram(nn.Module):
  def __init__(self, vocab_size, embedding_dim):
    super().__init__()
    self.embedding = nn.Embedding(
      num_embeddings=vocab_size,
      embedding_dim=embedding_dim
    )
    self.linear = nn.Linear(
      in_features=embedding_dim,
      out_features=vocab_size
    )
  
  def forward(self, input_ids):
    embeddings = self.embedding(input_ids)
    output = self.linear(embeddings)
    return output
```

기본 형식의 Skip-gram 모델은 입력 단어와 주변 단어를 룩업 테이블에서 가져와서 내적을 계산한 다음, 손실 함수를 통해 예측 오차를 최소화하는 방식으로 학습된다.

```python
# 영화 리뷰 데이터세트 전처리
import pandas as pd
from Korpora import Korpora
from konlpy.tag import Okt

corpus = Korpora.load("nsmc")
corpus = pd.DataFrame(corpus.test)

tokenizer = Okt()
tokens = [tokenizer.morphs(review) for review in corpus.text]
```

데이터세트를 `Okt` 토크나이저를 사용해 형태소를 추출하고 이를 통해 단어 사전을 구축한다.

```python
# 단어 사전 구축
from collections import Counter

def build_vocab(corpus, n_vocab, special_tokens):
  counter = Counter()
  for tokens in corpus:
    counter.update(tokens)
  vocab = special_tokens
  for token, count in counter.most_common(n_vocab):
    vocab.append(token)
  return vocab


vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=["<unk"])
token_to_id = {token: idx for idx, token in enumerate(vocab)}
id_to_token = {idx: token for idx, token in enumerate(vocab)}
```

`Okt` 토크나이저를 통해 토큰화된 데이터를 활용해 `build_vocab` 함수로 단어 사전을 구축한다.
- `n_vocab`: 구축할 단어 사전의 크기
  - 문서 내에 `n_vocab`보다 많은 종류의 토큰이 있다면, 가장 많이 등장한 토큰 순서로 사전을 구축
- `special_tokens`: 특별한 의미를 갖는 토큰들을 의미
  - `<unk>` 토큰은 OOV에 대응하기 위한 토큰으로 단어 사전 내에 없는 모든 단어는 `<unk>` 토큰으로 대체

단어 사전의 크기는 구축할 단어 사전의 크기와 **특수 토큰(Special Token)**의 크기 합과 동일하다.

그 다음은 윈도 크기를 정의하고 학습에 사용될 단어 쌍을 추출한다.

```python
# Skip-gram의 단어 쌍 추출
def get_word_pairs(tokens, window_size):
  pairs=[]
  for sentence in tokens:
    sentence_length = len(setence)
    for idx, center_word in enumerate(sentence):
      window_start = max(0, idx - window_size + 1)
      window_end = min(sentence_length, idx + window_size + 1)
      center_word = sentence[idx]
      context_words = sentence[window_start:idx] + sentence[idx+1:window_end]
      for context_word in context_words:
        pairs.append([center_word, context_word])
  return pairs

word_pairs = get_word_pairs(tokens, window_size=2)
```

`get_word_pairs` 함수는 토큰을 입력받아 Skip-gram 모델의 입력 데이터로 사용할 수 있게 전처리한다. 
- `window_size`: 주변 단어를 몇 개까지 고려할 것인지를 설정한다.
  - 각 문장에서는 중심 단어와 주변 단어를 고려하여 쌍을 생성
- `idx`: 현재 단어의 인덱스를 나타냄
- `center_word`: 중심 단어
- `window_start` & `window_end`: 현재 단어에서 얼마나 멀리 떨어진 주변 단어를 고려할 것인지를 결정
  - 문장의 경계를 넘어가는 경우가 없게 조정

출력 결과는 각 단어 쌍이 [중심 단어, 주변 단어]로 구성되어 있다. 임베딩 층은 단어의 인덱스를 입력으로 받기 때문에 단어 쌍을 인덱스 쌍으로 변환해야 한다.

```python
def get_index_pairs(word_pairs, token_to_id):
  pair = []
  unk_idx = token_to_id["<unk>"]
  for word_pair in word_pairs:
    center_word, context_word = word_pair
    center_index = token_to_id.get(center_word, unk_index)
    context_index = token_to_id.get(context_word, unk_index)
    pairs.append([center_index, context_index])
return pairs

index_pairs = get_index_pairs(word_pairs, token_to_id)
```

`get_index_pairs` 함수는 `get_word_pairs` 함수에서 생성된 단어 쌍을 토큰 인덱스 쌍으로 변환한다.
- `word_pairs` 단어와 해당 단어의 ID를 매핑한 딕셔너리인 `token_to_id`로 인덱스 쌍을 생성
- `get` 메서드로 토큰이 단어 사전 내에 있으면 해당 토큰의 인덱스를 반환하고, 단어 사전 내에 없다면 `<unk>` 토큰의 인덱스를 반환

생성된 인덱스 쌍은 Skip-gram 모델의 입력 데이터로 사용되며 이를 학습에 사용하기 위해서는 텐서 형식으로 변환해야한다.

```python
# 데이터로더 적용
import torch
from torch.utils.data import TensorDataset, DataLoader

index_pairs = torch.tensor(index_pairs)
center_indexs = index_pairs[:, 0]
contenxt_indexs = index_pairs[:, 1]

dataset = TensorDataset(center_indexs, context_indexs)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

`index_pairs`는 `get_index_pairs` 함수에서 생성된 중심 단어와 주변 단어 토큰의 인덱스 쌍으로 이루어진 리스트다. 이 리스트를 텐서 형식으로 변환한다. 이 텐서는 [N, 2]의 구조를 가지므로 중심 단어와 주변 단어로 나눠 데이터세트로 변환한다.

인덱스 싸을 텐서 데이터세트로 변환하고 데이터로더에 적용했다면 모델을 학습하기 위한 준비 작업을 진행한다.

```python
# Skip-gram 모델 준비 작업
from torch import optim

device = "cuda" if torch.cuda.is_available() else "cpu"
word2Vec = VaillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.SGD(word2vec.parameters(), lr=0.1)
```
`VanillaSkipgram` 클래스의 단어 사전 크기(`vocab_size`)에 전체 단어 집합의 크기를 전달하고 임베딩 크기(embedding_dim)는 128로 할당한다.

손실함수는 단어 사전 크기만큼 클래스가 있는 분류 문제이므로 교차 엔트로피를 사용하고 교차 엔트로피는 내부적으로 소프트맥스 연산을 수행하므로 신경망의 출력값을 후처리 없이 활용할 수 있다.


### 모델 실습: Gensim

## 🦥 fastText