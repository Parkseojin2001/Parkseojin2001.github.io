---
title: "10장 케라스를 사용한 인공 신경망 소개(1)"
excerpt: "인공 신경망(ANN) / 딥러닝 / 다층 퍼셉트론"

categories:
  - 핸즈온 머신러닝
tags:
  - [hands-on]

permalink: /hands-on/ANN-1/

toc: true
toc_sticky: true
math: true

date: 2025-02-08
last_modified_at: 2025-02-09
---

지능적인 기계를 만드는 법에 대한 영감을 얻으려면 뇌 구조를 살펴보는 것이 합리적이다. 이는 **인공신경망(ANN; Artificial Neural Network)**을 촉발시킨 근원이다. 인공신경망은 뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델이다. 하지만 최근 인공 신경망은 생물학적 뉴런에서 점점 멀어지고 있으며 이러한 특징을 반영하기 위해 뉴런을 대신해 **유닛(unit)**이라고 부른다.

# 10.1 생물학적 뉴런에서 인공 뉴런까지

<img src="https://user-images.githubusercontent.com/78655692/146319097-9bb0cf08-ca8d-452f-8611-ab0cdb036409.png">

예전과 다르게 인공 신경망이 우리 생활에 훨씬 커다란 영향을 줄 것이라고 믿고 있다.
- 신경망을 훈련하기 위한 데이터가 엄청 많아졌으며 인공 신경망은 종종 규모가 크고 복잡한 문제에서 다른 머신러닝 기법보다 좋은 성능을 낸다.
- 1990년대 이후 컴퓨터 하드웨어가 크게 발전했다. 덕분에 납득할 만한 시간 안에 대규모 신경망을 훈련할 수 있다. 또한 수백만 개의 강력한 GPU 카드를 생산해내는 게임 산업 덕분이기도 하고 클라우드 플랫폼은 이런 강력한 도구를 손쉽게 사용할 수 있는 환경을 제공한다.
- 훈련 알고리즘이 향상되었다.

## 10.1.1 생물학적 뉴런

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fce3B22%2FbtsHBSsMmYd%2FmSVpSQcG6MCNs8NRM2rXAk%2Fimg.png">

## 10.1.2 뉴런을 사용한 논리 연산

생물학적 뉴런에서 착안한 매우 단순한 신경망 모델을 제안했는데 이것은 나중에 **인공 뉴런**이 되었다. 이 모델은 하나 이상의 이진(on/off) 입력과 이진 출력 하나를 가지며 입력이 일정 개수만큼 활성화되었을 때 출력을 내보낸다. 또한 인공 뉴런을 이용해 논리 연산을 수행할 수도 있다.

<img src="https://user-images.githubusercontent.com/78655692/146321804-9fe68a2f-880e-4c72-a59b-bc9c5f6fb175.png">

①은 항등함수를 의미하며, 뉴런 A가 활성화 되면 뉴런 C 또한 활성화된다.<br>
②는 논리곱 연산을 의미하며, 뉴런 A와 B가 모두 활성화될 때만 뉴런 C가 활성화된다.<br>
③은 논리합 연산을 의미하며, 뉴런 A와 B 둘 중 하나라도 활성화되면 C가 활성화된다.<br>
④는 어떤 입력이 뉴런의 활성화를 억제할 수 있다고 가정하면, 위의 그림에서 처럼 뉴런 A가 활성화 되고 B가 비활성화될 때 뉴런 C가 활성화된다.

## 10.1.3 퍼셉트론

**퍼셉트론**은 가장 간단한 인공 신경망 구조 중 하나이며 TLU 또는 LTU라고 불리는 조금 다른 형태의 인공 뉴런을 기반으로 한다. TLU는 입력의 가중치 합을 계산($z=w_1x_1+w_2x_2+ ... + w_nx_n = \bf{x^Tw} $)한 뒤 계산된 합에 **계단 함수(step function)**를 적용하여 결과를 출력한다. 즉, $h_\bf{w}(\bf{x}) =$ step($z$), $z = \bf{x^Tw}$

<img src="https://user-images.githubusercontent.com/78655692/146322983-e8b764e7-e63c-4a9b-9fc6-b12e2840ced3.png">

퍼셉트론에서 가장 널리 사용되는 계단 함수는 **헤비사이드 계단 함수**이다. 부호 함수를 대신 사용하기도 한다.

$$
heaviside(z) = \begin{cases}
{0} \quad z < 0\\
{1} \quad \; z \ge 0
\end{cases}

\quad
\quad

sgn(z) = \begin{cases}
{-1} \quad z < 0\\
{0} \quad \; z = 0\\
{1} \quad \;\; z > 0
\end{cases}
$$

하나의 TLU는 간단한 선형 이진 분류 문제에 사용할 수 있다. 입력의 선형 조합을 계산해서 그 결과가 임계값을 넘으면 양성 클래스를 출력하고 그렇지 않으면 음성 클래스를 출력한다.

퍼셉트론은 층이 하나뿐인 TLU로 구성된다. 각 TLU은 모든 입력에 연결되어 있다. 한 층에 있는 모든 뉴런이 이전 층의 모든 뉴런과 연결되어 있을 때 이를 **완전 연결 층** 또는 **밀집 층**이라고 부른다. 퍼셉트론의 입력은 **입력 뉴런**이라고 불리는 특별한 통과 뉴런에 주입이되며 이 뉴런은 어떤 입력이든지 그냥 출력으로 통과시킨다. 

<img src="https://user-images.githubusercontent.com/78655692/146323635-ef8fdc8e-d06e-408d-aa4e-ab51b70327aa.png">

**입력층**은 모두 이런 입력 뉴런으로 구성되며 보통 **편향** 특성이 더해진다($x_0=1$). 이 편향 특성은 항상 1을 출력하는 특별한 종류의 뉴런인 편향 뉴런으로 표현된다. 위의 그림은 입력 두 개와 출력 세개로 구성된 퍼셉트론으로 세 개의 다른 이진 클래스로 동시에 분류할 수 있는 다중 출력 분류기이다.<br>

한 번에 여러 샘플에 대해 인공 뉴런 층의 출력을 효율적으로 계산하는 방식은 아래와 같다.

$$
h_{W,b}(\bf{X}) = \phi(\bf{XW+b})
$$

- $\bf{X}$: 입력 특성의 행렬
  - 이 행렬의 행은 샘플, 열은 특성
- $\bf{W}$: 가중치 행렬로 편향 뉴런을 제외한 모든 연결 가중치를 포함한다. 
  - 이 행렬의 행은 입력 뉴런이고 열은 출력층에 있는 인공 뉴런
- $\bf{b}$: 편향 벡터로 편향 뉴런과 인공 뉴런 사이의 모든 연결 가중치를 포함한다. 
  - 인공 뉴런마다 하나의 편향 값이 존재
- $\phi$: **활성화 함수**이며 TLU일 경우 이 함수는 계단 함수이다.

### 퍼셉트론 훈련
퍼셉트론의 훈련 알고리즘은 **헤브의 규칙**에서 영감을 받았다. 퍼셉트론은 네트워크가 예측할 때 만드는 오차를 반영하도록 조금 변형된 규칙을 사용하여 훈련하며 오차가 감소되도록 연결을 강화시킨다. 잘못된 예측을 하는 모든 출력 뉴런에 대해 올바른 예측을 만들 수 있도록 입력에 연결된 가중치를 강화시킨다.

$$
w_{i, j}^{next\,step} = w_{i, j} + \eta(y_j -\hat{y_j})x_i
$$

- $w_{i, j}$: $i$번째 입력 뉴런과 $j$번째 출력 뉴런 사이를 연결하는 가중치
- $x_i$: 현재 훈련 샘플의 $i$번째 뉴런의 입력값
- $\hat{y_j}$: 현재 훈련 샘플의 ${j}$번째 출력 뉴런의 출력값
- $y_{j}$: 현재 훈련 샘플의 ${j}$번째 출력 뉴런의 타깃값
- $\eta$: 학습률

각 출력 뉴런의 결정 경계는 선형이므로 퍼셉트론은 복잡한 패턴을 학습하지 못한다. 하지만 훈련 샘플이 선형적으로 구분될 수 있다면 이 알고리즘이 정답에 수렴한다는 것을 증명했으며 이를 **퍼셉트론 수렴 이론**이라고 한다.

### TLU 네트워크 구현 from sklearn 

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris()
X = iris.data[:, (2, 3)]  # 꽃잎의 길이과 너비
y = (iris.target == 0).astype(np.int)   # 부채붓꽃(Iris Setosa)인가?

per_clf = Perceptron()
per_clf.fit(X, y)

y_pred = per_clf.predict([[2, 0.5]])
```

하나의 퍼셉트론으로는 일부 간단한 문제를 풀 수 없지만 퍼셉트론을 여러 개 쌓아올리면 일부 제약을 줄일 수 있다. 이런 인공신경망을 **다층 퍼셉트론(MLP)**한다.
- MLP는 XOR 문제를 풀 수 있지만 하나의 퍼셉트론으로는 XOR 문제를 풀 수 없다.

<img src="https://velog.velcdn.com/images/kyungmin1029/post/76c79d04-32a0-4025-97cf-4ef338daa886/image.png">


## 10.1.4 다층 퍼셉트론과 역전파

다층 퍼셉트론은 **입력층(input layer)** 하나와 **은닉층(hidden layer)**이라 불리는 하나 이상의 TLU층과 **출력층(output layer)**으로 구성된다. 입력층과 가까운 층을 보통 **하위 층(lower layer)**이라 부르고 출력에 가까운 층을 **상위 층(upper layer)**이라 부른다. 출력층을 제외하고 모든 층은 편향 뉴런을 포함하며 다음 층과 완전히 연결되어 있다.

<img src="https://user-images.githubusercontent.com/78655692/146354772-28308ce6-34e1-415f-be77-7b268cef2d04.png">

은닉층을 여러 개 쌓아 올린 인공 신경망을 **심층 신경망(DNN)**이라고 한다. 다층 퍼셉트론은 **역전파(backpropagation)**훈련 알고리즘을 이용한다. 간단히 말하면 그레이디언트를 자동으로 계산하는 경사 하강법이다. 네트워크를 두 번(정방향 한번, 역방향 한 번) 통과하는 것만으로 이 역전파 알고리즘은 모든 모델 파라미터에 대한 네트워크 오차의 그레이디언트를 계산할 수 있다. 즉 오차 감소를 위해 가중치와 편향값을 바꾸는 과정이다.<br>

역전파 알고리즘은 한 번에 하나의 미니배치씩 진행하여 전체 훈련 세트를 처리하며 이 과정을 반복한다. 각 반복을 **에포크(epoch)**라고 부른다. 

### 훈련 알고리즘

1. 각 미니배치는 네트워크의 입력층으로 전달되어 첫 번째 은닉층으로 보내진다. 
2. 그 다음 해당 층에 있는 모든 뉴런의 출력을 계산하고 이 결과를 다음 층으로 전달된다. 
3. 다시 이 층의 출력을 계산하고 결과는 다음 층으로 전달된다. 
4. 이 방식으로 마지막 층인 출력층의 출력을 계산할 때까지 계속한다.
5. 정방향 계산(1 ~ 4)이 끝나면 네트워크의 출력 오차를 측정한다. 즉, 손실 함수를 사용하여 기대하는 출력과 네트워크의 실제 출력을 비교하고 오차 측정 값을 반환한다. 
6. 그 다음 각 출력 연결이 이 오차에 기여하는 정도를 **연쇄 법칙**을 적용하여 계산한다.
7. 또 다시 연쇄 법칙을 사용하여 이전 층의 연결 가중치가 이 오차의 기여 정도에 얼마나 기여했는지 측정한다.
8. 입력층에 도달할 때까지 역방향으로 계속된다.
9. 마지막으로 경사 하강법을 수행하여 방금 계산한 오차 그레이디언트를 사용해 네트워크에 있는 모든 연결 가중치를 수정한다.

1 ~ 5단계: 정방향 단계로 역방향 계산을 위해 중간 계산값을 모두 저장<br>
6 ~ 8 단계: 역방향 단계로 오차 그레이디언트를 거꾸로 전파함으로써 네트워크에 있는 모든 연결 가중치에 대한 오차 그레이디언트를 측정<br>
9단계: 경사 하강법 단계로 오차 감소를 위해 가중치 조정<br>

### 왜 활성화 함수가 필요할까?
- 선형 변환을 여러 개 연결해도 얻을 수 있는 것은 선형 변환뿐이다.
- 층 사이에 비선형성을 추가하지 않으면 아무리 층을 많이 쌓아도 하나의 층과 동일해진다. 이런 층으로는 복잡한 문제를 풀 수 없다.
- 반대로 비선형 활성화 함수가 있는 충분히 큰 심층 신경망은 이론적으로 어떤 연속 함수도 근사할 수 있다.

<img src="https://user-images.githubusercontent.com/78655692/146358398-658ecaf2-cd26-4cc9-8cbb-d97a4fdf68a3.png">

## 10.1.5 회귀를 위한 다층 퍼셉트론

다층 퍼셉트론은 회귀 작업에 사용할 수 있다. 값 하나를 예측하는 데 출력 뉴런이 하나만 필요하며 이 뉴런의 출력이 예측된 값이다. 다변향 회귀에서는 출력 차원마다 출력 뉴런이 하나씩 필요하다. 일반적으로 회귀용 다층 퍼셉트론을 만들 때 출력 뉴런에 활성화 함수를 사용하지 않고 어떤 범위의 값도 출력되도록 한다. 하지만 활성화 함수를 사용하는 경우도 있다.
- 출력이 항상 양수여야 한다면 출력층에 ReLU 활성화 함수를 사용할 수 있다.
- 어떤 범위 안의 값을 예측하고 싶다면 로지스틱 함수(0 ~ 1)나 하이퍼볼릭 탄젠트 함수(-1 ~ 1)를 사용하고 레이블의 스케일을 적절한 범위로 조장할 수 있다.

훈련에 사용하는 손실 함수는 전형적으로 평균 제곱 오차이지만 훈련 세트에 이상치가 많다면 대신 평균 절댓값 오차를 사용할 수 있다. 또는 이 둘을 조합한 후버 손실을 사용할 수 있다.

### 회귀 MLP 구조

|하이퍼파라미터|일반적인 값|
|----------|--------|
|입력 뉴런 수|특성마다 하나|
|은닉층 수|문제에 따라 다르며 일반적으로 1에서 5사이|
|은닉층의 뉴런 수|문제에 따라 다르지만 일반적으로 10에서 100 사이|
|출력 뉴런 수|예측 차원마다 하나|
|은닉층의 활성화 함수|ReLU(또는 SELU)|
|출력층의 활성화 함수|일반적으로 없으나 특정 조간의 출력값을 원한다면 사용할 수 있음|
|손실 함수|MSE 또는 MAE/Huber(이상치가 많은 경우)|

## 10.1.6 분류를 위한 다층 퍼셉트론

다층 퍼셉트론은 분류 작업에도 사용할 수 있다. 이진 분류 문제에서는 로지스틱 활성화 함수를 가진 하나의 출력 뉴런만 필요하다. 이때 출력은 0과 1 사이의 실수이다. 이를 양성 클래스에 대한 예측 확률로 해석할 수 있다.
음성 클래스에 대한 예측 확률은 1에서 양성 클래스의 예측 확률을 뺀 값이다.<br>
다층 퍼셉트론은 다중 레이블 이진 분류 문제를 쉽게 처리할 수 있다. 각 샘플이 3개 이상의 클래스에만 속할 수 있다면 클래스마다 하나의 출력 뉴런이 필요하다. 출력층에는 소프트맥스 활성화 함수를 사용해야 한다. 소프트맥스 함수는 모든 예측 확률을 0과 1 사이로 만들고 더했을 때 1이 되도록 만든다. 이를 **다중 분류**라고 한다.

<img src="https://user-images.githubusercontent.com/78655692/146359986-4fc952b8-0e93-48cc-9cfb-932850b03633.png">

확률 분포를 예측해야 하므로 손실 함수에는 일반적으로 크로스 엔트로피 손실(로그 손실)을 선택한다.

### 분류 MLP 구조

|하이퍼파라미터|이진 분류|다중 레이블 분류|다중 분류|
|----------|-------|------------|-------|
|입력층과 은닉층|회귀와 동일|회귀와 동일|회귀와 동일|
|출력 뉴런 수|1개|레이블마다 1개|클래스마다 1개|
|출력층의 활성화 함수|로지스틱 함수|로지스틱 함수|소프트맥스 함수|
|손실 함수|크로스 엔트로피|크로스 엔트로피|크로스 엔트로피|

>**레이블 vs 클래스**<br>
> 레이블은 어떤 데이터의 분류된 정답이다. <br> ex) 첫번째 샘플의 레이블은 1이다.<br>
> 클래스는 레이블들의 집합이다.<br> ex) 정답에는 1, 0이라는 2가지 클래스가 있다.


>**다중(클래스) 분류 vs 다중 레이블 분류**<br>
> 다중 클래스 분류는 두 개 이상의 클래스가 있는 분류 작업을 의미한다. 즉, 다중의 class가 존재할 수 있는 output에서, 하나를 선택하는 것이다. 다중 레이블 분류는 각 샘플에 대상 레이블 세트를 할당한다. 다중의 정답이 동시에 존재할 수 있다.
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmHqtb%2Fbtr0jhtAp3n%2FHrXfTOg8LDG9ELYGBuraG1%2Fimg.jpg">


