---
title: "합성곱 신경망"
description: "CNN / 완전 연결 계층 / 1차원 합성곱 & 자연어 처리"

categories: [파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터 비전 심층학습, NLP]
tags: [NLP, CV]

permalink: /pytorch-book/nlp/cnn/

toc: true
toc_sticky: true
math: true
mermaid: true

date: 2025-04-16
last_modified_at: 2025-04-17
---

**합성곱 신경망(Convolutional Neural Network, CNN)**은 주로 이미지 인식과 같은 컴퓨터 비전 분야의 데이터를 분석하기 위해 사용되는 인공 신경망의 한 종류다. 합성곱 신경망은 입력 데이터의 지역적인 특징을 추출하는 데 특화된 구조를 갖고 있으며 이를 위해 **합성곱(Convolution)** 연산을 사용한다.

합성곱 연산은 이미지 특정 영역에서 입력값의 분포 또는 변화량을 계산해 출력 노드를 생성한다. 특정 영역 안에서 연산을 수행하므로 **지역 특징(Local Features)**을 효과적으로 추출할 수 있다.

이미지 데이터는 고정된 프레임 내에 객체들의 위치와 형태가 자유분방하므로 여러 영역의 지역 특징을 조합해 입력 데이터의 전반적인 **전역 특징(Global Features)**을 파악할 수 있다.

합성곱 신경망은 원래 컴퓨터비전을 위해 고안되었지만, 자연어 처리에서도 사용되기 시작했다. 이를 통해 입력 데이터의 길이에 상관없이 병렬 처리가 가능하고, 학습에 필요한 가중치 수를 줄여 깊은 신경망을 구성할 수 있게 됐다.

## 합성곱 계층
---------

합성곱 계층은 입력 데이터와 필터를 합성곱해 출력 데이터를 생성하는 계층이다.
- 이미지나 음성 데이터와 같은 고차원 데이터를 처리하는 데 주로 사용한다.
- 필터를 사용해 데이터의 특징을 추출하므로 데이터의 지역적인 패턴을 인식할 수 있으며, 입력 데이터의 모든 위치에서 동일한 필터를 사용하므로 모델 매개변수를 공유한다.
  - 모델의 매개변수를 공유함으로써 모델이 학습해야 할 매개변수 수가 감소해 과대적합을 방지한다.
  - 입력 데이터에서 특징을 추출할 때, 해당 특징이 이미지 내 다른 위치에 존재하더라도 필터를 사용해 특징을 추출하므로 특징이 어디에 있어도 동일하게 추출할 수 있다.

이러한 합성곱 계층을 여러 겹 쌓아 모델을 구성하며, 합성곱 계층이 많아질수록 모델의 복잡도가 증가하므로 더 다양한 특징을 추출해 학습할 수 있다.

### 필터

합성곱 계층은 입력 데이터에 **필터(Filter)**를 이용해 합성곱 연산을 수행하는 계층이다. 필터는 **커널(Kernel)** 또는 **윈도(Window)**로 불리기도 한다.

- 필터를 일정 간격으로 이동하면서 입력 데이터와 합성곱 연산을 수행해 특징 맵을 생성한다. 이때, 필터의 가중치는 모델 학습 과정에서 갱신된다. 
- 각 계층에서 하나의 필터가 여러 번 사용되고 이를 공용 가중치로 공유함으로써 이미지 내에서 어느 위치에서도 동일한 패턴을 학습할 수 있게 된다.

<img src="https://images.velog.io/images/rnjsdb72/post/0bc3c991-ebfb-49a7-8941-cfe4d9bda920/%ED%95%A9%EC%84%B1%EA%B3%B1%EC%97%B0%EC%82%B0.png">

그 결과롤 생성된 특징 맵은 다음 합성곱 계층의 입력으로 사용된다. 

### 패딩

필터를 통한 합성곱 연산 수행하면 출력값인 특징 맵의 크기가 작아진다. 

**작은 특징 맵의 문제점**<br>
- 합성곱 신경망을 더 깊게 쌓는 데 제약사항이 될 수 있으며, 합성곱 신경망 성능에 악영향을 끼칠 수 있다.
- 가장자리에 있는 정보는 다른 위치에 있는 정보에 비해 학습하기가 더 어렵다. 
- 가장자리와 필터의 합성곱 연산 횟수가 중심과 필터의 합성곱 연산 횟수보다 적어 정보가 학습되는 데 제한이 있을 수 있다.

이러한 현상을 방지하기 위해 입력 이미지나 입력으로 사용되는 특징 맵 가장자리에 특정 값을 덧붙이는 **패딩(Padding)**을 추가한다. 가장자리에 덧붙이는 패딩 값은 0으로 할당하는데, 이를 제로 패딩(Zero padding)이라고 한다.

<img src="https://jackarp.github.io/assets/images/nlp/Padding.png">

### 간격

간격(Stride)이란 필터가 한 번에 움직이는 크기를 의미한다. 간격의 크기를 조절함으로써 출력 데이터의 크기를 조절할 수 있다. 이는 입력 데이터의 공간적인 정보를 유지하거나 감소시킬 수 있다. 입력 데이터의 공간적인 정보는 픽셀 간의 상대적인 위치나 거리에 대한 정보를 의미한다.

- 간격을 작게 설정하면 입력 데이터의 공간적인 정보를 보존
- 간격을 크게 설정하면 입력 데이터의 공간적인 정보를 감소

간격을 조절해 합성곱 신경망이 학습해야 하는 모델 매개변수의 수를 감소시킬 수 있으며, 이를 통해 모델의 복잡도를 낮추고 과대적합을 방지할 수 있다.

### 채널

입력 데이터와 필터 간의 연산은 **채널(Channel)**에서 수행된다. 채널은 입력 데이터와 필터가 3차원으로 구성되어 있을 때 같은 위치의 값끼리 연산되게 한다. 이를 통해 입력 데이터의 공간 정보를 유지하면서 추출되는 특징을 확장할 수 있다.

채널 개수는 일반적으로 합성곱 계층에서 설정되며, 이는 모델의 구조나 목적에 따라 달라진다. 채널의 개수가 많아질수록 학습할 수 있는 특징의 다양성이 증가해 모델의 **표현력(Representational Power)**이 높아지는 효과를 가져온다.

<img src="https://wikidocs.net/images/page/64066/conv15.png">


출력 채널이 많은 경우, 각 체널은 입력 데이터에서 서로 다른 특징을 학습할 수 있다. 따라서 모델은 더 많은 종류의 특징을 학습하게 되며, 그로 인해 더 복잡한 문제를 해결할 수 있는 능력을 갖추게 된다.

그러나 출력 채널이 많을수록 모델의 매개변수가 많아지므로 학습 시간과 메모리 사용량이 증가하는 단점이 있다.


### 팽창

**팽창(Dilation)**이란 합성곱 연산을 수행할 때 입력 데이터에 더 넓은 범위의 영역을 고려할 수 있게 하는 기법이다. 팽창은 필터와 입력 데이터 사이에 간격을 두는 방법이다.

<img src="https://www.researchgate.net/publication/336002670/figure/fig1/AS:806667134455815@1569335840531/An-illustration-of-the-receptive-field-for-one-dilated-convolution-with-different.png">

팽창은 입력 데이터의 각 픽셀이 출력에 미치는 영향이 강화되는 효과를 갖는다. 팽창은 필터의 크기를 키우지 않고 입력 데이터에 더 넓은 영역을 고려할 수 있게 해 더 깊고 복잡한 모델을 구성할 수 있게 한다. 

하지만 필터가 바라봐야 하는 입력 데이터의 범위가 커지므로 오히려 연산량이 늘어날 수도 있다. 또한 팽창 크기가 너무 크다면 인접한 픽셀값을 고려하지 않게 되므로 공간적인 정보가 보존되지 않아 특징 추출의 효과가 떨어질 수 있다.

### 합성곱 계층 클래스

```python
# 2차원 합성곱 계층 클래스
conv = torch.nn.Conv2d(
  in_channels,
  out_channels,
  kernel_size,
  stride=1,
  padding=0,
  dilation=1,
  groups=1,
  bias=True,
  padding_mode="zeros"
)
```

합성곱 계층의 출력 크기 계산 방법은 아래와 같다.

$$
L_{out} = \Bigg[\frac{L_{in} + 2 \times {padding} - {dilation} \times (kernel\_size - 1) - 1}{stride} + 1 \Bigg]
$$


## 활성화 맵
---------

**활성화 맵(Activation Map)**은 합성곱 계층의 특징 맵에 활성화 함수를 적용해 얻어진 출력값을 의미한다. 
- 합성곱 계층에서 입력 이미지와 필터의 합성곱 연산을 통해 특징 맵을 추출하면 이 값에 비선형성을 추가하기 위해 활성화 함수를 적용한다.
  - ReLU 함수가 적용되며, 이를 통해 특징 맵의 값이 0보다 크면 그 값을 그대로 출력하고, 0 이하일 경우에는 0을 출력한다.

이 과정을 거친 활성화 맵은 다음 계층의 입력값으로 사용된다. 다음 계층이 합성곱 계층이라면 동일한 방식을 거친다. 합성곱 연산과 활성화 함수를 여러 번 반복하여 신경망을 구성하면, 입력 이미지에서 추출된 추상적인 특징을 학습할 수 있게 된다.

*만약 합성곱 계층의 출력값에 활성화 함수를 적용하지 않으면?*<br>
모델이 선형적인 결합만 수행하게 되어 복잡한 패턴이나 추상적인 특징을 학습하는 것이 어려워진다.

따라서 활성화 함수를 적용함으로써 모델이 비선형성을 가지게 되어 입력 데이터에서 다양한 추상적인 특징을 학습할 수 있게 된다.

## 풀링
---------

**풀링(Pooling)**은 특징 맵의 크기를 줄이는 연산으로 합성곱 계층 다음에 적용된다. 
- 특징 맵의 크기를 줄여 연산량을 감소시키고 입력 데이터의 정보를 압축하는 효과를 가진다.
- 필터와 간격을 이용하며 일정한 크기의 필터 내 특정 값을 선택한다.
  - 최댓값 풀링(Max Pooling): 특정 크기의 필터 내 원소값 중 가장 큰 값을 선택해 특징 맵의 크기를 감소시키는 방법
  - 평균값 풀링(Average Pooling): 필터 내 원소값들의 평균값으로 특징 맵의 크기를 감소시키는 방법

<img src="https://www.researchgate.net/publication/333593451/figure/fig2/AS:765890261966848@1559613876098/llustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max.png">

**장점**<br>
- 입력 데이터의 공간적 크기를 줄이기 때문에 계산 비용을 감소시킬 수 있다.
- 입력 데이터의 특징 위치가 변경되더라도 인근 영역에 대한 연산을 적용하기 때문에 공간적 정보를 유지할 수 있다.

**단점**<br>
- 입력 데이터의 위치 정보를 일부 손실
- 세밀한 위치 정보가 필요한 작업에서는 성능 저하를 초래할 수 있다. 

최근에는 풀링을 이용해 공간적 크기를 감소하는 방법보다 연산량이 많더라도 합성곱 계층의 간격을 설정해 입력 데이터의 공간적인 크기를 줄이는 방법을 사용한다.

### 풀링 클래스

```python
# 2차원 최댓값 풀링 클래스
max_pool = torch.nn.MaxPool2d(
  kernel_size,
  stride=None,
  padding=0,
  dilation=1
)

# 2차원 평균값 풀링 클래스
avg_pool = torch.nn.AvgPool2d(
  kernel_size,
  stride=None,
  padding=0,
  count_include_pad=True
)
```

평균값 풀링은 팽창을 지원하지 않는다. 팽창의 경우 원소 간의 거리가 멀어지므로 주변 영역의 특징 계산이 어려워진다.

- `count_include_pad`: 패딩 영역의 값을 평균 계산에 포함할지 여부를 설정

평균값 풀링 클래스의 출력 크기 계산 방법은 아래와 같다.

$$
L_{out} = \Bigg[\frac{L_{in} + 2 \times {padding} - kernel\_size}{stride} + 1 \Bigg]
$$

## 완전 연결 계층
---------

**완전 연결 계층(Fully Connected Layer, FC)**은 각 입력 노드가 모든 출력 노드와 연결된 상태를 의미한다. 이를 통해 완전 연결 계층은 입력과 출력 간의 모든 가능한 관계를 학습할 수 있다. 이 계층은 출력 노드의 수를 조절할 수 있으므로 모델의 복잡성과 용량을 조절하는 데 사용된다.

합성곱 신경망에서는 합성곱 계층과 풀링 계층을 거친 결과물인 특징 맵을 입력으로 받는다. 이때 특징 맵은 3차원이다. 이 특징 맵을 평탄화하여 1차원 벡터로 변경하고 완전 연결 계층의 가중치와 내적 계산을 수행해 출력값을 계산한다.

이 출력값은 특성 맵의 공간 정보가 무시되고 모든 입력을 독립적으로 처리해 계산한다.

완전 연결 계층은 이전 계층에서 추출된 특징을 활용하여 최종적인 분류 작업을 수행하며 최종값은 소프트맥스나 시그모이드와 같은 활성화 함수를 적용해 분류 모델로 구성할 수 있다.

<img src="https://github.com/user-attachments/assets/30fbfba7-4eba-4180-8d01-aa9e2456a3b7">

## 합성곱 신경망을 이용한 자연어 처리
---------

이미지에서 2차원 합성곱 신경망 학습을 진행할 때, 필터를 수직/수평 방향으로 이동한다. 그러나 텍스트 데이터의 임베딩 값은 입력 순서를 제외하면 입력값의 위치가 의미를 가지지 않는다.

<img src="https://github.com/user-attachments/assets/9642d8fa-745b-4b06-b595-100b7f010635">

즉, 텍스트 데이터는 **1차원 합성곱(1-Dimensional Convolution)**을 적용해야 한다.

텍스트 데이터는 문장을 단어 단위로 분리하여 각 단어를 임베딩하여 나온 1차원 벡터 데이터를 입력값으로 사용한다. 1차원 합성곱은 이 벡터 입력값을 수직 방향으로만 이동하는 필터를 적용하여 해당 문장의 특징을 추출한다.

1차원 필터의 크기는 필터의 높이에만 영향을 미치며, 필터의 너비는 입력 임베딩의 크기가 된다.

<img src="https://github.com/user-attachments/assets/f73b7e1c-fdb2-4a93-8168-979b7f143c59">

- 필터 크기가 3인 합성곱 임베딩을 수행하면 인접한 3개의 토큰에 대한 연산 수행

1차원 합성곱을 이용한 신경망에서는 다양한 크기(높이 변경)의 합성곱 필터를 사용하여 여러 종류의 정보를 추출할 수 있으다. 또한 1차원 합성곱 연산을 하면 1차원 벡터를 얻으며 이 출력값으로 풀링을 적용하면 하나의 스칼라값이 도출된다.

<img src="https://github.com/user-attachments/assets/596722d4-4129-4fff-b8a4-d64ee5e8725f">

크기가 다른 여러 개의 합성곱 필터를 사용하면 여러 개의 스칼라값을 얻을 수 있고 이를 모으면 하나의 특징 벡터로 연결할 수 있다. 이 특징 벡터에 완전 연결 계층을 추가하여 분류 모델을 구성한다.

합성곱 신경망을 이용한 문장 분류는 사전 학습된 임베딩 벡터를 사용하는 것이 일반적이다.

