---
title: "토큰화"
excerpt: "단어 및 글자 토큰화 / 형태소 토큰화 / 하위 단어 토큰화"

categories:
  - Natural language processing
tags:
  - [NLP]

permalink: /nlp/tokenize/

toc: true
toc_sticky: true

date: 2025-03-29
last_modified_at: 2025-03-29
---

**자연어 처리(NLP)**는 컴퓨터가 인간의 언어를 이해하고 해석 및 생성하기 위한 기술을 의미한다.

자연어 처리는 인공지능의 하위 분야 중 하나로 컴퓨터가 인간과 유사한 방식으로 인간의 언어를 이해하고 처리하는 것이 주요 목표 중 하나다. 인간 언어의 구조, 의미, 맥락을 분석하고 이해할 수 있는 알고리즘과 모델을 개발한다. 이런 모델을 개발하기 위해서는 해결해야할 문제가 있다.

- 모호성(Ambiguity): 인간의 언어는 맥락에 따라 여러 의미를 갖게 되어 모호한 경우가 많다. 알고리즘이 이런 다양한 의미를 이해하고 명확하게 구분할 수 있어야 한다.
- 가변성(Variability): 사투리, 강세, 신조어, 작문 스타일로 인해 매우 가변적이므로 이를 처리할 수 있어야 하며 사용 중인 언어를 이해할 수 있어야한다.
- 구조(Structure): 구문을 파악하여 의미를 해석해야하므로 알고리즘도 문장의 구조와 문법적 요소를 이해하여 의미를 추론하거나 분석할 수 있어야 한다.

이를 해결하는 모델을 만들려면 **말뭉치(Corpus)**를 일정한 단위인 **토큰(Token)**으로 나눠야 한다.
- 말뭉치: 자연어 모델을 훈련하고 평가하는 데 사용되는 대규모의 자연어
- 토큰: 개별 단어나 문장 부호와 같은 텍스트를 의미하며 말뭉치보다 더 작은 단위

**토큰화(Tokenization)**는 컴퓨터가 자연어를 이해할 수 있게 토큰으로 나누는 과정이다.
- 자연어 처리 과정에서 중요한 단계
- 토큰화를 위해 **토크나이저(Tokenizer)**를 사용
    - 토크나이저는 텍스트 문자열을 토큰으로 나누는 알고리즘 또는 소프트웨어를 의미

토큰을 나누는 기준은 구축하려는 시스템이나 주어진 상황에 따라 다르며 어떻게 나누었느냐에 따라 시스템의 성능이나 처리 결과가 크게 달라지기도 한다.
- 공백 분할: 텍스트를 공백 단위로 분리해 개별 단어로 토큰화
- 정규 표현식 적용: 정규 표현식으로 특정 패턴을 식별해 텍스트를 분할
- 어휘 사전(Vocabulary)적용: 사전에 정의된 단어 집합을 토큰으로 사용
    - 미리 정의된 단어를 활용하므로 없는 단어나 토큰이 존재할 수 있다. 이를 **OOV(Out of Vocab)**라고 한다.
    - OOV 문제를 해결하기 위해 더 큰 어휘 사전을 구축한다면 학습 비용이 증대하고 **차원의 저주**에 빠질 수 있는 단점이 있다.
- 머신러닝 활용: 데이터세트를 기반으로 토큰화하는 방법을 학습한 머신러닝을 적용


## 🦥 단어 및 글자 토큰화

토큰화는 자연어 처리에서 매우 중요한 전처리 과정으로, 텍스트 데이터를 구조적으로 분해하여 개별 토큰으로 나누는 작업을 의미한다. 이를 통해 단어나 문장의 빈도수, 출현 패턴 등을 파악할 수 있다.

또한 작은 단위로 분해된 텍스트 데이터는 컴퓨터가 이해하고 처리하기 용이해 기계 번역, 문서 분류, 감성 분석 등 다양한 자연어 처리 작업에 활용할 수 있다.

입력된 텍스트 데이터를 단어(Word)나 글자(Character) 단위로 나누는 기법으로는 **단어 토큰화**와 **글자 토큰화**가 있다.

### 단어 토큰화

**단어 토큰화(Word Tokenization)**는 자연어 처리 분야에서 핵심적인 전처리 작업 중 하나로 텍스트 데이터를 의미있는 단위인 단어로 분리하는 작업이다.
- 띄어쓰기, 문장 부호, 대소문자 등의 특정 구분자를 활용해 토큰화를 수행
- 주로 품사 태깅, 개체명 인식, 기계번역 등의 작업에서 사용되며 가장 일반적인 토큰화 방법

```python
# 단어 토큰화
review = "현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!"
tokenized = review.split()
print(tokenized)
# ['현실과', '구분', '불가능한', 'cg.', '시각적', '즐거움은', '최고!', '더불어', 'ost는', '더더욱', '최고!!']
```

문자열 데이터 형태는 `split()` 메서드를 이용하여 토큰화한다. 
- 구분자를 통해 문자열을 리스트 데이터로 나눔
- 구분자를 입력하지 않으면 **공백(Whitespace)**가 기준

단어 토큰화는 한국어 접사, 문장 부호, 오타 혹은 띄어쓰기 오류 등에 취약하다.
ex) 'cg.', 'cg'를 다른 토큰으로 인식

### 글자 토큰화

**글자 토큰화(Character Tokenization)**는 띄어쓰기뿐만 아니라 글자 단위로 문장을 나누는 방식이다.
- 비교적 작은 단어 사전을 구축
- 작은 단어 사전을 사용하면 학습 시 컴퓨터 자원을 절약
- 전체 말뭉치를 학습할 때 각 단어를 더 자주 학습이 가능
- 언어 모델링과 같은 시퀀스 예측 작업에서 활용
    - 다음에 올 문자를 예측하는 언어 모델링

```python
# 글자 토큰화
review = "현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!"
tokenized = list(reivew)
print(tokenized)
# ['현', '실', '과', ' ', '구', '분', ' ', '불', '가', '능', '한', ' ', 'c', 'g', '.', ' ', '시', '각', '적', ' ', '즐', '거', '움', '은', ' ', '최', '고', '!', ' ', '더', '불', '어', ' ', 'o', 's', 't', '는', ' ', '더', '더', '욱', ' ', '최', '고', '!', '!']
```

글자 토큰화는 `list()`를 이용해 쉽게 수행할 수 있다. 
- 단어 토큰화와 다르게 공백도 토큰으로 나눔

영어의 경우는 각 알파벳으로 토큰화를 하지만 한글은 하나의 글자가 여러 자음과 모음의 조합으로 이루어져 있어 자소 단위로 나눠서 자소 단위 토큰화를 수행한다.
- **자모(jamo)** 라이브러리 활용
    - 한글 문자 및 자모 작업을 위한 한글 음절 분해 및 합성 라이브러리
    - 텍스트를 자소 단위로 분해해 토큰화를 수행

**컴퓨터가 한글을 인코딩하는 방식**<br>
- 완성형: 조합된 글자 자체에 값을 부여해 인코딩하는 방식
    ```python
    # 자모 변환 함수 - 입력된 한글을 조합형 한글로 변환
    retval = jamo.h2j(
        hangul_string
    )
    ```

- 조합형: 글자를 자모 단위로 나눠 인코딩한 뒤 이를 조합해 한글을 표현
    - 초성, 중성, 종성으로 분리

    ```python
    # 한글 호환성 자모 변환 함수 - 조합성 한글 문자열을 자소 단위로 나눠 반환
    retval = jamo.j2hcj(
        jamo
    )
    ```

자소 단위로 분해하여 토큰화를 수행하면 다음과 같이 분리된다.
    
```python
# 자소 단위 토큰화
from jamo import h2j, j2hcj

review = "현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!"
decomposed = j2hcj(h2j(review))
tokenized = list(decomposed)
print(tokenized)
# ['ㅎ', 'ㅕ', 'ㄴ', 'ㅅ', 'ㅣ', 'ㄹ', 'ㄱ', 'ㅘ', ' ', 'ㄱ', 'ㅜ', 'ㅂ', 'ㅜ', 'ㄴ', ' ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㄱ', 'ㅏ', 'ㄴ', 'ㅡ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ', ' ', 'c', 'g', '.', ' ', 'ㅅ', 'ㅣ', 'ㄱ', 'ㅏ', 'ㄱ', 'ㅈ', 'ㅓ', 'ㄱ', ' ', 'ㅈ', 'ㅡ', 'ㄹ', 'ㄱ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㅁ', 'ㅇ', 'ㅡ', 'ㄴ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', ' ', 'ㄷ', 'ㅓ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㅇ', 'ㅓ', ' ', 'o', 's', 't', 'ㄴ', 'ㅡ', 'ㄴ', ' ', 'ㄷ', 'ㅓ', 'ㄷ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㄱ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', '!']
```

**장점**<br>
- 단어 단위로 토큰화하는 것에 비해 비교적 적은 크기의 단어 사전 구축이 가능
- 단어 토큰화의 단점을 보완
    - 접사와 문장 부호의 의미 학습이 가능
- 작은 크기의 단어 사전으로도 OOV를 줄일 수 있음

**단점**<br>
- 개별 토큰은 아무런 의미가 없으므로 자연어 모델이 각 토큰의 의미를 조합해 결과를 도출해야 한다.
- 토큰 조합 방식을 사용해 문장 생성이나 **개체명 인식**등을 구현할 경우, 다의어나 동음이의어가 많은 도메인에서 구별하는 것이 어려울 수 있다.
- 모델 입력 **시퀀스(sequence)**의 길이가 길어질수록 연산량이 증가


## 🦥 형태소 토큰화

**형태소 토큰화(Morpheme Tokenization)**란 텍스트를 형태소 단위로 나누는 토큰화 방법으로 언어의 문법과 구조를 고려해 단어를 분리하고 이를 의미있는 단위로 분류하는 작업이다.

- 한국어와 같이 교착어인 언어에서 중요하게 수행된다.
    - 각 단어가 띄어쓰기로 구분되지 않고 어근에 다양한 접사와 조사가 조합되어 하나의 낱말을 이루기 떄문이다.

햔국어에는 두 가지의 형태소가 있다.
- 자립 형태소: 단어를 이루는 기본 단위로 스스로 의미를 가지고 있음
    - 명사, 동사, 형용사
- 의존 형태소: 자립 형태소와 함께 조합되며 의미를 가지고 있지 않음
    - 조사, 어미, 접두사, 접미사 등


### 형태소 어휘 사전

**형태소 어휘 사전(Morpheme Vocabulary)**은 자연어 처리에서 사용되는 단어의 집합인 어휘 사전 중에서도 각 단어의 형태소 정보를 포함하는 사전이다.

일반적으로 형태소 어휘 사전에는 각 형태소가 어떤 품사에 속하는지와 해당 품사의 뜻 등의 정보도 함께 제공된다.

**품사 태깅(POS Tagging)**은 텍스트 데이터를 형태소 분석하여 각 형태소에 해당하는 **품사(Part Of Speech, POS)**를 태깅하는 작업을 말한다.

ex. 그(명사) + 는(조사) + 나(명사) + 에게(조사) + 인사(명사) + 를(조사) + 했다(동사)

### KoNLPy

**KoNLPy**는 한국어 자연어 처리를 위해 개발된 라이브러리로 명사 추출, 형태소 분석, 품사 태깅 등의 기능을 제공한다.

KoNLPy 형태소 분석기<br>
- Okt(Open Korean Text)
- 꼬꼬마(Kkma)
- 코모란(Komoran)
- 한나눔(Hannanum)
- 메캅(Mecab)

```python
# Okt 토큰화
from konlpy.tag import Okt

okt = Okt()

sentence = "무엇을 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다."

nouns = okt.nouns(sentence)     # 명사
pharses = okt.phases(sentence)  # 구
morphs = okt.morphs(setence)    # 형태소
pos = okt.pos(setence)  # 품사 태깅
```

Okt에서 지원하는 대표적인 메서드는 명사 추출(`okt.nouns`), 구문 추출(`okt.phrases`), 형태소 추출(`okt.morphs`), 품사 태깅(`okt.pos`)이다.

```python
# 꼬꼬마 토큰화
from konlpy.tag import Kkma

kkma = Kkma()

sentence = "무엇을 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다."

nouns = kkma.nouns(sentence)
setences = kkma.setences(sentence)
morphs = kkma.morphs(sentence)
pos = kkma.pos(sentence)
```

### NLTK

**NLTK(Natural Language Toolkit)**는 자연어 처리를 위해 개발된 라이브러리이다.

- 토큰화, 행태소 분석, 구문 분석, 개체명 인식, 감성 분석 등과 같은 기능을 제공한다.
- 토큰화나 품사 태깅 작업을 위해서는 해당 작업을 수행할 수 있는 패키지나 모델을 다운로드해야 한다.
    - 대표적으로 Punkt 모델과 Averaged Perceptron Tagger 모델이 있으며 두 모델 모두 **트리뱅크(Treebank)**라는 대규모 영어 말뭉치를 기반으로 학습됐다.

```python
import nltk

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# 영문 토큰화
from nltk import tokenize

sentence = "Those who can imagine anything, can create the impossible."

word_tokens = tokenize.word_tokenize(setence)
sent_tokens = tokenize.sent_tokenize(setence)
```

- `word_tokenize()`는 문장을 입력받아 공백을 기준으로 단어를 분리하고, 구두점 등을 처리해 각각의 단어(token)를 추출해 리스트로 반환한다.
- `sent_tokenize()`는 문장을 입력받아 마침표(.), 느낌표(!), 물음표(?) 등의 구두점을 기준으로 문장을 분리해 리스트로 반환한다.

```python
# 영문 품사 태깅

from nltk import tag
from nltk import tokenize

sentence = "Those who can imagine anything, can create the impossible."

word_tokens = tokenize.word_tokenize(setence)
pos = tag.pos_tag(word_tokens)
```

### spaCy

**spaCy**는 사이썬 기반으로 개발된 오픈 소스 라이브러리로서, 자연어 처리를 위한 기능을 제공한다. 
- NLTK 라이브러리와의 주요한 차이점은 빠른 속도와 높은 정확도를 목표로 하는 머신러닝 기반의 자연어 처리 라이브러리이다.
    - NLTK는 자연어 처리를 위한 다양한 알고리즘 예제를 제공
    - spaCy는 효율적인 처리 속도와 높은 정화고를 제공
- NLTK에서 사용하는 모델보다 더 크고 복잡하다.

spaCy는 GPU 가속을 제공하며 24개 이상의 언어로 사전 학습된 모델을 제공한다 대표적으로 `en_core_web_sm` 모델은 영어로 사전 학습된 모델 중 하나이다.

```python
# spaCy 품사 태깅
import spacy

nlp = spacy.load("en_core_web_sm")
sentence = "Those who can imagine anything, can create the impossible."
doc = nlp(setence)

for token in doc:
    print(f"[{token.pos_:5} - {toekn.tag_:3}]: {token.text}")
# [PRON - DT] : Those
# [PRON - WP] : who
# [AUX  - MD] : can
# [VERB - VB] : imagine
#      .
#      .
#      .
# [ADJ  - JJ] : impossible]
# [PUNCT - .] : .
```

token 객체에는 여러 속성이 포함돼 있다.
- `pos_`: 기본 품사 속성
- `tag_`': 세분화 품사 속성
- `text`: 원본 텍스트 데이터
- `text_with_ws`: 토큰 사이의 공백을 포함하는 텍스트 데이터
- `vector`: 벡터
- `vector_norm`: 벡터 노름

## 🦥 하위 단어 토큰화

### 바이트 페어 인코딩

### 워드피스
