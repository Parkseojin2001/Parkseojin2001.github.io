---
title: "순환 신경망"
excerpt: "RNN / LSTM"

categories:
  - NLP
tags:
  - [NLP]

permalink: /nlp/rnn/

toc: true
toc_sticky: true

date: 2025-04-17
last_modified_at: 2025-04-17
---
**순환 신경망(Recurrent Neural Network, RNN)** 모델은 순서가 있는 **연속적인 데이터(Sequence data)**를 처리하는 데 적합한 구조를 갖고 있다. 순환 신경망은 각 **시점(Time step)**의 데이터가 이전 시점의 데이터와 독립적이지 않다는 특성 때문에 효과적으로 작동한다. 

- 연속성 데이터: 특정 시점 $t$에서의 데이터가 이전 시점($t_0, t_1, ..., t_{n-1}$)의 영향을 받는 데이터

자연어 데이터는 연속적인 데이터의 일종으로 볼 수 있다. 자연어는 한 단어가 이전 단어들과 상호작용하여 문맥을 이루고 의미를 형성한다. 

또한 긴 문장일수록 앞선 단어들과 뒤따르는 단어들 사이에 강한 **상관관계(Correlation)**가 존재한다.

## 🦥 순환 신경망

순환 신경망은 연속적인 데이터를 처리하기 위해 개발된 인공 신경망의 한 종류다. 이전에 처리한 데이터를 다음 단계에 활용하고 현재 입력 데이터와 함께 모델 내부에서 과거의 상태를 기억해 현재 상태를 예측하는 데 사용한다.

- 시계열 데이터
- 자연어 처리
- 음성 인식
- 시퀀스 데이터

순환 신경망은 연속형 데이터를 순서대로 입력받아 처리하며 각 시점마다 **은닉 상태(Hidden state)**의 형태로 저장한다. 각 시점의 데이터를 입력으로 받아 은닉 상태와 출력값을 계산하는 노드를 순환 신경망의 **셀(Cell)**이라 한다.

순환 신경망의 셀은 이전 시점의 은닉 상태 $h_{t-1}$을 입력으로 받아 현재 시점의 은닉 상태 $h_t$를 계산한다.

<img src="https://images.velog.io/images/yuns_u/post/ccbb28ea-fa08-4d23-804e-419e6f578e4b/image.png">

순환 신경망은 각 시점 $t$에서 현재 입력값 $x_t$와 이전 시점 $t-1$의 은닉 상태 $h_{t-1}$를 이용해 현재 시점의 은닉 상태 $h_t$와 출력값 $y_t$를 계산한다.

은닉 상태의 수식은 아래와 같다.

$$
h_t = \sigma_h(h_{t-1}, x_t) \\
h_t = \sigma_h(W_{hh}h_{t-1} + W_{xh}x_t + b)
$$

- $\sigma_h$: 순환 신경망의 은닉 상태를 계산하기 위한 활성화 함수
- $h_{t-1}$: 이전 시점 t-1의 은닉 상태
- $x_t$: 현재 시점 t의 입력값
- $h_t$: 현재 시점 t의 은닉 상태

$\sigma_h$는 가중치(W)와 편향(b)을 이용해 계산한다. 
- $W_{hh}$: 이전 시점의 은닉 상태 $h_{t-1}$에 대한 가중치
- $W_{xh}$: 입력값 $x_t$에 대한 가중치
- $b_h$: 은닉 상태 $h_t$의 편향

출력값 계산은 아래와 같다.

$$
y_t = \sigma_y(h_t)\\
y_t = \sigma_y(W_{hy}h_t + b_y)
$$

- $\sigma_y$: 순환 신경망의 출력값을 계산하기 위한 활성화 함수
- $W_{hy}$: 현재 시점의 은닉 상태 $h_t$에 대한 가중치
- $b_y$: 출력값 $y_t$의 편향

순환 신경망의 출력값은 이전 시점의 정보를 현재 시점에서 활용해 입력 시퀀스의 패턴을 파악하고 출력값을 예측하므로 연속형 데이터를 처리할 수 있다.

### 일대다 구조

**일대다 구조(One-to-Many)**는 하나의 입력 시퀀스에 대해 여러 개의 출력값을 생성하는 순환 신경망 구조다.

ex. 하나의 문장을 입력으로 받고, 문장에서 각 단어의 품사를 예측하는 작업

이러한 일대다 구조를 구현하기 위해서는 출력 시퀀스의 길이를 미리 알고 있어야 한다. 이를 위해 입력 시퀀스를 처리하면서 시퀀스의 정보를 활용해 출력 시퀀스의 길이를 예측하는 모델을 함께 구현해야 한다.

<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQr-e9ElLkpsqC_cjyvcuuSY-BDSbniymnfnezOgGUEbt-2ZHo8Mkv2MgsqMxFfPvpJWw&usqp=CAU" width="170px">

### 다대일 구조

**다대일 구조(Many-to-One)**는 여러 개의 입력 시퀀스에 대해 하나의 출력값을 생성하는 순환 신경망 구조다.

ex. 감성 분류 분야 - 입력 시퀀스는 문장으로 이루어져 있으며, 출력값은 해당 문장의 감정(긍정, 부정)을 예측하는 작업

입력 시퀀스가 어떤 범주에 속하는지를 구분하는 문장 분류, 두 문장 간의 관계를 추론하는 **자연어 추론(Natural Language Inference)** 등에도 적용할 수 있다.

<img src="https://github.com/user-attachments/assets/3272aa0f-560d-456d-8280-89cfd2111b9a" width="200px">

### 다대다 구조

**다대다 구조(Many-to-Many)**는 입력 시퀀스와 출력 시퀀스의 길이가 여러 개인 경우에 사용되는 신경망 구조다. 

ex. 입력 문장에 대해 번역된 출력 문장을 생성하는 번역기, 음성 인식 시스템에서 음성 신호를 입력으로 받아 문장을 출력하는 음성 인식기

다대다 구조에서는 입력 시퀀스와 출력 시퀀스의 길이가 서로 다른 경우가 있을 수 있다. 이런 경우 입력 시퀀스와 출력 시퀀스의 길이를 맞추기 위해 패딩을 추가하거나 잘라내는 등의 전처리 과정이 수행된다.

다대다 구조는 **시퀀스-시퀀스(Seq2Seq)** 구조로 이뤄져 있다.
- **인코더(Encoder)**: 입력 시퀀스 처리하며 고정 크기의 벡터를 출력
- **디코더(Decoder)**: 출력 시퀀스를 생성

<img src="https://github.com/user-attachments/assets/18ce0a8b-1350-4d13-86fe-82fa1daa06d3">

### 양방향 순환 신경망

**양방향 순환 신경망(Bidirectional Recurrent Neural Network, BiRNN)**은 기본적인 순환 신경망에서 시간 방향을 양방향으로 처리할 수 있도록 고안된 방식이다. 이전 시점(t-1)의 은닉 상태뿐만 아니라 이후 시점(t+1)의 은닉 상태도 함께 이용한다.

ex. "인생은 B와 _ 사이의 C다."라는 문장에서 _에 입력될 단어를 예측

- t 시점 이후의 데이터와 t 시점 이전의 데이터 모두 t 시점의 데이터를 예측하는 데 사용된다.
- 입력 데이터를 순방향으로 처리하는 것만 아니라 역방향으로 거꾸로 읽어 들여 처리한다.

<img src="https://miro.medium.com/max/1313/1*6QnPUSv_t9BY9Fv8_aLb-Q.png">

### 다중 순환 신경망

**다중 순환 신경망(Stacked Recurrent Neural Network)**은 여러 개의 순환 신경망을 연결하여 구성한 모델로 각 순환 신경망이 서로 다른 정보를 처리하도록 설계돼 있다.

다중 순환 신경망은 여러 개의 순환 신경망 층으로 구성되며 각 층에서의 출력값은 다음 층으로 전달되어 처리하도록 설계돼 있다. 각 층의 가중치는 각각 동일하다.

<img src="https://lh6.googleusercontent.com/rC1DSgjlmobtRxMPFi14hkMdDqSkEkuOX7EW_QrLFSymjasIM95Za2Wf-VwSC1Tq1sjJlOPLJ92q7PTKJh2hjBoXQawM6MQC27east67GFDklTalljlt0cFLZnPMdhp8erzO">

**장점**<br>
- 데이터의 다양한 특징을 추출할 수 있어 성능이 향상될 수 있다.
- 층이 깊어질수록 더 복잡한 패턴을 학습할 수 있다.

**단점**<br>
- 층이 많아질수록 학습 시간이 오래 걸린다.
- 기울기 소실 문제가 발생할 가능성이 높아진다.
- 시간적으로 먼 과거의 정보를 잘 기억하지 못한다.

## 🦥 장단기 메모리
