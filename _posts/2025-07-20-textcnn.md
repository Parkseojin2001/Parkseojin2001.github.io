---
title: "[논문 리뷰] Convolutional Neural Networks for Sentence Classification" 
description: "사전 학습된 워드 벡터와 합성곱 신경망을 활용한 문장 분류 모델에 대한 논문 리뷰 및 논문 구현"

categories: [논문 리뷰, 논문 구현, NLP]
tags: [NLP]

permalink: /paper-review/nlp/textcnn/

toc: true
toc_sticky: true
math: true
mermaid: true

date: 2025-07-20
last_modified_at: 2025-07-20
---

## Abstract
---------------

본 논문에서는 **pre-trained [word vector](https://parkseojin2001.github.io/pytorch-book/nlp/embedding-2/)**를 기반으로 **[CNN](https://parkseojin2001.github.io/pytorch-book/nlp/cnn/)**을 사용한 sentence classification 모델을 제안하고 있다. 하이퍼파라미터 튜닝을 거의 하지 않고 정적 벡터를 사용하여 간단한 CNN이 여러 벤치마크에서 우수한 결과를 달성함을 보여준다. 또한, 간단한 아키텍처 수정을 통해 task-specific과 static vectors의 사용이 가능하도록 제안한다. 이 모델은 감정 분석 및 질문 분류를 포함한 7개 작업 중 4개 작업에서 기존 모델보다 향상된 성능을 보였다.

## Introduction
----------------

합성곱 신경망(CNN)은 지역적 특징에 적용되는 합성곱 필터를 가진 layer를 활용한다. 원래 컴퓨터 비전을 위해 개발되었지만 자연어 처리에 효과적임을 보여주었다.

본 논문에선 비지도 학습 신경망 언어 모델에서 얻은 단어를 기반으로 한 합성곱 계층을 갖는 간단한 CNN 모델을 학습한다. 단어 벡터를 고정한 상태에서 모델의 다른 매개변수만 학습한다. 하이퍼파라미터를 거의 조정하지 않아도 이 간단한 모델은 여러 벤치마크에서 훌륭한 성과를 냈다. 이는 사전 학습된 모델로부터 얻은 feature extractors로 다양한 분류 작업에 좋은 성능을 보여준다.

## Model
-------------

모델 구조는 다음과 같다.

<img src="https://velog.velcdn.com/images%2Flm_minjin%2Fpost%2Fa48d6908-88b7-4d25-a1b1-bd206addbdac%2Fimage.png">

$\mathbf{x}_i \in \mathbb{R}^k$s는 $i$번째 단어에 해당하는 $k$차원의 단어 벡터라 하자. 길이가 $n$(padding 처리 필수)인 문장은 다음과 같이 표현될 수 있다.

$$ 
\mathbf{x}_{1:n} = \mathbf{x}_1 \oplus \mathbf{x}_2 \oplus \ldots \oplus \mathbf{x}_n
$$

여기서 $\oplus$는 concatenation 연산자이다.

일반적으로 $\mathbf{x}_{i:i+j}$는 $i$번째 단어에서 $i+j$번째 단어를 concatenation한 것을 의미한다. convolution operation은 필터 $\mathbf{w} \in \mathbb{R}^{hk}$를 포함하고, 이는 $h$개의 단어에 윈도우를 적용하여 새로운 feature을 만든다.

예를 들어, 하나의 feature $c_i$는 $\mathbf{x}_{i:i+h-1}$ 단어에 적용된 윈도우로 부터 만들어졌다. 이를 수식으로 표현하면 아래와 같다.

$$
c_i = f(\mathbf{w} \cdot \mathbf{x}_{i:i+h-1} + b)
$$

이 때 $b \in \mathbb{R}$은 편향(bias)이고 $f$는 $tanh$와 같은 비선형 함수이다. 이 필터는 문장 $\\{ \mathbf{x}_{1:h}, \mathbf{x} _{2:h+1}, \ldots, \mathbf{x} _{n-h+1:n} \\}$ 안에 있는 모든 단어들에 각각 적용되며, 다음과 같은 **feature map**을 만들어낸다. 

$$
\mathbf{c} = [c_1, c_2, \ldots, c_{n-h+1}]
$$

이 때 $\mathbf{c} \in \mathbb{R}^{n-h+1}$이다.

이 후 max-over-time pooling operation을 feature map에 적용해 가장 큰 값 $\hat{c} = max \\{ \mathbf{c} \\}$을 특정한 필터에 상응되는 특징으로 둔다. 이 Pooling 방식은 자연스럽게 가변적인 문장 길이를 처리한다.

지금까지 설명은 하나의 필터에서 하나의 특징을 추출하는 과정을 설명했다. 이 모델은 여러 필터를 사용해 여러 특징을 얻는다. 이러한 특징들은 두 번째 레이어를 형성하고, 레이블에 대한 확률 분포를 출력하는 fully connected softmax layer로 전달된다.



### Regularization

정규화를 위해 끝에서 두 번째 레이어에 dropout을 사용한다. Dropout은 순방향-역전파 동안 hidden unit의 비율 $p$만큼 무작위로 dropout(0으로 설정)하여 hidden unit의 co-adaptation을 방지한다.

> **Co-adaptation, 동조화**<br>
> : 특정 뉴런의 가중치나 바이어스가 큰 값을 갖게 되면, 그 특정 뉴런의 영향이 커지면서 다른 뉴런들의 학습 속도가 느려지거나 학습이 제대로 진행되지 못하는 경우.

즉, 마지막에서 두 번째 레이어 $\mathbf{z} = [\hat{c}_1, \ldots, \hat{c}_m]$ 주어졌을 때($m$은 필터의 개수),

$$
y = \mathbf{w} \cdot \mathbf{z} + b
$$

를 사용하는 대신에 

$$
y = \mathbf{w} \cdot (\mathbf{z} \circ \mathbf{r}) + b
$$

을 사용한다. 여기서 $\circ$는 원소별 곱셈을 나타내는 연산자이고, $\mathbf{r} \in \mathbb{R}^m$는 확률 $p$를 가지는 베르누이 확률 변수의 마스킹 벡터이다. 마스킹이 안된 유닛만 사용해 그레이디언트 역전파가 된다. 

테스트는 훈련 때 적용했던 드롭아웃을 직접 사용하지 않고, 대신 학습된 가중치 $\mathbf{w}$에 드롭아웃 확률 $p$를 곱하여 $\hat{\mathbf{w}}$로 조정한다($\hat{\mathbf{w}} = p\mathbf{w}$).

추가적으로 가중치 벡터의 $l_2-norm$을 크기를 제한하기 위해, 경사 하강 단계(gradient descent step) 후에 $\lVert \mathbf{w} \rVert _2 > s$일 경우 $\lVert \mathbf{w} \rVert _2 =s$가 되도록 가중치를 rescaling한다.

## Datasets and Experimental Setup
---------------

본 논문에서는 모델을 테스트하기 위해 여러 벤치마크를 사용했다. 이 중 SST-1을 사용하여 모델 구현을 진행할 예정이다.

- SST-1 : 스탠포드 감정 트리뱅크(Treebank) - MR의 확장 버전 느낌. train/dev/test가 나뉘어 있고, 감성이 좀 더 세밀하게 라벨링 되어 있음(아주 좋음, 좋음, 중립, 나쁨, 아주 나쁨)

<img src="https://velog.velcdn.com/images%2Flm_minjin%2Fpost%2F349997cd-99ab-4ae2-ac86-5d250f59bb29%2Fimage.png">

### Hyperparameters and Training

데이터에 적용된 것

- ReLU
- filter windows($h$) of 3, 4, 5 with 100 feature maps
- dropout rate($p$) of 0.5
- $l_2$ constraint ($s$) of 3
- mini-batch size of 50

훈련은 Adadelta 업데이트 규칙을 기반으로 무작위 미니 배치 SGD 방식을 사용했으며, 조기 종료 외에는 별도의 튜닝을 수행하지 않았음

### Pre-trained Word Vectors

대규모 지도 학습 데이터셋이 부족할 때 모델 성능을 향상하는 일반적인 방법으로 비지도 신경망 언어 모델을 통해 얻은 워드 벡터로 초기화

Google News 데이터셋의 1,000억 개 단어로 학습된 공개 Word2Vec 벡터를 사용함. 벡터들은 300차원이며, CBOW 아키텍처를 활용하여 훈련되었다. 사전 학습된 단어 집합에 없는 단어들은 무작위로 초기화됩니다.

### Model Variations

- CNN-rand : 모든 단어를 랜덤하게 초기화, 학습하면서 단어 벡터도 같이 학습.
- CNN-static : word2vec으로 사전 학습된 단어 벡터 사용. 학습 동안 모든 단어는 변동 X, 모델의 파라미터만 학습(사전 학습된 단어 집합에 없어서 랜덤하게 초기화한 단어 포함)
- CNN-non-static: word2vec으로 사전 학습된 단어 벡터 사용. 사전 학습된 단어가 각 태스크에 맞춰 파인 튜닝 됨.
- CNN-multichannel : 워드 벡터 집합이 2개인 모델. 각 벡터 집합은 "channel"로 처리되고, 각 필터는 두 채널 모두에 적용되지만 역전파는 채널 중 하나만 사용 . 그래서 이 모델은 한 채널은 파인 튜닝이 됐지만, 한 채널은 static을 유지. 두 채널 모두 word2vec으로 초기화 됨.

위의 변화와 다른 임의 요소의 효과를 분리하기 위해 CV-fold 할당, 단어 집합에 없는 단어 초기화, CNN 파라미터 초기화와 같은 임의성을 가지는 요소들을 균일하기 유지.

## Results and Discussion
---------------

<img src="https://velog.velcdn.com/images%2Flm_minjin%2Fpost%2F669f679c-1792-4900-826f-20e8376727e8%2Fimage.png">


- baseline model(CNN-rand)는 성능이 그리 좋지 않다.
- 성능을 기대했던 사전 학습된 벡터를 사용한 모델은 성능이 좋다.
- static vectors를 사용한 간단한 모델이 다른 복잡한 딥러닝 모델 또는 parse tree가 필요한 모델보다 현저히 높은 성능을 보임.
    - 결과를 보면, 사전 학습된 벡터들은 good, 'universal' feature extractors이고, 이는 여러 데이터셋에 활용될 수 있다는 것을 알 수 있다.
- 각 태스크에 맞춰 파인튜닝하는 것이 대체로 더 좋은 성능을 보임(CNN-non-static)

## 모델 구현
------------