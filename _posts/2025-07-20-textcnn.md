---
title: "[논문 리뷰] Convolutional Neural Networks for Sentence Classification" 
description: ""

categories: [논문 리뷰, NLP]
tags: [NLP]

permalink: /paper-review/nlp/textcnn/

toc: true
toc_sticky: true
math: true
mermaid: true

date: 2025-07-20
last_modified_at: 2025-07-20
---

## Abstract
---------------

본 논문에서는 **pre-trained [word vector](https://parkseojin2001.github.io/pytorch-book/nlp/embedding-2/)**를 기반으로 **[CNN](https://parkseojin2001.github.io/pytorch-book/nlp/cnn/)**을 사용한 sentence classification 모델을 제안하고 있다. 하이퍼파라미터 튜닝을 거의 하지 않고 정적 벡터를 사용하여 간단한 CNN이 여러 벤치마크에서 우수한 결과를 달성함을 보여준다. 또한, 간단한 아키텍처 수정을 통해 task-specific과 static vectors의 사용이 가능하도록 제안한다. 이 모델은 감정 분석 및 질문 분류를 포함한 7개 작업 중 4개 작업에서 기존 모델보다 향상된 성능을 보였다.

## Introduction
----------------

합성곱 신경망(CNN)은 지역적 특징에 적용되는 합성곱 필터를 가진 layer를 활용한다. 원래 컴퓨터 비전을 위해 개발되었지만 자연어 처리에 효과적임을 보여주었다.

본 논문에선 비지도 학습 신경망 언어 모델에서 얻은 단어를 기반으로 한 합성곱 계층을 갖는 간단한 CNN 모델을 학습한다. 단어 벡터를 고정한 상태에서 모델의 다른 매개변수만 학습한다. 하이퍼파라미터를 거의 조정하지 않아도 이 간단한 모델은 여러 벤치마크에서 훌륭한 성과를 냈다. 이는 사전 학습된 모델로부터 얻은 feature extractors로 다양한 분류 작업에 좋은 성능을 보여준다.

## Model
-------------

모델 구조는 다음과 같다.

<img src="https://velog.velcdn.com/images%2Flm_minjin%2Fpost%2Fa48d6908-88b7-4d25-a1b1-bd206addbdac%2Fimage.png">

$\mathbf{x}_i \in \mathbb{R}^k$s는 $i$번째 단어에 해당하는 $k$차원의 단어 벡터라 하자. 길이가 $n$(padding 처리 필수)인 문장은 다음과 같이 표현될 수 있다.

$$ \mathbf{x}_{1:n} = \mathbf{x}_1 \oplus \mathbf{x}_2 \oplus \ldots \oplus \mathbf{x}_n
$$

여기서 $\oplus$는 concatenation 연산자이다.

일반적으로 $\mathbf{x}_{i:i+j}$는 $i$번째 단어에서 $i+j$번째 단어를 concatenation한 것을 의미한다. convolution operation은 필터 $\mathbf{w} \in \mathbb{R}^{hk}$를 포함하고, 이는 $h$개의 단어에 윈도우를 적용하여 새로운 feature을 만든다.

예를 들어, 하나의 feature $c_i$는 $\mathbf{x}_{i:i+h-1}$ 단어에 적용된 윈도우로 부터 만들어졌다. 이를 수식으로 표현하면 아래와 같다.

$$
c_i = f(\mathbf{w} \cdot \mathbf{x}_{i:i+h-1} + b)
$$

이 때 $b \in \mathbb{R}$은 편향(bias)이고 $f$는 $tanh$와 같은 비선형 함수이다. 이 필터는 문장 ${\mathbf{x}_{1:h}, \mathbf{x}_{2:h+1}, \ldots, \mathbf{x}_{n-h+1:n}}$ 안에 있는 모든 단어들에 각각 적용되며, 다음과 같은 **feature map**을 만들어낸다. 

$$

$$\mathbf{c} = [c_1, c_2, \ldots, c_{n-h+1}]$$

이 때 $\mathbf{c} \in \mathbb{R}^{n-h+1}$이다.

이 후 max-over-time pooling operation을 feature map에 적용해 가장 큰 값 $\hat{c} = max{\mathbf{c}}$을 특정한 필터에 상응되는 특징으로 둔다. 이 Pooling 방식은 자연스럽게 가변적인 문장 길이를 처리한다.

지금까지 설명은 하나의 필터에서 하나의 특징을 추출하는 과정을 설명했다. 이 모델은 여러 필터를 사용해 여러 특징을 얻는다. 이러한 특징들은 두 번째 레이어를 형성하고, 레이블에 대한 확률 분포를 출력하는 fully connected softmax layer로 전달된다.



### Regularization


## Datasets and Experimental Setup
---------------

본 논문에서는 모델을 테스트하기 위해 여러 벤치마크를 사용했다. 이 중 SST-1을 사용하여 모델 구현을 진행할 예정이다.

- SST-1 : 스탠포드 감정 트리뱅크(Treebank) - MR의 확장 버전 느낌. train/dev/test가 나뉘어 있고, 감성이 좀 더 세밀하게 라벨링 되어 있음(아주 좋음, 좋음, 중립, 나쁨, 아주 나쁨)

<img src="https://velog.velcdn.com/images%2Flm_minjin%2Fpost%2F349997cd-99ab-4ae2-ac86-5d250f59bb29%2Fimage.png">

### Hyperparameters and Training

데이터에 적용된 것

- ReLU
- filter windows($h$) of 3, 4, 5 with 100 feature maps
- dropout rate($p$) of 0.5
- $l_2$ constraint ($s$) of 3
- mini-batch size of 50

훈련은 Adadelta 업데이트 규칙을 기반으로 무작위 미니 배치 SGD 방식을 사용했으며, 조기 종료 외에는 별도의 튜닝을 수행하지 않았음

### Pre-trained Word Vectors

대규모 지도 학습 데이터셋이 부족할 때 모델 성능을 향상하는 일반적인 방법으로 비지도 신경망 언어 모델을 통해 얻은 워드 벡터로 초기화

Google News 데이터셋의 1,000억 개 단어로 학습된 공개 Word2Vec 벡터를 사용함. 벡터들은 300차원이며, CBOW 아키텍처를 활용하여 훈련되었다. 사전 학습된 단어 집합에 없는 단어들은 무작위로 초기화됩니다.

### Model Variations

- CNN-rand : 모든 단어를 랜덤하게 초기화, 학습하면서 단어 벡터도 같이 학습.
- CNN-static : word2vec으로 사전 학습된 단어 벡터 사용. 학습 동안 모든 단어는 변동 X, 모델의 파라미터만 학습(사전 학습된 단어 집합에 없어서 랜덤하게 초기화한 단어 포함)
- CNN-non-static: word2vec으로 사전 학습된 단어 벡터 사용. 사전 학습된 단어가 각 태스크에 맞춰 파인 튜닝 됨.
- CNN-multichannel : 워드 벡터 집합이 2개인 모델. 각 벡터 집합은 "channel"로 처리되고, 각 필터는 두 채널 모두에 적용되지만 역전파는 채널 중 하나만 사용 . 그래서 이 모델은 한 채널은 파인 튜닝이 됐지만, 한 채널은 static을 유지. 두 채널 모두 word2vec으로 초기화 됨.

위의 변화와 다른 임의 요소의 효과를 분리하기 위해 CV-fold 할당, 단어 집합에 없는 단어 초기화, CNN 파라미터 초기화와 같은 임의성을 가지는 요소들을 균일하기 유지.

## Results and Discussion
---------------

<img src="https://velog.velcdn.com/images%2Flm_minjin%2Fpost%2F669f679c-1792-4900-826f-20e8376727e8%2Fimage.png">