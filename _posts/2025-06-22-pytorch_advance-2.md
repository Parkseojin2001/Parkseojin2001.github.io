---
title: "파이토치 심화(2)"
description: "정칙화 / 데이터 증강 및 변환 / 사전 학습된 모델"

categories: [파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터 비전 심층학습, Pytorch]
tags: [pytorch]

permalink: /pytorch/advance-2/

toc: true
toc_sticky: true
math: true
mermaid: true

date: 2025-06-22
last_modified_at: 2025-06-22
---

## 정칙화
-----------

**정칙화(Regularization)**란 모델 학습 시 발생하는 과대적합 문제를 방지하기 위해 사용되는 기술로, 모델이 **암기(Memorization)**가 아니라 **일반화(Generalization)**할 수 있도록 손실 함수에 **규제(Penalty)**를 가하는 방식이다.

- 암기: 모델이 데이터의 특성이나 패턴을 학습하는 것이 아니라 훈련 데이터의 노이즈를 학습했을 때 발생
    - 모델이 훈련 데이터에서는 잘 수행되지만, 새로운 데이터에서는 재대로 수행되지 못한다.
    - 데이터의 일반적인 패턴을 학습한 것이 아니라 학습 데이터의 노이즈나 특정 패턴을 학습
- 일반화: 모델이 새로운 데이터에서도 정확한 예측을 할 스 있음을 의미
    - 특정 데이터가 갖고 있는 노이즈가 아닌 데이터의 일반적인 패턴을 학습했을 때 일반화된 모델이라 부름
    - 일반화된 모델은 학습에 사용한 데이터와 약간 다르더라도 정확한 예측을 할 수 있음
- 정칙화: 노이즈에 강건하고 일반화된 모델을 구축하기 위해 사용하는 방법
    - 모델이 특정 피처나 특정 패턴에 너무 많은 비중을 할당하지 않도록 손실 함수에 규제를 가해 모델의 **일반화 성능(Generalization Performance)**을 향상시킴
    - 적용을 하면 학습 데이터들이 갖고 있는 작은 차이점에 대해 덜 민감해져 모델의 분산 값이 낮아져 모델이 의존하는 특징 수를 줄임으로써 모델의 추론 능력을 개선

<img src="https://blog.kakaocdn.net/dn/bumg11/btsqLlyJM7b/bOQ5SYMg8WHB8RhtYZbOz0/img.png">

정칙화는 모델이 비교적 복잡하고 학습에 사용되는 데이터의 수가 적을 때 활용한다. 노이즈에 강건하게 만드므로 이미 정규화되어 있는 경우에는 사용하지 않아도 된다.

### L1 정칙화

**L1 정칙화(L1 Regularization)**는 **라쏘 정칙화(Lasso Regularization)**라고도 하며, **L1 노름(L1 Norm)** 방식을 사용해 규제하는 방법이다.

> L1 노름은 벡터 또는 행렬값의 절댓값 합계를 계산한다. 

이러한 방식을 차용해 L1 정칙화는 손실 함수에 가중치 절댓값의 합을 추가해 과대적합을 방지한다.

- 모델 학습은 비용이 0이 되는 방향으로 진행하며 손실 함수에 가중치 절댓값을 추가하므로 이 값도 최소화 하는 방향으로 학습이 진행된다.
- 모델 학습 시 값이 크지 않은 가중치들은 0으로 수렴하게 되어 예측에 필요한 특징의 수가 줄어든다.
- 불필요한 가중치가 0으로 수렴하게 되어 예측에 필요한 특징의 수가 줄어들어 특징 선택 효과를 얻을 수 있다.

$$
L_1 = \lambda * \sum_{i=0}^{n}|w_i|
$$

- $\lambda$: 규제 강도로, 너무 많거나 적은 규제를 가하지 않게 조절하는 하이퍼파라미터로 0보다 큰 값으로 설정 
    - 0에 가까워질수록 더 많은 특징을 사용하기 때문에 과대적합에 민감해진다.
    - 규제 강도를 높이면 대부분의 가중치가 0에 수렴되기 때문에 과소적합 문제가 발생

L1 정칙화를 적용하는 경우 입력 데이터에 더 민감해지며, 항상 최적의 규제를 가하지 않으므로 사용에 주의해야한다.

```python
# L1 정칙화 적용
for x, y in train_dataloader:
    x = x.to(device)
    y = y.to(device)

    output = model(x)

    _lambda = 0.5
    l1_loss = sum(p.abs().sum() for p in model.parameters())

    loss = criterion(output, y) + _lambda * l1_loss
```

- 모델의 가중치를 모두 계산해 모델을 갱신해야 하므로 **계산 복잡도**를 높이게 된다.
- L1 정칙화는 미분이 불가능해 역전파를 계산하는 데 더 많은 리소스를 소모
- 하이퍼파라미터인 `_lambda`의 값이 적절하지 않으면 가중치 값들이 너무 작아져 모델을 해것하기 더 어렵게 만듦

L1 정칙화는 주로 선형 모델에 적용하며 이를 **라쏘 회귀**라 한다.ㄴ

### L2 정칙화

**L2 정칙화(L2 Regularization)**는 **릿지 정칙화(Ridge Regularization)**라고도 하며 **L2 노름(L2 Norm)** 방식을 사용해 규제하는 방법이다.

> L2 노름은 벡터 또는 행렬 값의 크기를 계산한다.

이 방식을 차용해 L2 정칙화는 손실 함수에 가중치 제곱의 합을 추가해 과대적합을 방지하도록 규제한다.

- 하나의 특징이 너무 중요한 요소가 되지 않도록 규제를 가함
- 모델 학습 시 오차를 최소화하면서 가중치를 작게 유지하고 골고루 분포되게끔 하므로 모델의 복잡도가 일부 조정
- L2 정칙화는 가중치 값들이 비교적 균일하게 분포되며, 가중치를 0으로 만들지 않고 0에 가깝게 만듦


$$
L_2 = \lambda * \sum_{i=0}^n|w_i^2|
$$

L1 정칙화와 L2 정칙화에 평균 제곱 오차를 적용했을 떄 $y=w \times x$의 결과를 통해 둘의 관계를 알 수 있다. 이떄 가중치($w=0.5$)로 설정한다.

<img src="../assets/img/post/L1_L2_before.png">

모델은 $y = 0.5x$ 이므로 평균 제곱 오차의 손실이 가장 낮은 가중치 값은 0.5가 된다. 
- L1 정칙화는 가중치 절댓값으로 계산되므로 선형적인 구조를 갖는다.
- L2 정칙화는 가중치 제곱으로 계산되므로 비선형적인 구조를 갖는다.

정칙화는 손실 함수에 규제 값을 더해주는 방법으로 적용되며 이를 적용하면 아래의 그래프처럼 그려진다.

<img src="../assets/img/post/L1_L2_after.png">

L1 정칙화는 선형적인 특성을 가져 가중치가 0이 아닌 곳에서는 모든 값에 고정적인 값을 추가하는 반면에 L2 정칙화는 비선형적인 특성을 가지므로 가중치가 0에 가까워질수록 규젯값이 줄어든다.

```python
# L2 정칙화 적용
for x, y in train_dataloader:
    x = x.to(device)
    y = y.to(device)

    output = model(x)

    _lambda = 0.5
    ls_loss = sum(p.pow(2.0).sum() for p in model.parameters())

    loss = criterion(output, y) + _lambda * l2_loss
```

- L2 정칙화는 모델 매개변수의 제곱 값을 계산하고 저장해야 하므로 L1 보다 많은 리소스를 소모
- 하이퍼파라미터인 `_lambda`도 여러 번 반복해 최적의 `_lambda` 값을 찾아야 함

과대적합을 효과적으로 방지하기 위해선느 조기 중지 또는 드롭아웃과 같은 기술과 함께 사용한다. L2 정칙화는 주로 심층 신경망 모델에서 사용하며, 선형 회귀 모델에서 L2 정칙화를 적용하는 경우를 **릿지 회귀(Ridge Regression)**라고 한다.

|      |L1 정칙화|L2 정칙화|
|------|--------|-------|
|계산 방식|가중치 절댓값의 합|가중치 제곱의 합|
|모델링|희소함(Sparse Solution)|희소하지 않음(Non-sparse Solution)|
|특징 선택|있음|없음|
|이상치|강함|약함|
|가중치|0이 될 수 있음|0에 가깝게 됨|
|학습|비교적 복잡한 데이터 패턴을 학습할 수 없음|비교적 복잡한 데이터 패턴을 학습할 수 있음|

### 가중치 감쇠

### 모멘텀


### 엘라스틱 넷

### 드롭아웃

### 그레이디언트 클리핑


## 데이터 증강 및 변환
------------


## 사전 학습된 모델
------------