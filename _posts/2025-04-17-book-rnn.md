---
title: "[파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터 비전 심층학습] 순환 신경망"
description: "RNN / LSTM"

categories: [Book, deep-learning-with-pytorch-transformers]
tags: [NLP]

permalink: /pytorch-book/nlp/rnn/

toc: true
toc_sticky: true
math: true
mermaid: true

date: 2025-04-17
last_modified_at: 2025-04-19
---
**순환 신경망(Recurrent Neural Network, RNN)** 모델은 순서가 있는 **연속적인 데이터(Sequence data)**를 처리하는 데 적합한 구조를 갖고 있다. 순환 신경망은 각 **시점(Time step)**의 데이터가 이전 시점의 데이터와 독립적이지 않다는 특성 때문에 효과적으로 작동한다. 

- 연속성 데이터: 특정 시점 $t$에서의 데이터가 이전 시점($t_0, t_1, ..., t_{n-1}$)의 영향을 받는 데이터

자연어 데이터는 연속적인 데이터의 일종으로 볼 수 있다. 자연어는 한 단어가 이전 단어들과 상호작용하여 문맥을 이루고 의미를 형성한다. 

또한 긴 문장일수록 앞선 단어들과 뒤따르는 단어들 사이에 강한 **상관관계(Correlation)**가 존재한다.

## 순환 신경망
---------

순환 신경망은 연속적인 데이터를 처리하기 위해 개발된 인공 신경망의 한 종류다. 이전에 처리한 데이터를 다음 단계에 활용하고 현재 입력 데이터와 함께 모델 내부에서 과거의 상태를 기억해 현재 상태를 예측하는 데 사용한다.

- 시계열 데이터
- 자연어 처리
- 음성 인식
- 시퀀스 데이터

순환 신경망은 연속형 데이터를 순서대로 입력받아 처리하며 각 시점마다 **은닉 상태(Hidden state)**의 형태로 저장한다. 각 시점의 데이터를 입력으로 받아 은닉 상태와 출력값을 계산하는 노드를 순환 신경망의 **셀(Cell)**이라 한다.

순환 신경망의 셀은 이전 시점의 은닉 상태 $h_{t-1}$을 입력으로 받아 현재 시점의 은닉 상태 $h_t$를 계산한다.

<img src="https://images.velog.io/images/yuns_u/post/ccbb28ea-fa08-4d23-804e-419e6f578e4b/image.png">

순환 신경망은 각 시점 $t$에서 현재 입력값 $x_t$와 이전 시점 $t-1$의 은닉 상태 $h_{t-1}$를 이용해 현재 시점의 은닉 상태 $h_t$와 출력값 $y_t$를 계산한다.

은닉 상태의 수식은 아래와 같다.

$$
h_t = \sigma_h(h_{t-1}, x_t) \\
h_t = \sigma_h(W_{hh}h_{t-1} + W_{xh}x_t + b)
$$

- $\sigma_h$: 순환 신경망의 은닉 상태를 계산하기 위한 활성화 함수
- $h_{t-1}$: 이전 시점 t-1의 은닉 상태
- $x_t$: 현재 시점 t의 입력값
- $h_t$: 현재 시점 t의 은닉 상태

$\sigma_h$는 가중치(W)와 편향(b)을 이용해 계산한다. 
- $W_{hh}$: 이전 시점의 은닉 상태 $h_{t-1}$에 대한 가중치
- $W_{xh}$: 입력값 $x_t$에 대한 가중치
- $b_h$: 은닉 상태 $h_t$의 편향

출력값 계산은 아래와 같다.

$$
y_t = \sigma_y(h_t)\\
y_t = \sigma_y(W_{hy}h_t + b_y)
$$

- $\sigma_y$: 순환 신경망의 출력값을 계산하기 위한 활성화 함수
- $W_{hy}$: 현재 시점의 은닉 상태 $h_t$에 대한 가중치
- $b_y$: 출력값 $y_t$의 편향

순환 신경망의 출력값은 이전 시점의 정보를 현재 시점에서 활용해 입력 시퀀스의 패턴을 파악하고 출력값을 예측하므로 연속형 데이터를 처리할 수 있다.

### 일대다 구조

**일대다 구조(One-to-Many)**는 하나의 입력 시퀀스에 대해 여러 개의 출력값을 생성하는 순환 신경망 구조다.

ex. 하나의 문장을 입력으로 받고, 문장에서 각 단어의 품사를 예측하는 작업

이러한 일대다 구조를 구현하기 위해서는 출력 시퀀스의 길이를 미리 알고 있어야 한다. 이를 위해 입력 시퀀스를 처리하면서 시퀀스의 정보를 활용해 출력 시퀀스의 길이를 예측하는 모델을 함께 구현해야 한다.

<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQr-e9ElLkpsqC_cjyvcuuSY-BDSbniymnfnezOgGUEbt-2ZHo8Mkv2MgsqMxFfPvpJWw&usqp=CAU" width="170px">

### 다대일 구조

**다대일 구조(Many-to-One)**는 여러 개의 입력 시퀀스에 대해 하나의 출력값을 생성하는 순환 신경망 구조다.

ex. 감성 분류 분야 - 입력 시퀀스는 문장으로 이루어져 있으며, 출력값은 해당 문장의 감정(긍정, 부정)을 예측하는 작업

입력 시퀀스가 어떤 범주에 속하는지를 구분하는 문장 분류, 두 문장 간의 관계를 추론하는 **자연어 추론(Natural Language Inference)** 등에도 적용할 수 있다.

<img src="https://github.com/user-attachments/assets/3272aa0f-560d-456d-8280-89cfd2111b9a" width="200px">

### 다대다 구조

**다대다 구조(Many-to-Many)**는 입력 시퀀스와 출력 시퀀스의 길이가 여러 개인 경우에 사용되는 신경망 구조다. 

ex. 입력 문장에 대해 번역된 출력 문장을 생성하는 번역기, 음성 인식 시스템에서 음성 신호를 입력으로 받아 문장을 출력하는 음성 인식기

다대다 구조에서는 입력 시퀀스와 출력 시퀀스의 길이가 서로 다른 경우가 있을 수 있다. 이런 경우 입력 시퀀스와 출력 시퀀스의 길이를 맞추기 위해 패딩을 추가하거나 잘라내는 등의 전처리 과정이 수행된다.

다대다 구조는 **시퀀스-시퀀스(Seq2Seq)** 구조로 이뤄져 있다.
- **인코더(Encoder)**: 입력 시퀀스 처리하며 고정 크기의 벡터를 출력
- **디코더(Decoder)**: 출력 시퀀스를 생성

<img src="https://github.com/user-attachments/assets/18ce0a8b-1350-4d13-86fe-82fa1daa06d3">

### 양방향 순환 신경망

**양방향 순환 신경망(Bidirectional Recurrent Neural Network, BiRNN)**은 기본적인 순환 신경망에서 시간 방향을 양방향으로 처리할 수 있도록 고안된 방식이다. 이전 시점(t-1)의 은닉 상태뿐만 아니라 이후 시점(t+1)의 은닉 상태도 함께 이용한다.

ex. "인생은 B와 _ 사이의 C다."라는 문장에서 _에 입력될 단어를 예측

- t 시점 이후의 데이터와 t 시점 이전의 데이터 모두 t 시점의 데이터를 예측하는 데 사용된다.
- 입력 데이터를 순방향으로 처리하는 것만 아니라 역방향으로 거꾸로 읽어 들여 처리한다.

<img src="https://miro.medium.com/max/1313/1*6QnPUSv_t9BY9Fv8_aLb-Q.png">

### 다중 순환 신경망

**다중 순환 신경망(Stacked Recurrent Neural Network)**은 여러 개의 순환 신경망을 연결하여 구성한 모델로 각 순환 신경망이 서로 다른 정보를 처리하도록 설계돼 있다.

다중 순환 신경망은 여러 개의 순환 신경망 층으로 구성되며 각 층에서의 출력값은 다음 층으로 전달되어 처리하도록 설계돼 있다. 각 층의 가중치는 각각 동일하다.

<img src="https://lh6.googleusercontent.com/rC1DSgjlmobtRxMPFi14hkMdDqSkEkuOX7EW_QrLFSymjasIM95Za2Wf-VwSC1Tq1sjJlOPLJ92q7PTKJh2hjBoXQawM6MQC27east67GFDklTalljlt0cFLZnPMdhp8erzO">

**장점**<br>
- 데이터의 다양한 특징을 추출할 수 있어 성능이 향상될 수 있다.
- 층이 깊어질수록 더 복잡한 패턴을 학습할 수 있다.

**단점**<br>
- 층이 많아질수록 학습 시간이 오래 걸린다.
- 기울기 소실 문제가 발생할 가능성이 높아진다.
- 시간적으로 먼 과거의 정보를 잘 기억하지 못한다.

## 장단기 메모리
---------

**장단기 메모리(Long Short-Term Memory, LSTM)**란 기존 순환 신경망이 갖고 있던 기억력 부족과 기울기 소실 문제를 해결한 모델이다.

일반적인 순환 신경망은 연속적인 데이터를 다룰 수 있지만 앞선 시점에서의 정보를 끊임없이 반영하기에 학습 데이터의 크기가 커질소록 앞의 정보가 충분히 전달되지 않는다. 

이 단점으로 인해 **장기 의존성 문제(Long-term dependencies)**가 발생할 수 있다. 또한, 활성화 함수로 사용되는 하이퍼볼릭 탄젠트 함수나 ReLU 함수의 특성으로 인해 역전파 과정에서 기울기 소실이나 기울기 폭주가 발생할 가능성이 있다.

이를 해결하기 위해 장단기 메모리를 사용한다. 순환 신경망과 비슷한 구조를 가지지만, **메모리 셀(Memory cell)**과 **게이트(Gate)**라는 구조를 도입해 장기 의존성 문제와 기울기 소실 문제를 해결한다.

<img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfqrpOQa51YfobrYRmLc7LoIyvIXYOEJbMVdwRJL8LZz_fm-qCGoffp2jwnQQCY4byM3HO1lYa1ArqKcTKB4d0yNOWSxkWn3V1JZy0W9zHJPEyTdlPSi3rAOzE54RajdEMhWmn5?key=7Vuyf3ohVqJy80Qyo-v2YFS1">

장단기 메모리는 4가지 종류의 방식으로 정보의 흐름을 제어한다.
- 셀 상태(Cell state): 정보를 저장하고 유지하는 메모리 역할을 하며 출력 게이트와 망각 게이트에 의해 제어
- 망각 게이트(Forget gate): 장단기 메모리에서 이전 셀 상태에서 어떠한 정보를 삭제할지 결정하는 역할을 하며 현재 입력과 이전 셀 상태를 입력으로 받아 어떤 정보를 삭제할지 결정
- 입력(기억) 게이트(Input gate): 새로운 정보를 어떤 부분에 추가할지 결정하는 역할을 하며 현재 입력과 이전 셀 상태를 입력으로 받아 어떤 정보를 추가할지 결정
- 출력 게이트(Output gate): 셀 상태의 정보 중 어떤 부분을 출력할지 결정하는 역할을 하며 현재 입력과 이전 셀 상태, 그리고 새로 추가된 정보를 입력으로 받아 출력할 정보를 결정

### 장단기 메모리 구조

LSTM의 cell state의 연산 과정은 다음과 같다.

<img src="https://wikidocs.net/images/page/152773/3.JPG">

메모리 셀은 순환 신경망의 은닉 상태와 유사하게 현재 시점의 입력과 이전 시점의 은닉 상태를 기반으로 정보를 계산하고 저장하는 역할을 한다. 

하지만 순환 신경망에서 은닉 상태는 출력값을 계산하는 데 사용되지만, LSTM의 메모리 셀은 출력값 계산에 직접 사용하지 않는다.

대신 forget gate, input gate, output gate를 통해 어떤 정보를 버릴지, 어떤 정보를 기억할지, 어떤 정보를 출력할지를 결정하는 추가적인 연산을 수행한다.

세 가지 게이트 모두 시그모이드를 활성화 함수로 사용하므로 게이트의 출력값은 각각 0에서 1 사이의 값으로 결정된다. 이 값이 해당 게이트가 입력값에 대해 얼마나 많은 정보를 통과시킬지 결정한다.

**Forget gate**

<img src="https://wikidocs.net/images/page/152773/4.JPG">

- $h_{t-1}$: 이전 시점의 은닉상태
- $x_t$: 현재 시점의 입력값
- $f_t$: 시그모이드 활성화 함수를 사용해 계산
- $W_f$: 입력값과 은닉 상태를 위한 가중치
- $b_f$: forget gate를 위한 편향

forget gate는 두 가중치를 통해 forget gate 출력값을 최적화하며 이 출력값은 메모리 셀을 계산하기 위한 가중치로 사용된다.

- forget gate = 1: 이전 시점의 기억 상태가 현재 시점의 기억 상태에 완전히 유지
- forget gate = 0: 이전 시점의 기억 상태는 현재 시점의 기억 상태에 전혀 반영되지 않음

**Input gate**

<img src="https://wikidocs.net/images/page/152773/5.JPG">

- $\tilde{C_t}$: -1에서 1 사이의 값을 가지므로 이전 시점의 은닉 상태와 현재 시점의 입력값은 모두 [-1, 1] 범위 안에 존재한다. 하지만 이것만으로는 새로운 은닉 상태를 계산하기 위해 이 은닉 상태를 얼마나 기억할지 제어하기가 어렵다.

- $i_t$: 새로운 은닉 상태의 기억을 제어한다. [0, 1]의 값을 가지므로 현재 시점에서 얼마나 많은 정보를 기억할 것인지를 결정하는 가중치 역할을 한다.
  - 1에 가까울수록 기억할 정보가 많아지고, 0에 가까울수록 정보를 망각하게 된다.

현재 시점의 입력과 이전 시점의 은닉 상태를 입력으로 받아 시그모이드 함수를 거친 값과 하이퍼볼릭 탄젠트 함수를 거친 값의 곱으로 새로운 기억 값을 계산한다.

**Update**

<img src="https://wikidocs.net/images/page/152773/6.JPG">

- $f_t$: forget gate의 출력값
- $c_{t-1}$: 이전 시점의 메모리 셀 값

forget gate와 input gate 값을 각각 원소별 곱셈 연선을 의미하는 **아다마르 곱**한 후 합산한다. 
- forget gate는 이전 시점의 메모리 셀을 얼마나 유지할지 결정
- input gate는 현재 시점의 새로운 정보를 얼마나 받아들일지를 결정

**Output gate**

<img src="https://wikidocs.net/images/page/152773/7.JPG">


- $o_t$: 현재 시점의 출력값
  - 출력값의 범위는 [0, 1]이며 은닉 상태의 값을 얼마나 사용할지 결정한다.
- $h_t$: 현재 시점의 은닉 상태
  - output gate와 하이퍼볼릭 탄젠트가 적용된 메모리 셀 값으로 계산되며 범위는 [-1, 1]이다.
